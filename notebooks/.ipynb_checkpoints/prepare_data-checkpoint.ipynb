{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Prepare Data From CSV Recording\n",
    "\n",
    "##Load File\n",
    "\n",
    "'Data is loaded from a CSV recording file, accepted through an input prompt. This includes all positional data related to the 6 trackers (HMD, Left Controller, Right Controller, Waist, Left Foot, Right Foot).'\n",
    "\n",
    "'Data is loaded into a Pandas dataframe. The primary tracking data is then extracted, leaving extraneous data such as booleans for button presses.'\n",
    "\n",
    "'The extracted columns are then concatenated into a new dataframe, and the columns are renamed for ease of reading.' \n",
    "\n",
    "'The new trimmed file is written to a directory (/trim_output), for further manipulation and loading into the model.'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd\n",
    "\n",
    "recording_path = \"../recordings/\"\n",
    "output_path = \"../trim_output/\"\n",
    "file_name = input(\"Input File Name\")\n",
    "\n",
    "#Read in CSV\n",
    "\n",
    "try:\n",
    "    dataframe = pd.read_csv(recording_path + file_name + \".csv\")\n",
    "except:\n",
    "    print(\"Error Reading File: Check Spelling and Try Again\")\n",
    "\n",
    "# Seperate each tracker to seperate dataframe\n",
    "HMD = dataframe.iloc[:, 1:4]\n",
    "controller_1 = dataframe.iloc[:, 7:10]\n",
    "controller_2 = dataframe.iloc[:, 37:40]\n",
    "left_foot = dataframe.iloc[:, 67:70]\n",
    "right_foot = dataframe.iloc[:, 73:76]\n",
    "waist = dataframe.iloc[:, 79:82]\n",
    "\n",
    "# Join all trackers together\n",
    "joined = pd.concat([HMD, controller_1, controller_2,\n",
    "                   waist, left_foot, right_foot], axis=1)\n",
    "\n",
    "# set new column headers\n",
    "joined.columns = [\n",
    "    \"head_x\",\n",
    "    \"head_y\",\n",
    "    \"head_z\",\n",
    "    \"r_controller_x\",\n",
    "    \"r_controller_y\",\n",
    "    \"r_controller_1_z\",\n",
    "    \"l_controller_x\",\n",
    "    \"l_controller_y\",\n",
    "    \"l_controller_z\",\n",
    "    \"waist_x\",\n",
    "    \"waist_y\",\n",
    "    \"waist_z\",\n",
    "    \"r_foot_x\",\n",
    "    \"r_foot_y\",\n",
    "    \"r_foot_z\",\n",
    "    \"l_foot_x\",\n",
    "    \"l_foot_y\",\n",
    "    \"l_foot_z\"\n",
    "]\n",
    "\n",
    "\n",
    "# output to new csv\n",
    "output_file = output_path + file_name + \"_trimmed.csv\"\n",
    "joined.to_csv(output_file, index=False)\n",
    "\n",
    "print(file_name + \" output to \" + output_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data Normalization\n",
    "\n",
    "## Data Scaling\n",
    "\n",
    "'The new CSV is loaded into memory, chosen through an input prompt'\n",
    "'The data is then split between the features (the HMD and controller tracking data), and the labels (the waist and foot trackers).'\n",
    "'These are loaded into Numpy arrays to peform normaliztion. The output from OpenVR Recorder is upscaled by 100. To correct this the array is divided by 100'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Input File Name arm_raise_1_trimmed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataframe created\n",
      "(905, 9) [[-0.01512766  1.5719162  -0.09888399 ... -0.26088011  0.76942291\n",
      "  -0.10149074]\n",
      " [-0.01512766  1.5719162  -0.09888399 ... -0.26017019  0.76955322\n",
      "  -0.10172288]\n",
      " [-0.01513591  1.5719162  -0.09887324 ... -0.25940933  0.76964912\n",
      "  -0.10187013]\n",
      " ...\n",
      " [-0.0277946   1.60442154 -0.08457804 ... -0.19361816  1.17218666\n",
      "  -0.25626904]\n",
      " [-0.0277946   1.60442154 -0.08457804 ... -0.19326099  1.17207565\n",
      "  -0.2562438 ]\n",
      " [-0.0277946   1.60442154 -0.08457804 ... -0.19324625  1.17204033\n",
      "  -0.25649668]]\n",
      "(905, 9) [[-0.0044589   0.96929459 -0.08480424 ... -0.20655287  0.09259642\n",
      "   0.00614011]\n",
      " [-0.00440914  0.96923447 -0.08475275 ... -0.20653749  0.09257123\n",
      "   0.00614011]\n",
      " [-0.00486888  0.96952217 -0.08502438 ... -0.20651741  0.09255754\n",
      "   0.00614011]\n",
      " ...\n",
      " [-0.01349738  0.97748932 -0.09893991 ... -0.20459211  0.09642521\n",
      "   0.00729573]\n",
      " [-0.01333761  0.97723671 -0.09890824 ... -0.20459616  0.09634519\n",
      "   0.00729573]\n",
      " [-0.01398552  0.97791611 -0.09938544 ... -0.20459616  0.09634519\n",
      "   0.00729573]]\n"
     ]
    }
   ],
   "source": [
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Flatten\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "#from sklearn.metrics import mean_absolute_error \n",
    "from matplotlib import pyplot as plt\n",
    "#import seaborn as sb\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "output_path = \"../trim_output/\"\n",
    "\n",
    "file_name = input(\"Input File Name\")\n",
    "\n",
    "#read in formatted CSV\n",
    "try:\n",
    "    dataframe = pd.read_csv(output_path + file_name + \".csv\")\n",
    "    print(\"Dataframe created\")\n",
    "except:\n",
    "    print(\"Error Reading File\")\n",
    "\n",
    "\n",
    "#Show shape of the dataframe\n",
    "dataframe.describe()\n",
    "\n",
    "x_train_df = dataframe.iloc[:, 0:9]\n",
    "y_train_df = dataframe.iloc[:, 9:18]\n",
    "#display(x_train_df)\n",
    "#display(y_train_df)\n",
    "\n",
    "#Load data into Numpy array\n",
    "x_train = np.array(x_train_df)\n",
    "y_train = np.array(y_train_df)\n",
    "\n",
    "\n",
    "\n",
    "#Divide values in array by 100\n",
    "x_train_normalized = np.divide(x_train, 100)\n",
    "y_train_normalized = np.divide(y_train, 100)\n",
    "\n",
    "print(x_train_normalized.shape, x_train_normalized)\n",
    "print(y_train_normalized.shape, y_train_normalized)\n",
    "\n",
    "#x_train, x_test, y_train, y_test = train_test_split(x_train_normalized, y_train_normalized)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Model Creation and Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_2 (Embedding)     (None, 9, 16)             14480     \n",
      "                                                                 \n",
      " bidirectional_1 (Bidirectio  (None, 64)               9600      \n",
      " nal)                                                            \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 10)                650       \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 1)                 11        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 24,741\n",
      "Trainable params: 24,741\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(input_dim=len(x_train_normalized), output_dim= 16, input_length=9 ),\n",
    "    tf.keras.layers.Bidirectional(tf.keras.layers.GRU(32)),\n",
    "    tf.keras.layers.Dense(10, activation='relu'),\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# fit the model\n",
    "model.fit(x_train_normalized, y_train_normalized, epochs=50, verbose=0)\n",
    "# evaluate the model\n",
    "loss, accuracy = model.evaluate(padded_docs, labels, verbose=0)\n",
    "print('Accuracy: %f' % (accuracy*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "vscode": {
   "interpreter": {
    "hash": "85eeaad1f84315a0c9c5600e08c8d22c182ff5487c2ae8eda931033c2f461801"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
