{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare Data From CSV Recording\n",
    "\n",
    "## Load File\n",
    "\n",
    "Data is loaded from a CSV recording file, accepted through an input prompt. This includes all positional data related to the 6 trackers (HMD, Left Controller, Right Controller, Waist, Left Foot, Right Foot).\n",
    "\n",
    "'Data is loaded into a Pandas dataframe. The primary tracking data is then extracted, leaving extraneous data such as booleans for button presses.\n",
    "\n",
    "The extracted columns are then concatenated into a new dataframe, and the columns are renamed for ease of reading.\n",
    "\n",
    "The new trimmed file is written to a directory (/trim_output), for further manipulation and loading into the model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Input File Name walking_2_test\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error Reading File: Check Spelling and Try Again\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Length mismatch: Expected axis has 6 elements, new values have 18 elements",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[1;32mIn [21]\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     24\u001b[0m joined \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mconcat([HMD, controller_1, controller_2,\n\u001b[0;32m     25\u001b[0m                    waist, left_foot, right_foot], axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     27\u001b[0m \u001b[38;5;66;03m# set new column headers\u001b[39;00m\n\u001b[1;32m---> 28\u001b[0m joined\u001b[38;5;241m.\u001b[39mcolumns \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m     29\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhead_x\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     30\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhead_y\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     31\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhead_z\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     32\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mr_controller_x\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     33\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mr_controller_y\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     34\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mr_controller_z\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     35\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124ml_controller_x\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     36\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124ml_controller_y\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     37\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124ml_controller_z\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     38\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwaist_x\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     39\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwaist_y\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     40\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwaist_z\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     41\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mr_foot_x\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     42\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mr_foot_y\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     43\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mr_foot_z\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     44\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124ml_foot_x\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     45\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124ml_foot_y\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     46\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124ml_foot_z\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     47\u001b[0m ]\n\u001b[0;32m     50\u001b[0m \u001b[38;5;66;03m# output to new csv\u001b[39;00m\n\u001b[0;32m     51\u001b[0m output_file \u001b[38;5;241m=\u001b[39m output_path \u001b[38;5;241m+\u001b[39m file_name \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_trimmed.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\generic.py:5588\u001b[0m, in \u001b[0;36mNDFrame.__setattr__\u001b[1;34m(self, name, value)\u001b[0m\n\u001b[0;32m   5586\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   5587\u001b[0m     \u001b[38;5;28mobject\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__getattribute__\u001b[39m(\u001b[38;5;28mself\u001b[39m, name)\n\u001b[1;32m-> 5588\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mobject\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__setattr__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   5589\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m:\n\u001b[0;32m   5590\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\_libs\\properties.pyx:70\u001b[0m, in \u001b[0;36mpandas._libs.properties.AxisProperty.__set__\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\generic.py:769\u001b[0m, in \u001b[0;36mNDFrame._set_axis\u001b[1;34m(self, axis, labels)\u001b[0m\n\u001b[0;32m    767\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_set_axis\u001b[39m(\u001b[38;5;28mself\u001b[39m, axis: \u001b[38;5;28mint\u001b[39m, labels: Index) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    768\u001b[0m     labels \u001b[38;5;241m=\u001b[39m ensure_index(labels)\n\u001b[1;32m--> 769\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_mgr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mset_axis\u001b[49m\u001b[43m(\u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    770\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_clear_item_cache()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\internals\\managers.py:214\u001b[0m, in \u001b[0;36mBaseBlockManager.set_axis\u001b[1;34m(self, axis, new_labels)\u001b[0m\n\u001b[0;32m    212\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mset_axis\u001b[39m(\u001b[38;5;28mself\u001b[39m, axis: \u001b[38;5;28mint\u001b[39m, new_labels: Index) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    213\u001b[0m     \u001b[38;5;66;03m# Caller is responsible for ensuring we have an Index object.\u001b[39;00m\n\u001b[1;32m--> 214\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_set_axis\u001b[49m\u001b[43m(\u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnew_labels\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    215\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maxes[axis] \u001b[38;5;241m=\u001b[39m new_labels\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\internals\\base.py:69\u001b[0m, in \u001b[0;36mDataManager._validate_set_axis\u001b[1;34m(self, axis, new_labels)\u001b[0m\n\u001b[0;32m     66\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[0;32m     68\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m new_len \u001b[38;5;241m!=\u001b[39m old_len:\n\u001b[1;32m---> 69\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m     70\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLength mismatch: Expected axis has \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mold_len\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m elements, new \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     71\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalues have \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnew_len\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m elements\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     72\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: Length mismatch: Expected axis has 6 elements, new values have 18 elements"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "import pandas as pd\n",
    "\n",
    "recording_path = \"../recordings/\"\n",
    "output_path = \"../trim_output/\"\n",
    "file_name = input(\"Input File Name\")\n",
    "\n",
    "#Read in CSV\n",
    "\n",
    "try:\n",
    "    dataframe = pd.read_csv(recording_path + file_name + \".csv\")\n",
    "except:\n",
    "    print(\"Error Reading File: Check Spelling and Try Again\")\n",
    "\n",
    "# Seperate each tracker to seperate dataframe\n",
    "HMD = dataframe.iloc[:, 1:4]\n",
    "controller_1 = dataframe.iloc[:, 7:10]\n",
    "controller_2 = dataframe.iloc[:, 37:40]\n",
    "left_foot = dataframe.iloc[:, 67:70]\n",
    "right_foot = dataframe.iloc[:, 73:76]\n",
    "waist = dataframe.iloc[:, 79:82]\n",
    "\n",
    "# Join all trackers together\n",
    "joined = pd.concat([HMD, controller_1, controller_2,\n",
    "                   waist, left_foot, right_foot], axis=1)\n",
    "\n",
    "# set new column headers\n",
    "joined.columns = [\n",
    "    \"head_x\",\n",
    "    \"head_y\",\n",
    "    \"head_z\",\n",
    "    \"r_controller_x\",\n",
    "    \"r_controller_y\", \n",
    "    \"r_controller_z\",\n",
    "    \"l_controller_x\",\n",
    "    \"l_controller_y\",\n",
    "    \"l_controller_z\",\n",
    "    \"waist_x\",\n",
    "    \"waist_y\",\n",
    "    \"waist_z\",\n",
    "    \"r_foot_x\",\n",
    "    \"r_foot_y\",\n",
    "    \"r_foot_z\",\n",
    "    \"l_foot_x\",\n",
    "    \"l_foot_y\",\n",
    "    \"l_foot_z\"\n",
    "]\n",
    "\n",
    "\n",
    "# output to new csv\n",
    "output_file = output_path + file_name + \"_trimmed.csv\"\n",
    "joined.to_csv(output_file, index=False)\n",
    "\n",
    "print(file_name + \" output to \" + output_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Normalization\n",
    "\n",
    "## Data Scaling\n",
    "\n",
    "The new CSV is loaded into memory, chosen through an input prompt\n",
    "The data is then split between the features (the HMD and controller tracking data), and the labels (the waist and foot trackers).\n",
    "These are loaded into Numpy arrays to peform normaliztion. The output from OpenVR Recorder is upscaled by 100. To correct this the array is divided by 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "#from sklearn.metrics import mean_absolute_error \n",
    "from matplotlib import pyplot as plt\n",
    "#import seaborn as sb\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "output_path = \"../trim_output/\"\n",
    "\n",
    "\n",
    "#read in formatted CSV\n",
    "def ReadCSV():\n",
    "    file_name = input(\"Input File Name\")\n",
    "    try:\n",
    "        dataframe = pd.read_csv(output_path + file_name + \".csv\")\n",
    "        print(\"Dataframe created\")\n",
    "    except:\n",
    "        print(\"Error Reading File\")\n",
    "    return dataframe\n",
    "\n",
    "def SplitFeaturesLabels(dataframe):\n",
    "    x = dataframe.iloc[:, 0:9]\n",
    "    y = dataframe.iloc[:, 9:18]\n",
    "    return x, y\n",
    "\n",
    "#Load data into Numpy array\n",
    "def LoadArray(x, y):\n",
    "    x_array = np.array(x)\n",
    "    y_array = np.array(y)\n",
    "    return x_array, y_array\n",
    "\n",
    "\n",
    "def NormalizeValues (x, y):\n",
    "    x =  np.divide(x, 100)\n",
    "    y =  np.divide(y, 100)\n",
    "    return x, y\n",
    "\n",
    "def SampleSize(x, y):\n",
    "    x_samples = x[0:900,:]\n",
    "    y_samples = y[0:900,:]\n",
    "    return x_samples, y_samples\n",
    "\n",
    "    \n",
    "    \n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Input File Name walking_1_train\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataframe created\n",
      "(900, 9) [[ 0.00396347  1.55321396 -0.09726781 ... -0.17044104  0.79216263\n",
      "  -0.12728346]\n",
      " [ 0.00396347  1.55321396 -0.0974095  ... -0.1698889   0.79222771\n",
      "  -0.12750142]\n",
      " [ 0.00419568  1.55321396 -0.09760312 ... -0.16916578  0.79246605\n",
      "  -0.12798414]\n",
      " ...\n",
      " [-0.02651245  1.59055893 -0.13678305 ... -0.11887306  0.75262321\n",
      "   0.13336996]\n",
      " [-0.03070768  1.59075287 -0.13581562 ... -0.13185926  0.75569618\n",
      "   0.13421907]\n",
      " [-0.03509516  1.5909549  -0.13504914 ... -0.14492013  0.75884453\n",
      "   0.13558317]]\n",
      "(900, 9) [[ 0.03550047  0.99325623 -0.06203985 ... -0.17026985  0.09899879\n",
      "   0.01698667]\n",
      " [ 0.03546809  0.99353798 -0.06230913 ... -0.17021811  0.09892341\n",
      "   0.01698667]\n",
      " [ 0.0358079   0.99352867 -0.06240509 ... -0.17014109  0.09882551\n",
      "   0.01698667]\n",
      " ...\n",
      " [-0.07211315  1.00282028 -0.1262004  ... -0.08610182  0.09165998\n",
      "   0.05806892]\n",
      " [-0.07784638  1.00439491 -0.1233117  ... -0.08573814  0.09138713\n",
      "   0.05814877]\n",
      " [-0.08183085  1.0055632  -0.12043599 ... -0.08560102  0.09083523\n",
      "   0.05812021]]\n"
     ]
    }
   ],
   "source": [
    "    \n",
    "#load train data from csv\n",
    "train_dataframe = ReadCSV()\n",
    "\n",
    "#split features and labels into seperate dataframes\n",
    "x_train_df, y_train_df = SplitFeaturesLabels(train_dataframe)\n",
    "\n",
    "#convert features and labels to numpy array\n",
    "x_train, y_train = LoadArray(x_train_df, y_train_df)\n",
    "\n",
    "#Divide values in array by 100\n",
    "x_train_normalized, y_train_normalized = NormalizeValues(x_train, y_train)\n",
    "\n",
    "\n",
    "x_samples, y_samples = SampleSize(x_train_normalized, y_train_normalized)\n",
    "\n",
    "print(x_samples.shape, x_samples)\n",
    "print(y_samples.shape, y_samples)\n",
    "\n",
    "#x_train, x_test, y_train, y_test = train_test_split(x_train_normalized, y_train_normalized)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reshape Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def ReshapeData(x, y):\n",
    "    x_reshaped = np.expand_dims(x, axis=1)\n",
    "    y_reshaped = np.expand_dims(y, axis=1)\n",
    "\n",
    "    return x_reshaped, y_reshaped\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(900, 1, 9) (900, 1, 9)\n",
      "1\n",
      "9\n"
     ]
    }
   ],
   "source": [
    "x_train, y_train = ReshapeData(x_samples, y_samples)\n",
    "\n",
    "print(x_train.shape, y_train.shape)\n",
    "\n",
    "print(x_train.shape[1])\n",
    "\n",
    "print(x_train.shape[2])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Test / Validation Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Input File Name walking_2_test\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataframe created\n",
      "(900, 1, 9) (900, 1, 9)\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "test_dataframe = ReadCSV()\n",
    "\n",
    "#split features and labels into seperate dataframes\n",
    "x_test_df, y_test_df = SplitFeaturesLabels(test_dataframe)\n",
    "\n",
    "#convert features and labels to numpy array\n",
    "x_test, y_test = LoadArray(x_test_df, y_test_df)\n",
    "\n",
    "#Divide values in array by 100\n",
    "x_test_normalized, y_test_normalized = NormalizeValues(x_test, y_test)\n",
    "\n",
    "x_test_samples, y_test_samples = SampleSize(x_test_normalized, y_test_normalized)\n",
    "\n",
    "x_test, y_test = ReshapeData(x_test_samples, y_test_samples)\n",
    "\n",
    "print(x_test.shape, y_test.shape)\n",
    "\n",
    "print(x_test.shape[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Creation and Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model compiled\n",
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " gru_2 (GRU)                 (None, 1, 9)              540       \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 1, 9)              0         \n",
      "                                                                 \n",
      " gru_3 (GRU)                 (None, 9)                 540       \n",
      "                                                                 \n",
      " dropout_3 (Dropout)         (None, 9)                 0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 9)                 90        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,170\n",
      "Trainable params: 1,170\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Flatten\n",
    "import tensorflow as tf\n",
    "from keras.layers import LSTM, GRU\n",
    "\n",
    "from keras.layers.core import Dense, Activation, Dropout\n",
    "\n",
    "tf.keras.backend.set_floatx('float64')\n",
    "\n",
    "\n",
    "model = Sequential()\n",
    "model.add(GRU(9, return_sequences=True, input_shape=(x_train.shape[1],x_train.shape[2])))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(GRU(9, return_sequences=False))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(9, activation = \"linear\"))\n",
    "\n",
    "model.compile(loss='mse', optimizer='adam')\n",
    "\n",
    "print ('model compiled')\n",
    "\n",
    "print (model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "90/90 [==============================] - 0s 4ms/step - loss: 0.0797\n",
      "Epoch 2/30\n",
      "90/90 [==============================] - 0s 4ms/step - loss: 0.0797\n",
      "Epoch 3/30\n",
      "90/90 [==============================] - 0s 4ms/step - loss: 0.0793\n",
      "Epoch 4/30\n",
      "90/90 [==============================] - 0s 4ms/step - loss: 0.0793\n",
      "Epoch 5/30\n",
      "90/90 [==============================] - 0s 4ms/step - loss: 0.0788\n",
      "Epoch 6/30\n",
      "90/90 [==============================] - 0s 4ms/step - loss: 0.0789\n",
      "Epoch 7/30\n",
      "90/90 [==============================] - 0s 5ms/step - loss: 0.0785\n",
      "Epoch 8/30\n",
      "90/90 [==============================] - 0s 4ms/step - loss: 0.0786\n",
      "Epoch 9/30\n",
      "90/90 [==============================] - 0s 4ms/step - loss: 0.0785\n",
      "Epoch 10/30\n",
      "90/90 [==============================] - 0s 4ms/step - loss: 0.0783\n",
      "Epoch 11/30\n",
      "90/90 [==============================] - 0s 4ms/step - loss: 0.0781\n",
      "Epoch 12/30\n",
      "90/90 [==============================] - 0s 4ms/step - loss: 0.0780\n",
      "Epoch 13/30\n",
      "90/90 [==============================] - 0s 4ms/step - loss: 0.0778\n",
      "Epoch 14/30\n",
      "90/90 [==============================] - 0s 4ms/step - loss: 0.0778\n",
      "Epoch 15/30\n",
      "90/90 [==============================] - 0s 4ms/step - loss: 0.0779\n",
      "Epoch 16/30\n",
      "90/90 [==============================] - 0s 4ms/step - loss: 0.0778\n",
      "Epoch 17/30\n",
      "90/90 [==============================] - 0s 4ms/step - loss: 0.0777\n",
      "Epoch 18/30\n",
      "90/90 [==============================] - 0s 5ms/step - loss: 0.0774\n",
      "Epoch 19/30\n",
      "90/90 [==============================] - 0s 4ms/step - loss: 0.0774\n",
      "Epoch 20/30\n",
      "90/90 [==============================] - 0s 5ms/step - loss: 0.0774\n",
      "Epoch 21/30\n",
      "90/90 [==============================] - 0s 4ms/step - loss: 0.0773\n",
      "Epoch 22/30\n",
      "90/90 [==============================] - 0s 4ms/step - loss: 0.0773\n",
      "Epoch 23/30\n",
      "90/90 [==============================] - 0s 4ms/step - loss: 0.0772\n",
      "Epoch 24/30\n",
      "90/90 [==============================] - 0s 4ms/step - loss: 0.0774\n",
      "Epoch 25/30\n",
      "90/90 [==============================] - 0s 4ms/step - loss: 0.0773\n",
      "Epoch 26/30\n",
      "90/90 [==============================] - 0s 4ms/step - loss: 0.0772\n",
      "Epoch 27/30\n",
      "90/90 [==============================] - 0s 4ms/step - loss: 0.0771\n",
      "Epoch 28/30\n",
      "90/90 [==============================] - 0s 4ms/step - loss: 0.0770\n",
      "Epoch 29/30\n",
      "90/90 [==============================] - 0s 4ms/step - loss: 0.0772\n",
      "Epoch 30/30\n",
      "90/90 [==============================] - 0s 4ms/step - loss: 0.0770\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2aba978b100>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "model.fit(x_train, y_train, epochs=30,batch_size=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29/29 [==============================] - 1s 2ms/step - loss: 0.0918\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.0917500489690446"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(x_test, y_test, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 16ms/step\n"
     ]
    }
   ],
   "source": [
    "predictions = model.predict(x_test[:1,])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predictions shape: (1, 9)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Must pass 2-d input. shape=(900, 1, 9)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[1;32mIn [31]\u001b[0m, in \u001b[0;36m<cell line: 4>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpredictions shape:\u001b[39m\u001b[38;5;124m\"\u001b[39m, predictions\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m      3\u001b[0m prediction_DF \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(predictions, columns\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWaist_X\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWaist_Y\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWaist_Z\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRigth_Foot_X\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRight_Foot_Y\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRight_Foot_Z\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLeft_Foot_X\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLeft_Foot_Y\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLeft_Foot_Z\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m----> 4\u001b[0m actual_DF \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mDataFrame\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mWaist_X\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mWaist_Y\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mWaist_Z\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mRigth_Foot_X\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mRight_Foot_Y\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mRight_Foot_Z\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mLeft_Foot_X\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mLeft_Foot_Y\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mLeft_Foot_Z\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      6\u001b[0m display(prediction_DF)\n\u001b[0;32m      7\u001b[0m display(actual_DF[:\u001b[38;5;241m1\u001b[39m,:,:])\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\frame.py:694\u001b[0m, in \u001b[0;36mDataFrame.__init__\u001b[1;34m(self, data, index, columns, dtype, copy)\u001b[0m\n\u001b[0;32m    684\u001b[0m         mgr \u001b[38;5;241m=\u001b[39m dict_to_mgr(\n\u001b[0;32m    685\u001b[0m             \u001b[38;5;66;03m# error: Item \"ndarray\" of \"Union[ndarray, Series, Index]\" has no\u001b[39;00m\n\u001b[0;32m    686\u001b[0m             \u001b[38;5;66;03m# attribute \"name\"\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    691\u001b[0m             typ\u001b[38;5;241m=\u001b[39mmanager,\n\u001b[0;32m    692\u001b[0m         )\n\u001b[0;32m    693\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 694\u001b[0m         mgr \u001b[38;5;241m=\u001b[39m \u001b[43mndarray_to_mgr\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    695\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    696\u001b[0m \u001b[43m            \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    697\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    698\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    699\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    700\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtyp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmanager\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    701\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    703\u001b[0m \u001b[38;5;66;03m# For data is list-like, or Iterable (will consume into list)\u001b[39;00m\n\u001b[0;32m    704\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m is_list_like(data):\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\internals\\construction.py:331\u001b[0m, in \u001b[0;36mndarray_to_mgr\u001b[1;34m(values, index, columns, dtype, copy, typ)\u001b[0m\n\u001b[0;32m    326\u001b[0m         values \u001b[38;5;241m=\u001b[39m values\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m    328\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    329\u001b[0m     \u001b[38;5;66;03m# by definition an array here\u001b[39;00m\n\u001b[0;32m    330\u001b[0m     \u001b[38;5;66;03m# the dtypes will be coerced to a single dtype\u001b[39;00m\n\u001b[1;32m--> 331\u001b[0m     values \u001b[38;5;241m=\u001b[39m \u001b[43m_prep_ndarray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy_on_sanitize\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    333\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m dtype \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_dtype_equal(values\u001b[38;5;241m.\u001b[39mdtype, dtype):\n\u001b[0;32m    334\u001b[0m     shape \u001b[38;5;241m=\u001b[39m values\u001b[38;5;241m.\u001b[39mshape\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\internals\\construction.py:591\u001b[0m, in \u001b[0;36m_prep_ndarray\u001b[1;34m(values, copy)\u001b[0m\n\u001b[0;32m    589\u001b[0m     values \u001b[38;5;241m=\u001b[39m values\u001b[38;5;241m.\u001b[39mreshape((values\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;241m1\u001b[39m))\n\u001b[0;32m    590\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m values\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m2\u001b[39m:\n\u001b[1;32m--> 591\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMust pass 2-d input. shape=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mvalues\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    593\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m values\n",
      "\u001b[1;31mValueError\u001b[0m: Must pass 2-d input. shape=(900, 1, 9)"
     ]
    }
   ],
   "source": [
    "print(\"predictions shape:\", predictions.shape)\n",
    "\n",
    "prediction_DF = pd.DataFrame(predictions, columns=[\"Waist_X\", \"Waist_Y\", \"Waist_Z\", \"Rigth_Foot_X\", \"Right_Foot_Y\", \"Right_Foot_Z\", \"Left_Foot_X\", \"Left_Foot_Y\", \"Left_Foot_Z\"])\n",
    "\n",
    "display(prediction_DF)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "vscode": {
   "interpreter": {
    "hash": "85eeaad1f84315a0c9c5600e08c8d22c182ff5487c2ae8eda931033c2f461801"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
