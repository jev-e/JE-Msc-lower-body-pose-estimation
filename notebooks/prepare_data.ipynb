{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Prepare Data From CSV Recording\n",
    "\n",
    "##Load File\n",
    "\n",
    "'Data is loaded from a CSV recording file, accepted through an input prompt. This includes all positional data related to the 6 trackers (HMD, Left Controller, Right Controller, Waist, Left Foot, Right Foot).'\n",
    "\n",
    "'Data is loaded into a Pandas dataframe. The primary tracking data is then extracted, leaving extraneous data such as booleans for button presses.'\n",
    "\n",
    "'The extracted columns are then concatenated into a new dataframe, and the columns are renamed for ease of reading.' \n",
    "\n",
    "'The new trimmed file is written to a directory (/trim_output), for further manipulation and loading into the model.'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Input File Name l\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error Reading File: Check Spelling and Try Again\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'dataframe' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [1]\u001b[0m, in \u001b[0;36m<cell line: 16>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     13\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError Reading File: Check Spelling and Try Again\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     15\u001b[0m \u001b[38;5;66;03m# Seperate each tracker to seperate dataframe\u001b[39;00m\n\u001b[1;32m---> 16\u001b[0m HMD \u001b[38;5;241m=\u001b[39m \u001b[43mdataframe\u001b[49m\u001b[38;5;241m.\u001b[39miloc[:, \u001b[38;5;241m1\u001b[39m:\u001b[38;5;241m4\u001b[39m]\n\u001b[0;32m     17\u001b[0m controller_1 \u001b[38;5;241m=\u001b[39m dataframe\u001b[38;5;241m.\u001b[39miloc[:, \u001b[38;5;241m7\u001b[39m:\u001b[38;5;241m10\u001b[39m]\n\u001b[0;32m     18\u001b[0m controller_2 \u001b[38;5;241m=\u001b[39m dataframe\u001b[38;5;241m.\u001b[39miloc[:, \u001b[38;5;241m37\u001b[39m:\u001b[38;5;241m40\u001b[39m]\n",
      "\u001b[1;31mNameError\u001b[0m: name 'dataframe' is not defined"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "import pandas as pd\n",
    "\n",
    "recording_path = \"../recordings/\"\n",
    "output_path = \"../trim_output/\"\n",
    "file_name = input(\"Input File Name\")\n",
    "\n",
    "#Read in CSV\n",
    "\n",
    "try:\n",
    "    dataframe = pd.read_csv(recording_path + file_name + \".csv\")\n",
    "except:\n",
    "    print(\"Error Reading File: Check Spelling and Try Again\")\n",
    "\n",
    "# Seperate each tracker to seperate dataframe\n",
    "HMD = dataframe.iloc[:, 1:4]\n",
    "controller_1 = dataframe.iloc[:, 7:10]\n",
    "controller_2 = dataframe.iloc[:, 37:40]\n",
    "left_foot = dataframe.iloc[:, 67:70]\n",
    "right_foot = dataframe.iloc[:, 73:76]\n",
    "waist = dataframe.iloc[:, 79:82]\n",
    "\n",
    "# Join all trackers together\n",
    "joined = pd.concat([HMD, controller_1, controller_2,\n",
    "                   waist, left_foot, right_foot], axis=1)\n",
    "\n",
    "# set new column headers\n",
    "joined.columns = [\n",
    "    \"head_x\",\n",
    "    \"head_y\",\n",
    "    \"head_z\",\n",
    "    \"r_controller_x\",\n",
    "    \"r_controller_y\",\n",
    "    \"r_controller_z\",\n",
    "    \"l_controller_x\",\n",
    "    \"l_controller_y\",\n",
    "    \"l_controller_z\",\n",
    "    \"waist_x\",\n",
    "    \"waist_y\",\n",
    "    \"waist_z\",\n",
    "    \"r_foot_x\",\n",
    "    \"r_foot_y\",\n",
    "    \"r_foot_z\",\n",
    "    \"l_foot_x\",\n",
    "    \"l_foot_y\",\n",
    "    \"l_foot_z\"\n",
    "]\n",
    "\n",
    "\n",
    "# output to new csv\n",
    "output_file = output_path + file_name + \"_trimmed.csv\"\n",
    "joined.to_csv(output_file, index=False)\n",
    "\n",
    "print(file_name + \" output to \" + output_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data Normalization\n",
    "\n",
    "## Data Scaling\n",
    "\n",
    "'The new CSV is loaded into memory, chosen through an input prompt'\n",
    "'The data is then split between the features (the HMD and controller tracking data), and the labels (the waist and foot trackers).'\n",
    "'These are loaded into Numpy arrays to peform normaliztion. The output from OpenVR Recorder is upscaled by 100. To correct this the array is divided by 100'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Input File Name walking_1_train\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataframe created\n",
      "(972, 9) [[ 0.00396347  1.55321396 -0.09726781 ... -0.17044104  0.79216263\n",
      "  -0.12728346]\n",
      " [ 0.00396347  1.55321396 -0.0974095  ... -0.1698889   0.79222771\n",
      "  -0.12750142]\n",
      " [ 0.00419568  1.55321396 -0.09760312 ... -0.16916578  0.79246605\n",
      "  -0.12798414]\n",
      " ...\n",
      " [-0.07326494  1.57561111 -0.15563388 ... -0.28583529  0.76833069\n",
      "   0.04462518]\n",
      " [-0.07258665  1.57578445 -0.15483627 ... -0.28606691  0.76840103\n",
      "   0.04523792]\n",
      " [-0.07203475  1.57610062 -0.15404955 ... -0.28627296  0.76849533\n",
      "   0.04572279]]\n",
      "(972, 9) [[ 0.03550047  0.99325623 -0.06203985 ... -0.17026985  0.09899879\n",
      "   0.01698667]\n",
      " [ 0.03546809  0.99353798 -0.06230913 ... -0.17021811  0.09892341\n",
      "   0.01698667]\n",
      " [ 0.0358079   0.99352867 -0.06240509 ... -0.17014109  0.09882551\n",
      "   0.01698667]\n",
      " ...\n",
      " [-0.13245131  1.00056854 -0.13201273 ... -0.08991505  0.09783401\n",
      "   0.05129308]\n",
      " [-0.13226092  1.00068802 -0.13195515 ... -0.09031304  0.09860064\n",
      "   0.05091204]\n",
      " [-0.13171673  1.00074806 -0.13161671 ... -0.08997025  0.09833285\n",
      "   0.05118186]]\n"
     ]
    }
   ],
   "source": [
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Flatten\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "#from sklearn.metrics import mean_absolute_error \n",
    "from matplotlib import pyplot as plt\n",
    "#import seaborn as sb\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "output_path = \"../trim_output/\"\n",
    "\n",
    "file_name = input(\"Input File Name\")\n",
    "\n",
    "#read in formatted CSV\n",
    "try:\n",
    "    dataframe = pd.read_csv(output_path + file_name + \".csv\")\n",
    "    print(\"Dataframe created\")\n",
    "except:\n",
    "    print(\"Error Reading File\")\n",
    "\n",
    "\n",
    "#Show shape of the dataframe\n",
    "dataframe.describe()\n",
    "\n",
    "x_train_df = dataframe.iloc[:, 0:9]\n",
    "y_train_df = dataframe.iloc[:, 9:18]\n",
    "#display(x_train_df)\n",
    "#display(y_train_df)\n",
    "\n",
    "#Load data into Numpy array\n",
    "x_train = np.array(x_train_df)\n",
    "y_train = np.array(y_train_df)\n",
    "\n",
    "\n",
    "\n",
    "#Divide values in array by 100\n",
    "x_train_normalized = np.divide(x_train, 100)\n",
    "y_train_normalized = np.divide(y_train, 100)\n",
    "\n",
    "print(x_train_normalized.shape, x_train_normalized)\n",
    "print(y_train_normalized.shape, y_train_normalized)\n",
    "\n",
    "#x_train, x_test, y_train, y_test = train_test_split(x_train_normalized, y_train_normalized)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 972, 9)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[[ 0.00396347,  1.55321396, -0.09726781, ..., -0.17044104,\n",
       "          0.79216263, -0.12728346],\n",
       "        [ 0.00396347,  1.55321396, -0.0974095 , ..., -0.1698889 ,\n",
       "          0.79222771, -0.12750142],\n",
       "        [ 0.00419568,  1.55321396, -0.09760312, ..., -0.16916578,\n",
       "          0.79246605, -0.12798414],\n",
       "        ...,\n",
       "        [-0.07326494,  1.57561111, -0.15563388, ..., -0.28583529,\n",
       "          0.76833069,  0.04462518],\n",
       "        [-0.07258665,  1.57578445, -0.15483627, ..., -0.28606691,\n",
       "          0.76840103,  0.04523792],\n",
       "        [-0.07203475,  1.57610062, -0.15404955, ..., -0.28627296,\n",
       "          0.76849533,  0.04572279]]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "x_train_normalized.shape\n",
    "\n",
    "x_train_reshaped = np.expand_dims(x_train_normalized, axis=0)\n",
    "print(x_train_reshaped.shape)\n",
    "display(x_train_reshaped)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Model Creation and Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_3 (Embedding)     (None, 9, 16)             14480     \n",
      "                                                                 \n",
      " bidirectional_2 (Bidirectio  (None, 64)               9600      \n",
      " nal)                                                            \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 10)                650       \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 1)                 11        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 24,741\n",
      "Trainable params: 24,741\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(input_dim=len(x_train_normalized), output_dim= 16, input_length=9 ),\n",
    "    tf.keras.layers.Bidirectional(tf.keras.layers.GRU(32)),\n",
    "    tf.keras.layers.Dense(10, activation='relu'),\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "model.summary()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    File \"C:\\Users\\Jev\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\engine\\training.py\", line 1021, in train_function  *\n        return step_function(self, iterator)\n    File \"C:\\Users\\Jev\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\engine\\training.py\", line 1010, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"C:\\Users\\Jev\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\engine\\training.py\", line 1000, in run_step  **\n        outputs = model.train_step(data)\n    File \"C:\\Users\\Jev\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\engine\\training.py\", line 860, in train_step\n        loss = self.compute_loss(x, y, y_pred, sample_weight)\n    File \"C:\\Users\\Jev\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\engine\\training.py\", line 918, in compute_loss\n        return self.compiled_loss(\n    File \"C:\\Users\\Jev\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\engine\\compile_utils.py\", line 201, in __call__\n        loss_value = loss_obj(y_t, y_p, sample_weight=sw)\n    File \"C:\\Users\\Jev\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\losses.py\", line 141, in __call__\n        losses = call_fn(y_true, y_pred)\n    File \"C:\\Users\\Jev\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\losses.py\", line 245, in call  **\n        return ag_fn(y_true, y_pred, **self._fn_kwargs)\n    File \"C:\\Users\\Jev\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\losses.py\", line 1932, in binary_crossentropy\n        backend.binary_crossentropy(y_true, y_pred, from_logits=from_logits),\n    File \"C:\\Users\\Jev\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\backend.py\", line 5247, in binary_crossentropy\n        return tf.nn.sigmoid_cross_entropy_with_logits(labels=target, logits=output)\n\n    ValueError: `logits` and `labels` must have the same shape, received ((None, 1) vs (None, 9)).\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[1;32mIn [19]\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# fit the model\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_train_normalized\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train_normalized\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# evaluate the model\u001b[39;00m\n\u001b[0;32m      4\u001b[0m loss, accuracy \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mevaluate(x_train_normalized, y_train_normalized, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\utils\\traceback_utils.py:67\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     65\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# pylint: disable=broad-except\u001b[39;00m\n\u001b[0;32m     66\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m---> 67\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[0;32m     68\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     69\u001b[0m   \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\framework\\func_graph.py:1147\u001b[0m, in \u001b[0;36mfunc_graph_from_py_func.<locals>.autograph_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m   1145\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# pylint:disable=broad-except\u001b[39;00m\n\u001b[0;32m   1146\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(e, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mag_error_metadata\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m-> 1147\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mag_error_metadata\u001b[38;5;241m.\u001b[39mto_exception(e)\n\u001b[0;32m   1148\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1149\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[1;31mValueError\u001b[0m: in user code:\n\n    File \"C:\\Users\\Jev\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\engine\\training.py\", line 1021, in train_function  *\n        return step_function(self, iterator)\n    File \"C:\\Users\\Jev\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\engine\\training.py\", line 1010, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"C:\\Users\\Jev\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\engine\\training.py\", line 1000, in run_step  **\n        outputs = model.train_step(data)\n    File \"C:\\Users\\Jev\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\engine\\training.py\", line 860, in train_step\n        loss = self.compute_loss(x, y, y_pred, sample_weight)\n    File \"C:\\Users\\Jev\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\engine\\training.py\", line 918, in compute_loss\n        return self.compiled_loss(\n    File \"C:\\Users\\Jev\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\engine\\compile_utils.py\", line 201, in __call__\n        loss_value = loss_obj(y_t, y_p, sample_weight=sw)\n    File \"C:\\Users\\Jev\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\losses.py\", line 141, in __call__\n        losses = call_fn(y_true, y_pred)\n    File \"C:\\Users\\Jev\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\losses.py\", line 245, in call  **\n        return ag_fn(y_true, y_pred, **self._fn_kwargs)\n    File \"C:\\Users\\Jev\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\losses.py\", line 1932, in binary_crossentropy\n        backend.binary_crossentropy(y_true, y_pred, from_logits=from_logits),\n    File \"C:\\Users\\Jev\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\backend.py\", line 5247, in binary_crossentropy\n        return tf.nn.sigmoid_cross_entropy_with_logits(labels=target, logits=output)\n\n    ValueError: `logits` and `labels` must have the same shape, received ((None, 1) vs (None, 9)).\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# fit the model\n",
    "model.fit(x_train_normalized, y_train_normalized, epochs=50, verbose=0)\n",
    "# evaluate the model\n",
    "loss, accuracy = model.evaluate(x_train_normalized, y_train_normalized, verbose=0)\n",
    "print('Accuracy: %f' % (accuracy*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "vscode": {
   "interpreter": {
    "hash": "85eeaad1f84315a0c9c5600e08c8d22c182ff5487c2ae8eda931033c2f461801"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
