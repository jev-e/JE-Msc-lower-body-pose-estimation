{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare Data From CSV Recording\n",
    "\n",
    "## Load File\n",
    "\n",
    "Data is loaded from a CSV recording file, accepted through an input prompt. This includes all positional data related to the 6 trackers (HMD, Left Controller, Right Controller, Waist, Left Foot, Right Foot).\n",
    "\n",
    "'Data is loaded into a Pandas dataframe. The primary tracking data is then extracted, leaving extraneous data such as booleans for button presses.\n",
    "\n",
    "The extracted columns are then concatenated into a new dataframe, and the columns are renamed for ease of reading.\n",
    "\n",
    "The columns are reorded in the order of head/r_controller/l_controller/waist/r_foot/l_foot.\n",
    "\n",
    "The new trimmed file is written to a directory (/test_data or /train_data), for further manipulation and loading into the model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "#Read in CSV\n",
    "def GetRecording(path):\n",
    "    recording_path = \"../recordings/\"\n",
    "    file_name = input(\"Input Recording File Name\")\n",
    "    try:\n",
    "        dataframe = pd.read_csv(recording_path + file_name + \".csv\")\n",
    "        return dataframe, file_name\n",
    "    except: \n",
    "        print(\"Error Reading File: Check Spelling and Try Again\")\n",
    "        return 0\n",
    "    \n",
    "    \n",
    "#Seperate each tracker to seperate dataframe\n",
    "\n",
    "def GetColByName(dataframe):\n",
    "    HMD = dataframe.loc[:, [\"HMD0_tx\", \"HMD0_ty\", \"HMD0_tz\"]]\n",
    "    \n",
    "    controller_1 = dataframe.loc[:, ['controller3_tx', 'controller3_ty', 'controller3_tz']]\n",
    "\n",
    "    controller_2 = dataframe.loc[:, ['controller4_tx', 'controller4_ty', 'controller4_tz']]\n",
    "\n",
    "    tracker_1 = dataframe.loc[:, ['generic7_tx', 'generic7_ty', 'generic7_tz']]\n",
    "\n",
    "    tracker_2 = dataframe.loc[:, ['generic8_tx', 'generic8_ty', 'generic8_tz']]\n",
    "\n",
    "    tracker_3 = dataframe.loc[:, ['generic9_tx', 'generic9_ty', 'generic9_tz']]\n",
    "\n",
    "    joined = pd.concat([HMD,controller_1, controller_2, tracker_1 ,tracker_2 ,tracker_3], axis=1)\n",
    "    return joined\n",
    "\n",
    "def AssignTracker(dataframe):\n",
    "    display(dataframe.iloc[0:1,:])\n",
    "    trackerNum = 7\n",
    "    for x in range(3):\n",
    "        trackerStr = str(trackerNum)\n",
    "        tracker = input('assign generic' + trackerStr)\n",
    "        dataframe.rename(columns={'generic' + trackerStr + '_tx': tracker + '_x', 'generic' + trackerStr + '_ty': tracker + \"_y\", 'generic' + trackerStr + '_tz': tracker + '_z'}, inplace=True)\n",
    "        trackerNum += 1\n",
    "        \n",
    "    controllerNum = 3\n",
    "    for x in range(2):\n",
    "        controllerStr = str(controllerNum)\n",
    "        controller = input('assign controller' + controllerStr)\n",
    "        dataframe.rename(columns={'controller' + controllerStr + '_tx': controller + '_x', 'controller' + controllerStr + '_ty': controller + \"_y\", 'controller' + controllerStr + '_tz': controller + '_z'}, inplace=True)\n",
    "        controllerNum += 1\n",
    "    dataframe.rename(columns={'HMD0_tx': 'head_x', 'HMD0_ty': 'head_y', 'HMD0_tz': 'head_z'}, inplace=True)\n",
    "    return dataframe\n",
    "\n",
    "def GetDirectory():\n",
    "    choice = input(\"train or test data:\")\n",
    "    if choice == \"test\":\n",
    "        output_path = \"../test_data/\"\n",
    "    else:\n",
    "        output_path = \"../train_data/\"\n",
    "    return output_path\n",
    "\n",
    "def OrderFeatures(dataframe):\n",
    "    head = dataframe.loc[:, ['head_x', 'head_y', 'head_z']]\n",
    "    l_controller = dataframe.loc[:, ['l_controller_x', 'l_controller_y', 'l_controller_z']]\n",
    "    r_controller = dataframe.loc[:, ['r_controller_x', 'r_controller_y', 'r_controller_z']]\n",
    "    waist = dataframe.loc[:, ['waist_x', 'waist_y', 'waist_z']]\n",
    "    r_foot = dataframe.loc[:, ['r_foot_x', 'r_foot_y', 'r_foot_z']]\n",
    "    l_foot = dataframe.loc[:, ['l_foot_x', 'l_foot_y', 'l_foot_z']]\n",
    "    reordered = pd.concat([head , r_controller, l_controller, waist, r_foot, l_foot], axis=1)\n",
    "    return reordered\n",
    "\n",
    "    \n",
    "    \n",
    "def WriteOutput(path, dataframe, filename):\n",
    "    output_file = path + filename + \"_trimmed.csv\"\n",
    "    dataframe.to_csv(output_file, index = False)\n",
    "    print(file_name + \" output to \" + path)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run data trimming functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Input Recording File Name leg_raise_5\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>HMD0_tx</th>\n",
       "      <th>HMD0_ty</th>\n",
       "      <th>HMD0_tz</th>\n",
       "      <th>controller3_tx</th>\n",
       "      <th>controller3_ty</th>\n",
       "      <th>controller3_tz</th>\n",
       "      <th>controller4_tx</th>\n",
       "      <th>controller4_ty</th>\n",
       "      <th>controller4_tz</th>\n",
       "      <th>generic7_tx</th>\n",
       "      <th>generic7_ty</th>\n",
       "      <th>generic7_tz</th>\n",
       "      <th>generic8_tx</th>\n",
       "      <th>generic8_ty</th>\n",
       "      <th>generic8_tz</th>\n",
       "      <th>generic9_tx</th>\n",
       "      <th>generic9_ty</th>\n",
       "      <th>generic9_tz</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.964998</td>\n",
       "      <td>158.131058</td>\n",
       "      <td>3.838474</td>\n",
       "      <td>-18.907917</td>\n",
       "      <td>92.860924</td>\n",
       "      <td>-9.813828</td>\n",
       "      <td>29.159588</td>\n",
       "      <td>80.373276</td>\n",
       "      <td>2.166218</td>\n",
       "      <td>3.899813</td>\n",
       "      <td>97.88607</td>\n",
       "      <td>6.474656</td>\n",
       "      <td>21.754217</td>\n",
       "      <td>11.036432</td>\n",
       "      <td>13.468838</td>\n",
       "      <td>-12.954676</td>\n",
       "      <td>11.488569</td>\n",
       "      <td>19.045788</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    HMD0_tx     HMD0_ty   HMD0_tz  controller3_tx  controller3_ty  \\\n",
       "0  1.964998  158.131058  3.838474      -18.907917       92.860924   \n",
       "\n",
       "   controller3_tz  controller4_tx  controller4_ty  controller4_tz  \\\n",
       "0       -9.813828       29.159588       80.373276        2.166218   \n",
       "\n",
       "   generic7_tx  generic7_ty  generic7_tz  generic8_tx  generic8_ty  \\\n",
       "0     3.899813     97.88607     6.474656    21.754217    11.036432   \n",
       "\n",
       "   generic8_tz  generic9_tx  generic9_ty  generic9_tz  \n",
       "0    13.468838   -12.954676    11.488569    19.045788  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "assign generic7 waist\n",
      "assign generic8 r_foot\n",
      "assign generic9 l_foot\n",
      "assign controller3 l_controller\n",
      "assign controller4 r_controller\n",
      "train or test data: test\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "leg_raise_5 output to ../test_data/\n"
     ]
    }
   ],
   "source": [
    "recording_path = \" ../recordings\"\n",
    "\n",
    "dataframe, file_name = GetRecording(recording_path)\n",
    "joined = GetColByName(dataframe)\n",
    "renamed = AssignTracker(joined)\n",
    "path = GetDirectory()\n",
    "reordered = OrderFeatures(renamed)\n",
    "WriteOutput(path, reordered, file_name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Normalization\n",
    "\n",
    "## Data Scaling\n",
    "\n",
    "The new CSV is loaded into memory, chosen through an input prompt\n",
    "The data is then split between the features (the HMD and controller tracking data), and the labels (the waist and foot trackers).\n",
    "These are loaded into Numpy arrays to peform normaliztion. The output from OpenVR Recorder is upscaled by 100. To correct this the array is divided by 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "#from sklearn.metrics import mean_absolute_error \n",
    "from matplotlib import pyplot as plt\n",
    "#import seaborn as sb\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "output_path = \"../trim_output/\"\n",
    "\n",
    "\n",
    "#read in formatted CSV\n",
    "def ReadCSV(path):\n",
    "    file_name = input(\"Input File Name\")\n",
    "    try:\n",
    "        dataframe = pd.read_csv(path + file_name + \".csv\")\n",
    "        print(\"Dataframe created\")\n",
    "    except:\n",
    "        print(\"Error Reading File\")\n",
    "    return dataframe\n",
    "\n",
    "def SplitFeaturesLabels(dataframe):\n",
    "    x = dataframe.iloc[:, 0:9]\n",
    "    y = dataframe.iloc[:, 9:18]\n",
    "    return x, y\n",
    "\n",
    "#Load data into Numpy array\n",
    "def LoadArray(x, y):\n",
    "    x_array = np.array(x)\n",
    "    y_array = np.array(y)\n",
    "    return x_array, y_array\n",
    "\n",
    "\n",
    "def NormalizeValues (x, y):\n",
    "    x =  np.divide(x, 100)\n",
    "    y =  np.divide(y, 100)\n",
    "    return x, y\n",
    "\n",
    "def SampleSize(x, y):\n",
    "    x_samples = x[0:600,:]\n",
    "    y_samples = y[0:600,:]\n",
    "    return x_samples, y_samples\n",
    "\n",
    "def RoundValues(x, y): \n",
    "    x_rounded = np.around(x, 3)\n",
    "    y_rounded = np.around(y, 3)\n",
    "    return x_rounded, y_rounded\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Input File Name walking_7_trimmed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataframe created\n",
      "(600, 9) [[0.56456936 1.65256363 0.82371849 ... 0.35004543 0.81725983 0.93927994]\n",
      " [0.56244183 1.6511261  0.82760582 ... 0.35082798 0.81521797 0.94517693]\n",
      " [0.56021255 1.64923187 0.8309227  ... 0.3510099  0.81342445 0.95069672]\n",
      " ...\n",
      " [0.42225151 1.55701126 0.6327589  ... 0.21533564 0.75101753 0.71076973]\n",
      " [0.41390022 1.55681183 0.62624306 ... 0.20336998 0.75672852 0.69413803]\n",
      " [0.40570499 1.55652176 0.6195657  ... 0.19236187 0.76251534 0.67823952]]\n",
      "(600, 9) [[0.56388847 0.9584938  0.82307373 ... 0.6701664  0.12568272 1.02205978]\n",
      " [0.5649939  0.95908966 0.8268927  ... 0.67017311 0.12555384 1.02190422]\n",
      " [0.5658884  0.95991432 0.83057884 ... 0.67164154 0.12405634 1.02306038]\n",
      " ...\n",
      " [0.4459425  0.94995354 0.64034004 ... 0.39146503 0.11636929 0.66756081]\n",
      " [0.43529736 0.9514679  0.63556534 ... 0.39199577 0.115468   0.66869583]\n",
      " [0.42703381 0.95304367 0.63249866 ... 0.39176373 0.11532265 0.66892319]]\n"
     ]
    }
   ],
   "source": [
    "train_path = \"../train_data/\"\n",
    "#load train data from csv\n",
    "train_dataframe = ReadCSV(train_path)\n",
    "\n",
    "#split features and labels into seperate dataframes\n",
    "x_train_df, y_train_df = SplitFeaturesLabels(train_dataframe)\n",
    "\n",
    "#convert features and labels to numpy array\n",
    "x_train, y_train = LoadArray(x_train_df, y_train_df)\n",
    "\n",
    "#Divide values in array by 100\n",
    "x_samples, y_samples = NormalizeValues(x_train, y_train)\n",
    "\n",
    "print(x_samples.shape, x_samples)\n",
    "print(y_samples.shape, y_samples)\n",
    "\n",
    "#x_train, x_test, y_train, y_test = train_test_split(x_train_normalized, y_train_normalized)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scale Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "print(x_samples.max())\n",
    "print(x_samples.min())\n",
    "\n",
    "print(y_samples.max())\n",
    "print(y_samples.min())\n",
    "\n",
    "scaler =MinMaxScaler()\n",
    "print(x_samples[0:1])\n",
    "scaled = scaler.fit(x_samples)\n",
    "print(scaler.transform(x_samples[0:1]))\n",
    "print(scaler.inverse_transform(x_samples[0:1]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reshape Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def ReshapeData(x, y):\n",
    "    x_reshaped = np.expand_dims(x, axis=1)\n",
    "    y_reshaped = np.expand_dims(y, axis=1)\n",
    "\n",
    "    return x_reshaped, y_reshaped\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(600, 1, 9) (600, 1, 9)\n",
      "1\n",
      "9\n"
     ]
    }
   ],
   "source": [
    "x_train, y_train = ReshapeData(x_samples, y_samples)\n",
    "\n",
    "print(x_train.shape, y_train.shape)\n",
    "\n",
    "print(x_train.shape[1])\n",
    "\n",
    "print(x_train.shape[2])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Test / Validation Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "Interrupted by user",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [42]\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m#Create a single test data file\u001b[39;00m\n\u001b[0;32m      3\u001b[0m test_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m../test_data/\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m----> 6\u001b[0m test_dataframe \u001b[38;5;241m=\u001b[39m \u001b[43mReadCSV\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m#split features and labels into seperate dataframes\u001b[39;00m\n\u001b[0;32m      9\u001b[0m x_test_df, y_test_df \u001b[38;5;241m=\u001b[39m SplitFeaturesLabels(test_dataframe)\n",
      "Input \u001b[1;32mIn [1]\u001b[0m, in \u001b[0;36mReadCSV\u001b[1;34m(path)\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mReadCSV\u001b[39m(path):\n\u001b[1;32m---> 16\u001b[0m     file_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43minput\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mInput File Name\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     17\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     18\u001b[0m         dataframe \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(path \u001b[38;5;241m+\u001b[39m file_name \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\ipykernel\\kernelbase.py:1044\u001b[0m, in \u001b[0;36mKernel.raw_input\u001b[1;34m(self, prompt)\u001b[0m\n\u001b[0;32m   1040\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_allow_stdin:\n\u001b[0;32m   1041\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m StdinNotImplementedError(\n\u001b[0;32m   1042\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraw_input was called, but this frontend does not support input requests.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1043\u001b[0m     )\n\u001b[1;32m-> 1044\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_input_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1045\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1046\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_parent_ident\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mshell\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1047\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_parent\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mshell\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1048\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpassword\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m   1049\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\ipykernel\\kernelbase.py:1089\u001b[0m, in \u001b[0;36mKernel._input_request\u001b[1;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[0;32m   1086\u001b[0m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m   1087\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m:\n\u001b[0;32m   1088\u001b[0m     \u001b[38;5;66;03m# re-raise KeyboardInterrupt, to truncate traceback\u001b[39;00m\n\u001b[1;32m-> 1089\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInterrupted by user\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[0;32m   1090\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[0;32m   1091\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog\u001b[38;5;241m.\u001b[39mwarning(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid Message:\u001b[39m\u001b[38;5;124m\"\u001b[39m, exc_info\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
     ]
    }
   ],
   "source": [
    "#Create a single test data file\n",
    "\n",
    "test_path = \"../test_data/\"\n",
    "\n",
    "\n",
    "test_dataframe = ReadCSV(test_path)\n",
    "\n",
    "#split features and labels into seperate dataframes\n",
    "x_test_df, y_test_df = SplitFeaturesLabels(test_dataframe)\n",
    "\n",
    "#convert features and labels to numpy array\n",
    "x_test, y_test = LoadArray(x_test_df, y_test_df)\n",
    "\n",
    "#Divide values in array by 100\n",
    "x_test_normalized, y_test_normalized = NormalizeValues(x_test, y_test)\n",
    "\n",
    "x_test_samples, y_test_samples = SampleSize(x_test_normalized, y_test_normalized)\n",
    "\n",
    "x_test, y_test = ReshapeData(x_test_samples, y_test_samples)\n",
    "\n",
    "print(x_test.shape, y_test.shape)\n",
    "\n",
    "print(x_test.shape[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combine Test and Train datasets\n",
    "\n",
    "Combine all data sets in /train_data and /test_data into one, for more samples when training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import modules \n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Combine all datasets in a directory into one dataframe\n",
    "def CombineDatasets(path):\n",
    "    data_list = []\n",
    "    for file in os.listdir(path):\n",
    "        filename = os.fsdecode(file)\n",
    "        if filename.endswith(\".csv\"):\n",
    "            df = pd.read_csv(path + filename)\n",
    "            \n",
    "            data_list.append(df)\n",
    "         \n",
    "\n",
    "    data_df = pd.concat(data_list, axis=0, ignore_index=True)\n",
    "    return data_df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#combine data in a directory into two lists of x and y features\n",
    "def DatasetsLists(path):\n",
    "    x_list = []\n",
    "    y_list = []\n",
    "    for file in os.listdir(path):\n",
    "        filename = os.fsdecode(file)\n",
    "        print(filename)\n",
    "        if filename.endswith(\".csv\"):\n",
    "            df = pd.read_csv(path + filename)\n",
    "            x_features, y_features = SplitFeaturesLabels(df)\n",
    "            x_train, y_train = LoadArray(x_features, y_features)\n",
    "            x_normalized, y_normalized = NormalizeValues(x_train, y_train)\n",
    "            x_reshape, y_reshape = ReshapeData(x_normalized, y_normalized)\n",
    "            x_list.append(x_reshape)\n",
    "            y_list.append(y_reshape)\n",
    "    return x_list, y_list\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".ipynb_checkpoints\n",
      "arm_raise_1_trimmed.csv\n",
      "arm_raise_3_trimmed.csv\n",
      "boxing_1_trimmed.csv\n",
      "crouch_walking_1_trimmed.csv\n",
      "jumping_1_trimmed.csv\n",
      "jumping_2_trimmed.csv\n",
      "jumping_3_trimmed.csv\n",
      "leg_raise_1_trimmed.csv\n",
      "leg_raise_2_trimmed.csv\n",
      "leg_raise_3_trimmed.csv\n",
      "leg_raise_4_trimmed.csv\n",
      "picking_up_1_trimmed.csv\n",
      "running_on_spot_2_trimmed.csv\n",
      "sitting_on_floor_1_trimmed.csv\n",
      "sitting_standing_1_trimmed.csv\n",
      "sitting_standing_2_trimmed.csv\n",
      "sitting_standing_3_trimmed.csv\n",
      "walking_1_train.csv\n",
      "walking_2_test.csv\n",
      "walking_3_trimmed.csv\n",
      "walking_5_trimmed.csv\n",
      "walking_6_trimmed.csv\n",
      "walking_7_trimmed.csv\n"
     ]
    }
   ],
   "source": [
    "train_path = \"../train_data/\"\n",
    "x, y = DatasetsLists(train_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combine all data in the training data directory "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(16108, 1, 9) (16108, 1, 9)\n"
     ]
    }
   ],
   "source": [
    "train_path = \"../train_data/\"\n",
    "\n",
    "combined_train_dataframe = CombineDatasets(train_path)\n",
    "\n",
    "x_train, y_train = SplitFeaturesLabels(combined_train_dataframe)\n",
    "        \n",
    "x_train_arr, y_train_arr = LoadArray(x_train, y_train)\n",
    "\n",
    "#Divide values in array by 100\n",
    "x_train_normalized, y_train_normalized = NormalizeValues(x_train_arr, y_train_arr)\n",
    "\n",
    "x_train, y_train = ReshapeData(x_train_normalized, y_train_normalized)\n",
    "\n",
    "print(x_train.shape, y_train.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combine all data in the test data directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5400, 1, 9)\n",
      "[[[ 0.07488835  1.60079437 -0.06116605  0.29804235  0.82701195\n",
      "   -0.13233584 -0.16509771  0.80914047 -0.10186529]]]\n",
      "(5400, 9)\n"
     ]
    }
   ],
   "source": [
    "test_path = \"../test_data/\"\n",
    "\n",
    "combined_test_dataframe = CombineDatasets(test_path)\n",
    "\n",
    "x_test, y_test = SplitFeaturesLabels(combined_test_dataframe)\n",
    "        \n",
    "x_test_arr, y_test_arr = LoadArray(x_test, y_test)\n",
    "\n",
    "x_test_normalized, y_test_normalized = NormalizeValues(x_test_arr, y_test_arr)\n",
    "\n",
    "x_test, y_test = ReshapeData(x_test_normalized, y_test_normalized)\n",
    "\n",
    "print(x_test.shape)\n",
    "print(x_test[0:1,:,:])\n",
    "print(x_test_normalized.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Creation and Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import required modules\n",
    "\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Flatten, BatchNormalization\n",
    "import tensorflow as tf\n",
    "from keras.layers import LSTM, GRU\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "from keras.layers.core import Dense, Activation, Dropout\n",
    "from keras.losses import SparseCategoricalCrossentropy\n",
    "\n",
    "tf.keras.backend.set_floatx('float64')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fit model to two equally sized lists of x features and y labels \n",
    "\n",
    "def FitToList(x, y, model, epoch, b_size, verbose):\n",
    "    for i in range(len(x)):\n",
    "        print(i)\n",
    "        print(x[i].shape)\n",
    "        print(y[i].shape)\n",
    "        model.fit(x[i], y[i], epochs=epoch,batch_size=b_size, verbose=verbose)\n",
    "        \n",
    " # fit the model to a given set of features (x) and labels (y)    \n",
    "def FitModel(x, y, model, epoch, b_size):\n",
    "    model.fit(x, y, validation_split=0.33, epochs=epoch,batch_size = b_size)\n",
    "\n",
    "    \n",
    "def EvaluateModel(x, y, model):\n",
    "    metrics = model.evaluate(x, y, batch_size=256)\n",
    "    \n",
    "    \n",
    "    \n",
    "def PredictModel(x, y, model):\n",
    "    predictions = model.predict(x)\n",
    "    y_reshaped = y.reshape(-1,9)\n",
    "    return predictions, y_reshaped\n",
    "\n",
    "\n",
    "def DisplayPredictions(prediction, actual, range_1, range_2):\n",
    "    if range_2 <= 0:\n",
    "        r_1 = 0\n",
    "        r_2 = len(prediction)\n",
    "    else:\n",
    "        r_1 = range_1\n",
    "        r_2 = range_2\n",
    "    print(\"predictions shape:\", prediction.shape)\n",
    "    prediction_df = pd.DataFrame(prediction, columns=[\"Waist_X\", \"Waist_Y\", \"Waist_Z\", \"R_Foot_X\", \"R_Foot_Y\", \"R_Foot_Z\", \"L_Foot_X\", \"L_Foot_Y\", \"L_Foot_Z\"])\n",
    "    actual_df = pd.DataFrame(actual, columns=[\"Waist_X\", \"Waist_Y\", \"Waist_Z\", \"R_Foot_X\", \"R_Foot_Y\", \"R_Foot_Z\", \"L_Foot_X\", \"L_Foot_Y\", \"L_Foot_Z\"])\n",
    "    print(\"Actual Values\")\n",
    "    display(actual_df[r_1:r_2])\n",
    "    print(\"Predicited Values\")\n",
    "    display(prediction_df[r_1:r_2])\n",
    "    return actual_df, prediction_df\n",
    "    \n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model compiled\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " gru (GRU)                   (None, 1, 16)             1296      \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 1, 16)             0         \n",
      "                                                                 \n",
      " gru_1 (GRU)                 (None, 16)                1632      \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 16)                0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 9)                 153       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 3,081\n",
      "Trainable params: 3,081\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "sgd = SGD(learning_rate=0.001, momentum=0.8, decay=0.999, nesterov=False)\n",
    "\n",
    "tf.keras.backend.set_floatx('float64')\n",
    "\n",
    "model = Sequential()\n",
    "model.add(GRU(16, return_sequences=True, input_shape=(5, 9)))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(GRU(16, return_sequences=False))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(9, activation = \"linear\"))\n",
    "\n",
    "model.compile(loss='mse', optimizer=\"adam\")\n",
    "\n",
    "print ('model compiled')\n",
    "\n",
    "print (model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_10\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " gru_12 (GRU)                (None, 64)                14400     \n",
      "                                                                 \n",
      " dense_7 (Dense)             (None, 9)                 585       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 14,985\n",
      "Trainable params: 14,985\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model_2 = Sequential()\n",
    "\n",
    "#add one GRU layer of 64 cellss with input shape 1,9\n",
    "model_2.add(GRU(64, input_shape=(1, 9)))\n",
    "#model_2.add(Dropout(0.5))\n",
    "\n",
    "#Add Batch Normalization layer\n",
    "#model_2.add(BatchNormalization())\n",
    "\n",
    "\n",
    "#Add dropout layer\n",
    "#model_2.add(Dropout(0.2))\n",
    "\n",
    "#Add Dense layer with 9 outputs\n",
    "model_2.add(Dense(9))\n",
    "\n",
    "print(model_2.summary())\n",
    "\n",
    "#Compile model\n",
    "model_2.compile(\n",
    "    loss=\"mse\",\n",
    "    optimizer=\"adam\",\n",
    "    metrics=[\"accuracy\"],\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "401/401 [==============================] - 2s 2ms/step - loss: 0.0430 - accuracy: 0.8404 - val_loss: 0.0129 - val_accuracy: 0.8191\n",
      "Epoch 2/200\n",
      "401/401 [==============================] - 1s 1ms/step - loss: 0.0073 - accuracy: 0.9127 - val_loss: 0.0125 - val_accuracy: 0.5628\n",
      "Epoch 3/200\n",
      "401/401 [==============================] - 1s 1ms/step - loss: 0.0068 - accuracy: 0.9177 - val_loss: 0.0104 - val_accuracy: 0.6231\n",
      "Epoch 4/200\n",
      "401/401 [==============================] - 1s 1ms/step - loss: 0.0064 - accuracy: 0.9377 - val_loss: 0.0099 - val_accuracy: 0.6382\n",
      "Epoch 5/200\n",
      "401/401 [==============================] - 0s 1ms/step - loss: 0.0059 - accuracy: 0.9501 - val_loss: 0.0103 - val_accuracy: 0.5729\n",
      "Epoch 6/200\n",
      "401/401 [==============================] - 0s 1ms/step - loss: 0.0055 - accuracy: 0.9426 - val_loss: 0.0095 - val_accuracy: 0.6583\n",
      "Epoch 7/200\n",
      "401/401 [==============================] - 0s 1ms/step - loss: 0.0054 - accuracy: 0.9476 - val_loss: 0.0095 - val_accuracy: 0.7035\n",
      "Epoch 8/200\n",
      "401/401 [==============================] - 0s 1ms/step - loss: 0.0051 - accuracy: 0.9401 - val_loss: 0.0108 - val_accuracy: 0.6482\n",
      "Epoch 9/200\n",
      "401/401 [==============================] - 0s 1ms/step - loss: 0.0046 - accuracy: 0.9551 - val_loss: 0.0124 - val_accuracy: 0.8342\n",
      "Epoch 10/200\n",
      "401/401 [==============================] - 0s 1ms/step - loss: 0.0044 - accuracy: 0.9526 - val_loss: 0.0112 - val_accuracy: 0.6131\n",
      "Epoch 11/200\n",
      "401/401 [==============================] - 0s 1ms/step - loss: 0.0041 - accuracy: 0.9626 - val_loss: 0.0122 - val_accuracy: 0.7437\n",
      "Epoch 12/200\n",
      "401/401 [==============================] - 0s 1ms/step - loss: 0.0037 - accuracy: 0.9601 - val_loss: 0.0161 - val_accuracy: 0.7186\n",
      "Epoch 13/200\n",
      "401/401 [==============================] - 0s 1ms/step - loss: 0.0036 - accuracy: 0.9501 - val_loss: 0.0147 - val_accuracy: 0.7538\n",
      "Epoch 14/200\n",
      "401/401 [==============================] - 0s 1ms/step - loss: 0.0035 - accuracy: 0.9576 - val_loss: 0.0170 - val_accuracy: 0.7035\n",
      "Epoch 15/200\n",
      "401/401 [==============================] - 0s 1ms/step - loss: 0.0033 - accuracy: 0.9526 - val_loss: 0.0217 - val_accuracy: 0.6734\n",
      "Epoch 16/200\n",
      "401/401 [==============================] - 0s 1ms/step - loss: 0.0032 - accuracy: 0.9551 - val_loss: 0.0218 - val_accuracy: 0.7588\n",
      "Epoch 17/200\n",
      "401/401 [==============================] - 0s 1ms/step - loss: 0.0032 - accuracy: 0.9626 - val_loss: 0.0173 - val_accuracy: 0.6784\n",
      "Epoch 18/200\n",
      "401/401 [==============================] - 0s 1ms/step - loss: 0.0030 - accuracy: 0.9476 - val_loss: 0.0187 - val_accuracy: 0.6533\n",
      "Epoch 19/200\n",
      "401/401 [==============================] - 0s 1ms/step - loss: 0.0028 - accuracy: 0.9601 - val_loss: 0.0160 - val_accuracy: 0.6935\n",
      "Epoch 20/200\n",
      "401/401 [==============================] - 0s 1ms/step - loss: 0.0029 - accuracy: 0.9526 - val_loss: 0.0225 - val_accuracy: 0.6884\n",
      "Epoch 21/200\n",
      "401/401 [==============================] - 0s 1ms/step - loss: 0.0027 - accuracy: 0.9526 - val_loss: 0.0280 - val_accuracy: 0.6281\n",
      "Epoch 22/200\n",
      "401/401 [==============================] - 0s 1ms/step - loss: 0.0026 - accuracy: 0.9676 - val_loss: 0.0238 - val_accuracy: 0.7085\n",
      "Epoch 23/200\n",
      "401/401 [==============================] - 0s 1ms/step - loss: 0.0025 - accuracy: 0.9601 - val_loss: 0.0262 - val_accuracy: 0.6935\n",
      "Epoch 24/200\n",
      "401/401 [==============================] - 0s 1ms/step - loss: 0.0025 - accuracy: 0.9551 - val_loss: 0.0262 - val_accuracy: 0.6683\n",
      "Epoch 25/200\n",
      "401/401 [==============================] - 0s 1ms/step - loss: 0.0024 - accuracy: 0.9476 - val_loss: 0.0209 - val_accuracy: 0.6985\n",
      "Epoch 26/200\n",
      "401/401 [==============================] - 0s 1ms/step - loss: 0.0023 - accuracy: 0.9426 - val_loss: 0.0296 - val_accuracy: 0.7286\n",
      "Epoch 27/200\n",
      "401/401 [==============================] - 0s 1ms/step - loss: 0.0022 - accuracy: 0.9526 - val_loss: 0.0320 - val_accuracy: 0.7136\n",
      "Epoch 28/200\n",
      "401/401 [==============================] - 0s 1ms/step - loss: 0.0022 - accuracy: 0.9526 - val_loss: 0.0253 - val_accuracy: 0.6884\n",
      "Epoch 29/200\n",
      "401/401 [==============================] - 0s 1ms/step - loss: 0.0020 - accuracy: 0.9526 - val_loss: 0.0248 - val_accuracy: 0.7136\n",
      "Epoch 30/200\n",
      "401/401 [==============================] - 1s 1ms/step - loss: 0.0021 - accuracy: 0.9626 - val_loss: 0.0244 - val_accuracy: 0.7035\n",
      "Epoch 31/200\n",
      "401/401 [==============================] - 0s 1ms/step - loss: 0.0020 - accuracy: 0.9526 - val_loss: 0.0287 - val_accuracy: 0.7136\n",
      "Epoch 32/200\n",
      "401/401 [==============================] - 0s 1ms/step - loss: 0.0018 - accuracy: 0.9451 - val_loss: 0.0267 - val_accuracy: 0.7085\n",
      "Epoch 33/200\n",
      "401/401 [==============================] - 1s 1ms/step - loss: 0.0019 - accuracy: 0.9501 - val_loss: 0.0248 - val_accuracy: 0.7186\n",
      "Epoch 34/200\n",
      "401/401 [==============================] - 0s 1ms/step - loss: 0.0018 - accuracy: 0.9451 - val_loss: 0.0261 - val_accuracy: 0.6985\n",
      "Epoch 35/200\n",
      "401/401 [==============================] - 0s 1ms/step - loss: 0.0017 - accuracy: 0.9426 - val_loss: 0.0264 - val_accuracy: 0.7186\n",
      "Epoch 36/200\n",
      "401/401 [==============================] - 0s 1ms/step - loss: 0.0017 - accuracy: 0.9327 - val_loss: 0.0279 - val_accuracy: 0.7286\n",
      "Epoch 37/200\n",
      "401/401 [==============================] - 0s 1ms/step - loss: 0.0016 - accuracy: 0.9476 - val_loss: 0.0241 - val_accuracy: 0.7136\n",
      "Epoch 38/200\n",
      "401/401 [==============================] - 0s 1ms/step - loss: 0.0015 - accuracy: 0.9377 - val_loss: 0.0287 - val_accuracy: 0.7387\n",
      "Epoch 39/200\n",
      "401/401 [==============================] - 0s 1ms/step - loss: 0.0015 - accuracy: 0.9476 - val_loss: 0.0260 - val_accuracy: 0.7186\n",
      "Epoch 40/200\n",
      "401/401 [==============================] - 0s 1ms/step - loss: 0.0014 - accuracy: 0.9576 - val_loss: 0.0255 - val_accuracy: 0.7236\n",
      "Epoch 41/200\n",
      "401/401 [==============================] - 0s 1ms/step - loss: 0.0014 - accuracy: 0.9551 - val_loss: 0.0230 - val_accuracy: 0.7437\n",
      "Epoch 42/200\n",
      "401/401 [==============================] - 0s 1ms/step - loss: 0.0014 - accuracy: 0.9426 - val_loss: 0.0271 - val_accuracy: 0.7136\n",
      "Epoch 43/200\n",
      "401/401 [==============================] - 0s 1ms/step - loss: 0.0014 - accuracy: 0.9526 - val_loss: 0.0223 - val_accuracy: 0.7136\n",
      "Epoch 44/200\n",
      "401/401 [==============================] - 0s 1ms/step - loss: 0.0013 - accuracy: 0.9526 - val_loss: 0.0230 - val_accuracy: 0.7437\n",
      "Epoch 45/200\n",
      "401/401 [==============================] - 1s 1ms/step - loss: 0.0013 - accuracy: 0.9426 - val_loss: 0.0232 - val_accuracy: 0.7286\n",
      "Epoch 46/200\n",
      "401/401 [==============================] - 0s 1ms/step - loss: 0.0013 - accuracy: 0.9551 - val_loss: 0.0219 - val_accuracy: 0.7487\n",
      "Epoch 47/200\n",
      "401/401 [==============================] - 0s 1ms/step - loss: 0.0012 - accuracy: 0.9426 - val_loss: 0.0197 - val_accuracy: 0.7638\n",
      "Epoch 48/200\n",
      "401/401 [==============================] - 0s 1ms/step - loss: 0.0012 - accuracy: 0.9576 - val_loss: 0.0199 - val_accuracy: 0.7437\n",
      "Epoch 49/200\n",
      "401/401 [==============================] - 0s 1ms/step - loss: 0.0013 - accuracy: 0.9377 - val_loss: 0.0213 - val_accuracy: 0.7638\n",
      "Epoch 50/200\n",
      "401/401 [==============================] - 0s 1ms/step - loss: 0.0013 - accuracy: 0.9601 - val_loss: 0.0211 - val_accuracy: 0.7638\n",
      "Epoch 51/200\n",
      "401/401 [==============================] - 0s 1ms/step - loss: 0.0012 - accuracy: 0.9476 - val_loss: 0.0203 - val_accuracy: 0.7638\n",
      "Epoch 52/200\n",
      "401/401 [==============================] - 0s 1ms/step - loss: 0.0012 - accuracy: 0.9476 - val_loss: 0.0212 - val_accuracy: 0.7688\n",
      "Epoch 53/200\n",
      "401/401 [==============================] - 0s 1ms/step - loss: 0.0011 - accuracy: 0.9476 - val_loss: 0.0202 - val_accuracy: 0.7186\n",
      "Epoch 54/200\n",
      "401/401 [==============================] - 0s 1ms/step - loss: 0.0010 - accuracy: 0.9526 - val_loss: 0.0202 - val_accuracy: 0.6985\n",
      "Epoch 55/200\n",
      "401/401 [==============================] - 0s 1ms/step - loss: 0.0011 - accuracy: 0.9551 - val_loss: 0.0202 - val_accuracy: 0.7588\n",
      "Epoch 56/200\n",
      "401/401 [==============================] - 0s 1ms/step - loss: 0.0011 - accuracy: 0.9601 - val_loss: 0.0185 - val_accuracy: 0.7739\n",
      "Epoch 57/200\n",
      "401/401 [==============================] - 1s 1ms/step - loss: 9.4366e-04 - accuracy: 0.9551 - val_loss: 0.0240 - val_accuracy: 0.7387\n",
      "Epoch 58/200\n",
      "401/401 [==============================] - 0s 1ms/step - loss: 0.0011 - accuracy: 0.9526 - val_loss: 0.0185 - val_accuracy: 0.7638\n",
      "Epoch 59/200\n",
      "401/401 [==============================] - 0s 1ms/step - loss: 9.7874e-04 - accuracy: 0.9501 - val_loss: 0.0180 - val_accuracy: 0.7739\n",
      "Epoch 60/200\n",
      "401/401 [==============================] - 0s 1ms/step - loss: 0.0010 - accuracy: 0.9651 - val_loss: 0.0188 - val_accuracy: 0.7739\n",
      "Epoch 61/200\n",
      "401/401 [==============================] - 0s 1ms/step - loss: 9.2467e-04 - accuracy: 0.9576 - val_loss: 0.0180 - val_accuracy: 0.7186\n",
      "Epoch 62/200\n",
      "401/401 [==============================] - 0s 1ms/step - loss: 0.0010 - accuracy: 0.9576 - val_loss: 0.0182 - val_accuracy: 0.7789\n",
      "Epoch 63/200\n",
      "401/401 [==============================] - 0s 1ms/step - loss: 9.3646e-04 - accuracy: 0.9551 - val_loss: 0.0193 - val_accuracy: 0.6734\n",
      "Epoch 64/200\n",
      "401/401 [==============================] - 0s 1ms/step - loss: 9.7470e-04 - accuracy: 0.9701 - val_loss: 0.0186 - val_accuracy: 0.7638\n",
      "Epoch 65/200\n",
      "401/401 [==============================] - 0s 1ms/step - loss: 9.2129e-04 - accuracy: 0.9626 - val_loss: 0.0184 - val_accuracy: 0.7437\n",
      "Epoch 66/200\n",
      "401/401 [==============================] - 0s 1ms/step - loss: 9.4826e-04 - accuracy: 0.9626 - val_loss: 0.0172 - val_accuracy: 0.7789\n",
      "Epoch 67/200\n",
      "401/401 [==============================] - 0s 1ms/step - loss: 9.2465e-04 - accuracy: 0.9476 - val_loss: 0.0174 - val_accuracy: 0.7889\n",
      "Epoch 68/200\n",
      "401/401 [==============================] - 0s 1ms/step - loss: 8.9684e-04 - accuracy: 0.9576 - val_loss: 0.0189 - val_accuracy: 0.7387\n",
      "Epoch 69/200\n",
      "401/401 [==============================] - 0s 1ms/step - loss: 8.5235e-04 - accuracy: 0.9551 - val_loss: 0.0187 - val_accuracy: 0.7337\n",
      "Epoch 70/200\n",
      "401/401 [==============================] - 0s 1ms/step - loss: 8.5326e-04 - accuracy: 0.9626 - val_loss: 0.0179 - val_accuracy: 0.7538\n",
      "Epoch 71/200\n",
      "401/401 [==============================] - 0s 1ms/step - loss: 8.7295e-04 - accuracy: 0.9551 - val_loss: 0.0190 - val_accuracy: 0.7638\n",
      "Epoch 72/200\n",
      "401/401 [==============================] - 0s 1ms/step - loss: 8.3128e-04 - accuracy: 0.9651 - val_loss: 0.0194 - val_accuracy: 0.7739\n",
      "Epoch 73/200\n",
      "401/401 [==============================] - 0s 1ms/step - loss: 8.4227e-04 - accuracy: 0.9601 - val_loss: 0.0187 - val_accuracy: 0.7839\n",
      "Epoch 74/200\n",
      "401/401 [==============================] - 0s 1ms/step - loss: 8.5133e-04 - accuracy: 0.9576 - val_loss: 0.0197 - val_accuracy: 0.6633\n",
      "Epoch 75/200\n",
      "401/401 [==============================] - 0s 1ms/step - loss: 7.5756e-04 - accuracy: 0.9526 - val_loss: 0.0218 - val_accuracy: 0.5427\n",
      "Epoch 76/200\n",
      "401/401 [==============================] - 0s 1ms/step - loss: 7.3995e-04 - accuracy: 0.9651 - val_loss: 0.0179 - val_accuracy: 0.7136\n",
      "Epoch 77/200\n",
      "401/401 [==============================] - 0s 1ms/step - loss: 7.7466e-04 - accuracy: 0.9726 - val_loss: 0.0187 - val_accuracy: 0.7588\n",
      "Epoch 78/200\n",
      "401/401 [==============================] - 0s 1ms/step - loss: 8.0938e-04 - accuracy: 0.9551 - val_loss: 0.0199 - val_accuracy: 0.7538\n",
      "Epoch 79/200\n",
      "401/401 [==============================] - 0s 1ms/step - loss: 7.4784e-04 - accuracy: 0.9601 - val_loss: 0.0191 - val_accuracy: 0.7990\n",
      "Epoch 80/200\n",
      "401/401 [==============================] - 0s 1ms/step - loss: 7.2332e-04 - accuracy: 0.9726 - val_loss: 0.0208 - val_accuracy: 0.7839\n",
      "Epoch 81/200\n",
      "401/401 [==============================] - 0s 1ms/step - loss: 7.5010e-04 - accuracy: 0.9601 - val_loss: 0.0201 - val_accuracy: 0.7839\n",
      "Epoch 82/200\n",
      "401/401 [==============================] - 0s 1ms/step - loss: 7.3583e-04 - accuracy: 0.9601 - val_loss: 0.0217 - val_accuracy: 0.7236\n",
      "Epoch 83/200\n",
      "401/401 [==============================] - 0s 1ms/step - loss: 6.8632e-04 - accuracy: 0.9701 - val_loss: 0.0223 - val_accuracy: 0.7236\n",
      "Epoch 84/200\n",
      "401/401 [==============================] - 0s 1ms/step - loss: 6.1805e-04 - accuracy: 0.9701 - val_loss: 0.0226 - val_accuracy: 0.6131\n",
      "Epoch 85/200\n",
      "401/401 [==============================] - 0s 1ms/step - loss: 7.5698e-04 - accuracy: 0.9501 - val_loss: 0.0229 - val_accuracy: 0.6080\n",
      "Epoch 86/200\n",
      "401/401 [==============================] - 0s 1ms/step - loss: 6.8259e-04 - accuracy: 0.9601 - val_loss: 0.0228 - val_accuracy: 0.7487\n",
      "Epoch 87/200\n",
      "401/401 [==============================] - 0s 1ms/step - loss: 6.7265e-04 - accuracy: 0.9626 - val_loss: 0.0208 - val_accuracy: 0.7487\n",
      "Epoch 88/200\n",
      "401/401 [==============================] - 0s 1ms/step - loss: 6.9554e-04 - accuracy: 0.9751 - val_loss: 0.0231 - val_accuracy: 0.5678\n",
      "Epoch 89/200\n",
      "401/401 [==============================] - 0s 1ms/step - loss: 5.8677e-04 - accuracy: 0.9701 - val_loss: 0.0219 - val_accuracy: 0.7538\n",
      "Epoch 90/200\n",
      "401/401 [==============================] - 0s 1ms/step - loss: 6.2923e-04 - accuracy: 0.9701 - val_loss: 0.0218 - val_accuracy: 0.6533\n",
      "Epoch 91/200\n",
      "401/401 [==============================] - 1s 1ms/step - loss: 5.7849e-04 - accuracy: 0.9751 - val_loss: 0.0240 - val_accuracy: 0.6231\n",
      "Epoch 92/200\n",
      "401/401 [==============================] - 1s 1ms/step - loss: 5.9548e-04 - accuracy: 0.9676 - val_loss: 0.0237 - val_accuracy: 0.6533\n",
      "Epoch 93/200\n",
      "401/401 [==============================] - 1s 2ms/step - loss: 6.1879e-04 - accuracy: 0.9651 - val_loss: 0.0253 - val_accuracy: 0.5729\n",
      "Epoch 94/200\n",
      "401/401 [==============================] - 1s 1ms/step - loss: 5.6785e-04 - accuracy: 0.9651 - val_loss: 0.0228 - val_accuracy: 0.7035\n",
      "Epoch 95/200\n",
      "401/401 [==============================] - 0s 1ms/step - loss: 5.7611e-04 - accuracy: 0.9676 - val_loss: 0.0252 - val_accuracy: 0.6432\n",
      "Epoch 96/200\n",
      "401/401 [==============================] - 0s 1ms/step - loss: 5.6407e-04 - accuracy: 0.9751 - val_loss: 0.0252 - val_accuracy: 0.5930\n",
      "Epoch 97/200\n",
      "401/401 [==============================] - 0s 1ms/step - loss: 5.5822e-04 - accuracy: 0.9701 - val_loss: 0.0258 - val_accuracy: 0.5930\n",
      "Epoch 98/200\n",
      "401/401 [==============================] - 0s 1ms/step - loss: 5.1990e-04 - accuracy: 0.9676 - val_loss: 0.0293 - val_accuracy: 0.5477\n",
      "Epoch 99/200\n",
      "401/401 [==============================] - 0s 1ms/step - loss: 5.7091e-04 - accuracy: 0.9701 - val_loss: 0.0259 - val_accuracy: 0.5829\n",
      "Epoch 100/200\n",
      "401/401 [==============================] - 0s 1ms/step - loss: 5.6666e-04 - accuracy: 0.9651 - val_loss: 0.0306 - val_accuracy: 0.5528\n",
      "Epoch 101/200\n",
      "401/401 [==============================] - 0s 1ms/step - loss: 5.1329e-04 - accuracy: 0.9726 - val_loss: 0.0286 - val_accuracy: 0.6181\n",
      "Epoch 102/200\n",
      "401/401 [==============================] - 0s 1ms/step - loss: 5.3223e-04 - accuracy: 0.9651 - val_loss: 0.0264 - val_accuracy: 0.6382\n",
      "Epoch 103/200\n",
      "401/401 [==============================] - 0s 1ms/step - loss: 4.9413e-04 - accuracy: 0.9726 - val_loss: 0.0255 - val_accuracy: 0.6382\n",
      "Epoch 104/200\n",
      "401/401 [==============================] - 0s 1ms/step - loss: 5.2057e-04 - accuracy: 0.9776 - val_loss: 0.0243 - val_accuracy: 0.6281\n",
      "Epoch 105/200\n",
      "401/401 [==============================] - 0s 1ms/step - loss: 4.9153e-04 - accuracy: 0.9850 - val_loss: 0.0282 - val_accuracy: 0.5678\n",
      "Epoch 106/200\n",
      "401/401 [==============================] - 0s 1ms/step - loss: 5.1762e-04 - accuracy: 0.9626 - val_loss: 0.0238 - val_accuracy: 0.6734\n",
      "Epoch 107/200\n",
      "401/401 [==============================] - 0s 1ms/step - loss: 5.0598e-04 - accuracy: 0.9701 - val_loss: 0.0289 - val_accuracy: 0.5678\n",
      "Epoch 108/200\n",
      "401/401 [==============================] - 0s 1ms/step - loss: 5.4320e-04 - accuracy: 0.9751 - val_loss: 0.0245 - val_accuracy: 0.6633\n",
      "Epoch 109/200\n",
      "401/401 [==============================] - 0s 1ms/step - loss: 5.1432e-04 - accuracy: 0.9651 - val_loss: 0.0272 - val_accuracy: 0.6281\n",
      "Epoch 110/200\n",
      "401/401 [==============================] - 0s 1ms/step - loss: 4.4762e-04 - accuracy: 0.9676 - val_loss: 0.0263 - val_accuracy: 0.6332\n",
      "Epoch 111/200\n",
      "401/401 [==============================] - 0s 1ms/step - loss: 4.4189e-04 - accuracy: 0.9751 - val_loss: 0.0256 - val_accuracy: 0.6332\n",
      "Epoch 112/200\n",
      "401/401 [==============================] - 1s 1ms/step - loss: 4.8752e-04 - accuracy: 0.9601 - val_loss: 0.0307 - val_accuracy: 0.5879\n",
      "Epoch 113/200\n",
      "401/401 [==============================] - 1s 1ms/step - loss: 5.3686e-04 - accuracy: 0.9551 - val_loss: 0.0282 - val_accuracy: 0.5829\n",
      "Epoch 114/200\n",
      "401/401 [==============================] - 0s 1ms/step - loss: 4.3679e-04 - accuracy: 0.9751 - val_loss: 0.0270 - val_accuracy: 0.5980\n",
      "Epoch 115/200\n",
      "401/401 [==============================] - 1s 1ms/step - loss: 4.4726e-04 - accuracy: 0.9701 - val_loss: 0.0259 - val_accuracy: 0.6181\n",
      "Epoch 116/200\n",
      "401/401 [==============================] - 1s 2ms/step - loss: 4.3769e-04 - accuracy: 0.9676 - val_loss: 0.0325 - val_accuracy: 0.5879\n",
      "Epoch 117/200\n",
      "401/401 [==============================] - 1s 1ms/step - loss: 4.5155e-04 - accuracy: 0.9676 - val_loss: 0.0278 - val_accuracy: 0.6332\n",
      "Epoch 118/200\n",
      "401/401 [==============================] - 1s 1ms/step - loss: 4.7135e-04 - accuracy: 0.9701 - val_loss: 0.0277 - val_accuracy: 0.5879\n",
      "Epoch 119/200\n",
      "401/401 [==============================] - 0s 1ms/step - loss: 4.4218e-04 - accuracy: 0.9726 - val_loss: 0.0305 - val_accuracy: 0.5276\n",
      "Epoch 120/200\n",
      "401/401 [==============================] - 0s 1ms/step - loss: 4.2300e-04 - accuracy: 0.9701 - val_loss: 0.0326 - val_accuracy: 0.5377\n",
      "Epoch 121/200\n",
      "401/401 [==============================] - 0s 1ms/step - loss: 4.7611e-04 - accuracy: 0.9651 - val_loss: 0.0272 - val_accuracy: 0.6131\n",
      "Epoch 122/200\n",
      "401/401 [==============================] - 0s 1ms/step - loss: 4.3683e-04 - accuracy: 0.9751 - val_loss: 0.0297 - val_accuracy: 0.5729\n",
      "Epoch 123/200\n",
      "401/401 [==============================] - 0s 1ms/step - loss: 4.6600e-04 - accuracy: 0.9651 - val_loss: 0.0300 - val_accuracy: 0.5779\n",
      "Epoch 124/200\n",
      "401/401 [==============================] - 1s 1ms/step - loss: 4.1803e-04 - accuracy: 0.9726 - val_loss: 0.0295 - val_accuracy: 0.6030\n",
      "Epoch 125/200\n",
      "401/401 [==============================] - 1s 1ms/step - loss: 3.8619e-04 - accuracy: 0.9701 - val_loss: 0.0283 - val_accuracy: 0.5779\n",
      "Epoch 126/200\n",
      "401/401 [==============================] - 0s 1ms/step - loss: 4.0599e-04 - accuracy: 0.9726 - val_loss: 0.0265 - val_accuracy: 0.6181\n",
      "Epoch 127/200\n",
      "401/401 [==============================] - 0s 1ms/step - loss: 3.7665e-04 - accuracy: 0.9751 - val_loss: 0.0282 - val_accuracy: 0.5829\n",
      "Epoch 128/200\n",
      "401/401 [==============================] - 0s 1ms/step - loss: 4.2340e-04 - accuracy: 0.9751 - val_loss: 0.0263 - val_accuracy: 0.6231\n",
      "Epoch 129/200\n",
      "401/401 [==============================] - 0s 1ms/step - loss: 4.1331e-04 - accuracy: 0.9676 - val_loss: 0.0254 - val_accuracy: 0.6432\n",
      "Epoch 130/200\n",
      "401/401 [==============================] - 0s 1ms/step - loss: 4.0497e-04 - accuracy: 0.9626 - val_loss: 0.0284 - val_accuracy: 0.5829\n",
      "Epoch 131/200\n",
      "401/401 [==============================] - 0s 1ms/step - loss: 3.9169e-04 - accuracy: 0.9726 - val_loss: 0.0283 - val_accuracy: 0.5930\n",
      "Epoch 132/200\n",
      "401/401 [==============================] - 0s 1ms/step - loss: 4.2161e-04 - accuracy: 0.9726 - val_loss: 0.0252 - val_accuracy: 0.6332\n",
      "Epoch 133/200\n",
      "401/401 [==============================] - 0s 1ms/step - loss: 3.5246e-04 - accuracy: 0.9726 - val_loss: 0.0253 - val_accuracy: 0.7085\n",
      "Epoch 134/200\n",
      "401/401 [==============================] - 0s 1ms/step - loss: 4.7527e-04 - accuracy: 0.9676 - val_loss: 0.0286 - val_accuracy: 0.5879\n",
      "Epoch 135/200\n",
      "401/401 [==============================] - 0s 1ms/step - loss: 3.7070e-04 - accuracy: 0.9776 - val_loss: 0.0312 - val_accuracy: 0.5678\n",
      "Epoch 136/200\n",
      "401/401 [==============================] - 0s 1ms/step - loss: 3.7834e-04 - accuracy: 0.9751 - val_loss: 0.0281 - val_accuracy: 0.5879\n",
      "Epoch 137/200\n",
      "401/401 [==============================] - 0s 1ms/step - loss: 4.3080e-04 - accuracy: 0.9726 - val_loss: 0.0295 - val_accuracy: 0.5779\n",
      "Epoch 138/200\n",
      "401/401 [==============================] - 0s 1ms/step - loss: 3.7100e-04 - accuracy: 0.9676 - val_loss: 0.0308 - val_accuracy: 0.5327\n",
      "Epoch 139/200\n",
      "401/401 [==============================] - 0s 1ms/step - loss: 4.1560e-04 - accuracy: 0.9676 - val_loss: 0.0293 - val_accuracy: 0.5879\n",
      "Epoch 140/200\n",
      "401/401 [==============================] - 0s 1ms/step - loss: 4.0171e-04 - accuracy: 0.9701 - val_loss: 0.0308 - val_accuracy: 0.5678\n",
      "Epoch 141/200\n",
      "401/401 [==============================] - 0s 1ms/step - loss: 3.7029e-04 - accuracy: 0.9701 - val_loss: 0.0351 - val_accuracy: 0.5427\n",
      "Epoch 142/200\n",
      "401/401 [==============================] - 0s 1ms/step - loss: 4.2111e-04 - accuracy: 0.9800 - val_loss: 0.0312 - val_accuracy: 0.5276\n",
      "Epoch 143/200\n",
      "401/401 [==============================] - 0s 1ms/step - loss: 3.5811e-04 - accuracy: 0.9800 - val_loss: 0.0300 - val_accuracy: 0.5879\n",
      "Epoch 144/200\n",
      "401/401 [==============================] - 0s 1ms/step - loss: 3.3431e-04 - accuracy: 0.9701 - val_loss: 0.0297 - val_accuracy: 0.5829\n",
      "Epoch 145/200\n",
      "401/401 [==============================] - 0s 1ms/step - loss: 3.8097e-04 - accuracy: 0.9701 - val_loss: 0.0303 - val_accuracy: 0.5578\n",
      "Epoch 146/200\n",
      "401/401 [==============================] - 0s 1ms/step - loss: 3.3435e-04 - accuracy: 0.9751 - val_loss: 0.0281 - val_accuracy: 0.5980\n",
      "Epoch 147/200\n",
      "401/401 [==============================] - 0s 1ms/step - loss: 3.8385e-04 - accuracy: 0.9651 - val_loss: 0.0292 - val_accuracy: 0.5879\n",
      "Epoch 148/200\n",
      "401/401 [==============================] - 0s 1ms/step - loss: 3.5291e-04 - accuracy: 0.9726 - val_loss: 0.0306 - val_accuracy: 0.5779\n",
      "Epoch 149/200\n",
      "401/401 [==============================] - 0s 1ms/step - loss: 3.3702e-04 - accuracy: 0.9701 - val_loss: 0.0341 - val_accuracy: 0.5176\n",
      "Epoch 150/200\n",
      "401/401 [==============================] - 0s 1ms/step - loss: 3.9866e-04 - accuracy: 0.9626 - val_loss: 0.0285 - val_accuracy: 0.6181\n",
      "Epoch 151/200\n",
      "401/401 [==============================] - 0s 1ms/step - loss: 3.4612e-04 - accuracy: 0.9751 - val_loss: 0.0309 - val_accuracy: 0.5779\n",
      "Epoch 152/200\n",
      "401/401 [==============================] - 0s 1ms/step - loss: 3.5620e-04 - accuracy: 0.9701 - val_loss: 0.0275 - val_accuracy: 0.6030\n",
      "Epoch 153/200\n",
      "401/401 [==============================] - 0s 1ms/step - loss: 3.9127e-04 - accuracy: 0.9776 - val_loss: 0.0267 - val_accuracy: 0.6080\n",
      "Epoch 154/200\n",
      "401/401 [==============================] - 0s 1ms/step - loss: 3.6876e-04 - accuracy: 0.9701 - val_loss: 0.0319 - val_accuracy: 0.5628\n",
      "Epoch 155/200\n",
      "401/401 [==============================] - 0s 1ms/step - loss: 3.6336e-04 - accuracy: 0.9676 - val_loss: 0.0298 - val_accuracy: 0.5779\n",
      "Epoch 156/200\n",
      "401/401 [==============================] - 0s 1ms/step - loss: 4.0868e-04 - accuracy: 0.9701 - val_loss: 0.0303 - val_accuracy: 0.5578\n",
      "Epoch 157/200\n",
      "401/401 [==============================] - 0s 1ms/step - loss: 3.5394e-04 - accuracy: 0.9726 - val_loss: 0.0296 - val_accuracy: 0.5829\n",
      "Epoch 158/200\n",
      "401/401 [==============================] - 0s 1ms/step - loss: 3.2203e-04 - accuracy: 0.9626 - val_loss: 0.0309 - val_accuracy: 0.5779\n",
      "Epoch 159/200\n",
      "401/401 [==============================] - 0s 1ms/step - loss: 3.4727e-04 - accuracy: 0.9701 - val_loss: 0.0304 - val_accuracy: 0.5729\n",
      "Epoch 160/200\n",
      "401/401 [==============================] - 0s 1ms/step - loss: 3.3886e-04 - accuracy: 0.9726 - val_loss: 0.0309 - val_accuracy: 0.5477\n",
      "Epoch 161/200\n",
      "401/401 [==============================] - 1s 1ms/step - loss: 3.3525e-04 - accuracy: 0.9800 - val_loss: 0.0321 - val_accuracy: 0.5377\n",
      "Epoch 162/200\n",
      "401/401 [==============================] - 1s 1ms/step - loss: 3.1408e-04 - accuracy: 0.9751 - val_loss: 0.0265 - val_accuracy: 0.6181\n",
      "Epoch 163/200\n",
      "401/401 [==============================] - 0s 1ms/step - loss: 3.3219e-04 - accuracy: 0.9776 - val_loss: 0.0265 - val_accuracy: 0.5879\n",
      "Epoch 164/200\n",
      "401/401 [==============================] - 0s 1ms/step - loss: 3.8313e-04 - accuracy: 0.9651 - val_loss: 0.0300 - val_accuracy: 0.5628\n",
      "Epoch 165/200\n",
      "401/401 [==============================] - 0s 1ms/step - loss: 3.4235e-04 - accuracy: 0.9776 - val_loss: 0.0324 - val_accuracy: 0.5427\n",
      "Epoch 166/200\n",
      "401/401 [==============================] - 1s 1ms/step - loss: 3.3249e-04 - accuracy: 0.9676 - val_loss: 0.0292 - val_accuracy: 0.5829\n",
      "Epoch 167/200\n",
      "401/401 [==============================] - 1s 1ms/step - loss: 3.1955e-04 - accuracy: 0.9726 - val_loss: 0.0299 - val_accuracy: 0.5829\n",
      "Epoch 168/200\n",
      "401/401 [==============================] - 0s 1ms/step - loss: 3.2354e-04 - accuracy: 0.9726 - val_loss: 0.0286 - val_accuracy: 0.5879\n",
      "Epoch 169/200\n",
      "401/401 [==============================] - 0s 1ms/step - loss: 3.4911e-04 - accuracy: 0.9800 - val_loss: 0.0275 - val_accuracy: 0.5980\n",
      "Epoch 170/200\n",
      "401/401 [==============================] - 0s 1ms/step - loss: 2.9861e-04 - accuracy: 0.9726 - val_loss: 0.0308 - val_accuracy: 0.5578\n",
      "Epoch 171/200\n",
      "401/401 [==============================] - 1s 1ms/step - loss: 3.4203e-04 - accuracy: 0.9751 - val_loss: 0.0316 - val_accuracy: 0.5628\n",
      "Epoch 172/200\n",
      "401/401 [==============================] - 1s 1ms/step - loss: 2.9836e-04 - accuracy: 0.9751 - val_loss: 0.0283 - val_accuracy: 0.5980\n",
      "Epoch 173/200\n",
      "401/401 [==============================] - 0s 1ms/step - loss: 3.1487e-04 - accuracy: 0.9651 - val_loss: 0.0298 - val_accuracy: 0.5829\n",
      "Epoch 174/200\n",
      "401/401 [==============================] - 1s 1ms/step - loss: 2.9550e-04 - accuracy: 0.9751 - val_loss: 0.0339 - val_accuracy: 0.5427\n",
      "Epoch 175/200\n",
      "401/401 [==============================] - 0s 1ms/step - loss: 3.0011e-04 - accuracy: 0.9651 - val_loss: 0.0319 - val_accuracy: 0.5377\n",
      "Epoch 176/200\n",
      "401/401 [==============================] - 1s 1ms/step - loss: 2.8085e-04 - accuracy: 0.9800 - val_loss: 0.0277 - val_accuracy: 0.5930\n",
      "Epoch 177/200\n",
      "401/401 [==============================] - 0s 1ms/step - loss: 3.5150e-04 - accuracy: 0.9676 - val_loss: 0.0284 - val_accuracy: 0.5829\n",
      "Epoch 178/200\n",
      "401/401 [==============================] - 0s 1ms/step - loss: 2.9179e-04 - accuracy: 0.9800 - val_loss: 0.0278 - val_accuracy: 0.5930\n",
      "Epoch 179/200\n",
      "401/401 [==============================] - 0s 1ms/step - loss: 3.1619e-04 - accuracy: 0.9726 - val_loss: 0.0304 - val_accuracy: 0.5729\n",
      "Epoch 180/200\n",
      "401/401 [==============================] - 0s 1ms/step - loss: 2.9578e-04 - accuracy: 0.9701 - val_loss: 0.0306 - val_accuracy: 0.5678\n",
      "Epoch 181/200\n",
      "401/401 [==============================] - 0s 1ms/step - loss: 3.0253e-04 - accuracy: 0.9726 - val_loss: 0.0319 - val_accuracy: 0.5729\n",
      "Epoch 182/200\n",
      "401/401 [==============================] - 0s 1ms/step - loss: 3.2428e-04 - accuracy: 0.9751 - val_loss: 0.0312 - val_accuracy: 0.5980\n",
      "Epoch 183/200\n",
      "401/401 [==============================] - 0s 1ms/step - loss: 2.8406e-04 - accuracy: 0.9751 - val_loss: 0.0301 - val_accuracy: 0.5729\n",
      "Epoch 184/200\n",
      "401/401 [==============================] - 0s 1ms/step - loss: 3.1163e-04 - accuracy: 0.9726 - val_loss: 0.0311 - val_accuracy: 0.5578\n",
      "Epoch 185/200\n",
      "401/401 [==============================] - 0s 1ms/step - loss: 3.1233e-04 - accuracy: 0.9701 - val_loss: 0.0330 - val_accuracy: 0.5427\n",
      "Epoch 186/200\n",
      "401/401 [==============================] - 0s 1ms/step - loss: 2.7598e-04 - accuracy: 0.9800 - val_loss: 0.0336 - val_accuracy: 0.5528\n",
      "Epoch 187/200\n",
      "401/401 [==============================] - 1s 1ms/step - loss: 2.9694e-04 - accuracy: 0.9676 - val_loss: 0.0300 - val_accuracy: 0.5729\n",
      "Epoch 188/200\n",
      "401/401 [==============================] - 0s 1ms/step - loss: 2.8788e-04 - accuracy: 0.9676 - val_loss: 0.0294 - val_accuracy: 0.5829\n",
      "Epoch 189/200\n",
      "401/401 [==============================] - 0s 1ms/step - loss: 2.8551e-04 - accuracy: 0.9751 - val_loss: 0.0304 - val_accuracy: 0.5678\n",
      "Epoch 190/200\n",
      "401/401 [==============================] - 0s 1ms/step - loss: 2.8626e-04 - accuracy: 0.9800 - val_loss: 0.0334 - val_accuracy: 0.5528\n",
      "Epoch 191/200\n",
      "401/401 [==============================] - 1s 1ms/step - loss: 2.9642e-04 - accuracy: 0.9726 - val_loss: 0.0304 - val_accuracy: 0.5779\n",
      "Epoch 192/200\n",
      "401/401 [==============================] - 0s 1ms/step - loss: 2.7704e-04 - accuracy: 0.9701 - val_loss: 0.0384 - val_accuracy: 0.5075\n",
      "Epoch 193/200\n",
      "401/401 [==============================] - 1s 1ms/step - loss: 3.1497e-04 - accuracy: 0.9701 - val_loss: 0.0296 - val_accuracy: 0.5779\n",
      "Epoch 194/200\n",
      "401/401 [==============================] - 1s 1ms/step - loss: 2.8422e-04 - accuracy: 0.9800 - val_loss: 0.0330 - val_accuracy: 0.5477\n",
      "Epoch 195/200\n",
      "401/401 [==============================] - 1s 1ms/step - loss: 3.0297e-04 - accuracy: 0.9701 - val_loss: 0.0346 - val_accuracy: 0.4824\n",
      "Epoch 196/200\n",
      "401/401 [==============================] - 1s 1ms/step - loss: 3.2701e-04 - accuracy: 0.9701 - val_loss: 0.0316 - val_accuracy: 0.5578\n",
      "Epoch 197/200\n",
      "401/401 [==============================] - 1s 1ms/step - loss: 2.9505e-04 - accuracy: 0.9751 - val_loss: 0.0333 - val_accuracy: 0.5477\n",
      "Epoch 198/200\n",
      "401/401 [==============================] - 0s 1ms/step - loss: 2.5471e-04 - accuracy: 0.9800 - val_loss: 0.0323 - val_accuracy: 0.5578\n",
      "Epoch 199/200\n",
      "401/401 [==============================] - 0s 1ms/step - loss: 2.8841e-04 - accuracy: 0.9751 - val_loss: 0.0310 - val_accuracy: 0.5879\n",
      "Epoch 200/200\n",
      "401/401 [==============================] - 0s 1ms/step - loss: 2.8457e-04 - accuracy: 0.9726 - val_loss: 0.0289 - val_accuracy: 0.6181\n"
     ]
    }
   ],
   "source": [
    "#Fit single dataset to model\n",
    "FitModel(x_train, y_train, model_2, 200, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "(905, 1, 9)\n",
      "(905, 1, 9)\n",
      "Epoch 1/20\n",
      "905/905 [==============================] - 3s 2ms/step - loss: 0.0021 - accuracy: 0.9945\n",
      "Epoch 2/20\n",
      "905/905 [==============================] - 1s 1ms/step - loss: 2.1529e-05 - accuracy: 1.0000\n",
      "Epoch 3/20\n",
      "905/905 [==============================] - 1s 997us/step - loss: 1.9848e-05 - accuracy: 1.0000\n",
      "Epoch 4/20\n",
      "905/905 [==============================] - 1s 940us/step - loss: 2.0158e-05 - accuracy: 1.0000\n",
      "Epoch 5/20\n",
      "905/905 [==============================] - 1s 853us/step - loss: 2.0968e-05 - accuracy: 1.0000\n",
      "Epoch 6/20\n",
      "905/905 [==============================] - 1s 843us/step - loss: 2.2221e-05 - accuracy: 1.0000\n",
      "Epoch 7/20\n",
      "905/905 [==============================] - 1s 854us/step - loss: 2.1092e-05 - accuracy: 1.0000\n",
      "Epoch 8/20\n",
      "905/905 [==============================] - 1s 848us/step - loss: 2.0868e-05 - accuracy: 1.0000\n",
      "Epoch 9/20\n",
      "905/905 [==============================] - 1s 853us/step - loss: 1.9991e-05 - accuracy: 1.0000\n",
      "Epoch 10/20\n",
      "905/905 [==============================] - 1s 855us/step - loss: 1.8846e-05 - accuracy: 1.0000\n",
      "Epoch 11/20\n",
      "905/905 [==============================] - 1s 873us/step - loss: 1.7361e-05 - accuracy: 1.0000\n",
      "Epoch 12/20\n",
      "905/905 [==============================] - 1s 857us/step - loss: 1.8630e-05 - accuracy: 1.0000\n",
      "Epoch 13/20\n",
      "905/905 [==============================] - 1s 866us/step - loss: 1.5849e-05 - accuracy: 1.0000\n",
      "Epoch 14/20\n",
      "905/905 [==============================] - 1s 859us/step - loss: 1.5923e-05 - accuracy: 1.0000\n",
      "Epoch 15/20\n",
      "905/905 [==============================] - 1s 849us/step - loss: 1.4468e-05 - accuracy: 1.0000\n",
      "Epoch 16/20\n",
      "905/905 [==============================] - 1s 850us/step - loss: 1.4397e-05 - accuracy: 1.0000\n",
      "Epoch 17/20\n",
      "905/905 [==============================] - 1s 865us/step - loss: 1.4212e-05 - accuracy: 1.0000\n",
      "Epoch 18/20\n",
      "905/905 [==============================] - 1s 853us/step - loss: 1.3543e-05 - accuracy: 1.0000\n",
      "Epoch 19/20\n",
      "905/905 [==============================] - 1s 884us/step - loss: 1.2696e-05 - accuracy: 1.0000\n",
      "Epoch 20/20\n",
      "905/905 [==============================] - 1s 864us/step - loss: 1.4518e-05 - accuracy: 1.0000\n",
      "1\n",
      "(600, 1, 9)\n",
      "(600, 1, 9)\n",
      "Epoch 1/20\n",
      "600/600 [==============================] - 1s 869us/step - loss: 5.9910e-05 - accuracy: 1.0000\n",
      "Epoch 2/20\n",
      "600/600 [==============================] - 1s 936us/step - loss: 1.3324e-05 - accuracy: 1.0000\n",
      "Epoch 3/20\n",
      "600/600 [==============================] - 1s 889us/step - loss: 1.1586e-05 - accuracy: 1.0000\n",
      "Epoch 4/20\n",
      "600/600 [==============================] - 1s 871us/step - loss: 1.1322e-05 - accuracy: 1.0000\n",
      "Epoch 5/20\n",
      "600/600 [==============================] - 1s 904us/step - loss: 9.8681e-06 - accuracy: 1.0000\n",
      "Epoch 6/20\n",
      "600/600 [==============================] - 1s 881us/step - loss: 1.0380e-05 - accuracy: 1.0000\n",
      "Epoch 7/20\n",
      "600/600 [==============================] - 1s 884us/step - loss: 9.2665e-06 - accuracy: 1.0000\n",
      "Epoch 8/20\n",
      "600/600 [==============================] - 1s 836us/step - loss: 7.8351e-06 - accuracy: 1.0000\n",
      "Epoch 9/20\n",
      "600/600 [==============================] - 1s 851us/step - loss: 8.1997e-06 - accuracy: 1.0000\n",
      "Epoch 10/20\n",
      "600/600 [==============================] - 1s 912us/step - loss: 8.4138e-06 - accuracy: 1.0000\n",
      "Epoch 11/20\n",
      "600/600 [==============================] - 1s 891us/step - loss: 8.5255e-06 - accuracy: 1.0000\n",
      "Epoch 12/20\n",
      "600/600 [==============================] - 1s 862us/step - loss: 7.8428e-06 - accuracy: 1.0000\n",
      "Epoch 13/20\n",
      "600/600 [==============================] - 1s 914us/step - loss: 7.2059e-06 - accuracy: 1.0000\n",
      "Epoch 14/20\n",
      "600/600 [==============================] - 1s 964us/step - loss: 7.2771e-06 - accuracy: 1.0000\n",
      "Epoch 15/20\n",
      "600/600 [==============================] - 1s 861us/step - loss: 7.6061e-06 - accuracy: 1.0000\n",
      "Epoch 16/20\n",
      "600/600 [==============================] - 1s 892us/step - loss: 7.4701e-06 - accuracy: 1.0000\n",
      "Epoch 17/20\n",
      "600/600 [==============================] - 1s 876us/step - loss: 6.7901e-06 - accuracy: 1.0000\n",
      "Epoch 18/20\n",
      "600/600 [==============================] - 1s 856us/step - loss: 6.4517e-06 - accuracy: 1.0000\n",
      "Epoch 19/20\n",
      "600/600 [==============================] - 1s 872us/step - loss: 6.8528e-06 - accuracy: 1.0000\n",
      "Epoch 20/20\n",
      "600/600 [==============================] - 1s 864us/step - loss: 6.5009e-06 - accuracy: 1.0000\n",
      "2\n",
      "(600, 1, 9)\n",
      "(600, 1, 9)\n",
      "Epoch 1/20\n",
      "600/600 [==============================] - 1s 841us/step - loss: 3.3417e-04 - accuracy: 1.0000\n",
      "Epoch 2/20\n",
      "600/600 [==============================] - 1s 867us/step - loss: 2.3227e-04 - accuracy: 1.0000\n",
      "Epoch 3/20\n",
      "600/600 [==============================] - 1s 877us/step - loss: 2.1329e-04 - accuracy: 1.0000\n",
      "Epoch 4/20\n",
      "600/600 [==============================] - 1s 891us/step - loss: 1.8723e-04 - accuracy: 1.0000\n",
      "Epoch 5/20\n",
      "600/600 [==============================] - 1s 877us/step - loss: 1.8121e-04 - accuracy: 1.0000\n",
      "Epoch 6/20\n",
      "600/600 [==============================] - 1s 881us/step - loss: 1.7568e-04 - accuracy: 1.0000\n",
      "Epoch 7/20\n",
      "600/600 [==============================] - 1s 881us/step - loss: 1.6759e-04 - accuracy: 1.0000\n",
      "Epoch 8/20\n",
      "600/600 [==============================] - 1s 854us/step - loss: 1.6845e-04 - accuracy: 1.0000\n",
      "Epoch 9/20\n",
      "600/600 [==============================] - 1s 897us/step - loss: 1.6419e-04 - accuracy: 1.0000\n",
      "Epoch 10/20\n",
      "600/600 [==============================] - 1s 1ms/step - loss: 1.6113e-04 - accuracy: 1.0000\n",
      "Epoch 11/20\n",
      "600/600 [==============================] - 1s 877us/step - loss: 1.5567e-04 - accuracy: 1.0000\n",
      "Epoch 12/20\n",
      "600/600 [==============================] - 1s 981us/step - loss: 1.5890e-04 - accuracy: 1.0000\n",
      "Epoch 13/20\n",
      "600/600 [==============================] - 1s 896us/step - loss: 1.4753e-04 - accuracy: 1.0000\n",
      "Epoch 14/20\n",
      "600/600 [==============================] - 1s 884us/step - loss: 1.5039e-04 - accuracy: 1.0000\n",
      "Epoch 15/20\n",
      "600/600 [==============================] - 1s 899us/step - loss: 1.4772e-04 - accuracy: 1.0000\n",
      "Epoch 16/20\n",
      "600/600 [==============================] - 1s 1ms/step - loss: 1.3949e-04 - accuracy: 1.0000\n",
      "Epoch 17/20\n",
      "600/600 [==============================] - 1s 941us/step - loss: 1.4039e-04 - accuracy: 1.0000\n",
      "Epoch 18/20\n",
      "600/600 [==============================] - 1s 862us/step - loss: 1.3784e-04 - accuracy: 1.0000\n",
      "Epoch 19/20\n",
      "600/600 [==============================] - 1s 1ms/step - loss: 1.3131e-04 - accuracy: 1.0000\n",
      "Epoch 20/20\n",
      "600/600 [==============================] - 0s 814us/step - loss: 1.3486e-04 - accuracy: 1.0000\n",
      "3\n",
      "(600, 1, 9)\n",
      "(600, 1, 9)\n",
      "Epoch 1/20\n",
      "600/600 [==============================] - 0s 816us/step - loss: 0.0193 - accuracy: 0.7500\n",
      "Epoch 2/20\n",
      "600/600 [==============================] - 1s 891us/step - loss: 0.0067 - accuracy: 0.7933\n",
      "Epoch 3/20\n",
      "600/600 [==============================] - 1s 871us/step - loss: 0.0057 - accuracy: 0.8017\n",
      "Epoch 4/20\n",
      "600/600 [==============================] - 1s 876us/step - loss: 0.0050 - accuracy: 0.8533\n",
      "Epoch 5/20\n",
      "600/600 [==============================] - 1s 842us/step - loss: 0.0045 - accuracy: 0.8500\n",
      "Epoch 6/20\n",
      "600/600 [==============================] - 1s 842us/step - loss: 0.0041 - accuracy: 0.8617\n",
      "Epoch 7/20\n",
      "600/600 [==============================] - 1s 932us/step - loss: 0.0037 - accuracy: 0.8500\n",
      "Epoch 8/20\n",
      "600/600 [==============================] - 1s 911us/step - loss: 0.0032 - accuracy: 0.8633\n",
      "Epoch 9/20\n",
      "600/600 [==============================] - 0s 829us/step - loss: 0.0029 - accuracy: 0.8717\n",
      "Epoch 10/20\n",
      "600/600 [==============================] - 1s 849us/step - loss: 0.0025 - accuracy: 0.8583\n",
      "Epoch 11/20\n",
      "600/600 [==============================] - 1s 917us/step - loss: 0.0023 - accuracy: 0.8700\n",
      "Epoch 12/20\n",
      "600/600 [==============================] - 1s 882us/step - loss: 0.0022 - accuracy: 0.8700\n",
      "Epoch 13/20\n",
      "600/600 [==============================] - 1s 867us/step - loss: 0.0019 - accuracy: 0.8800\n",
      "Epoch 14/20\n",
      "600/600 [==============================] - 1s 866us/step - loss: 0.0019 - accuracy: 0.8733\n",
      "Epoch 15/20\n",
      "600/600 [==============================] - 1s 889us/step - loss: 0.0018 - accuracy: 0.8750\n",
      "Epoch 16/20\n",
      "600/600 [==============================] - 1s 881us/step - loss: 0.0018 - accuracy: 0.8700\n",
      "Epoch 17/20\n",
      "600/600 [==============================] - 1s 976us/step - loss: 0.0016 - accuracy: 0.8783\n",
      "Epoch 18/20\n",
      "600/600 [==============================] - 1s 1ms/step - loss: 0.0016 - accuracy: 0.8817\n",
      "Epoch 19/20\n",
      "600/600 [==============================] - 1s 952us/step - loss: 0.0016 - accuracy: 0.8900\n",
      "Epoch 20/20\n",
      "600/600 [==============================] - 0s 823us/step - loss: 0.0014 - accuracy: 0.9033\n",
      "4\n",
      "(916, 1, 9)\n",
      "(916, 1, 9)\n",
      "Epoch 1/20\n",
      "916/916 [==============================] - 1s 977us/step - loss: 0.0025 - accuracy: 0.9989\n",
      "Epoch 2/20\n",
      "916/916 [==============================] - 1s 875us/step - loss: 0.0013 - accuracy: 1.0000\n",
      "Epoch 3/20\n",
      "916/916 [==============================] - 1s 856us/step - loss: 0.0011 - accuracy: 1.0000\n",
      "Epoch 4/20\n",
      "916/916 [==============================] - 1s 1ms/step - loss: 9.6119e-04 - accuracy: 1.0000\n",
      "Epoch 5/20\n",
      "916/916 [==============================] - 1s 997us/step - loss: 8.4999e-04 - accuracy: 1.0000\n",
      "Epoch 6/20\n",
      "916/916 [==============================] - 1s 896us/step - loss: 7.8692e-04 - accuracy: 1.0000\n",
      "Epoch 7/20\n",
      "916/916 [==============================] - 1s 953us/step - loss: 7.1238e-04 - accuracy: 1.0000\n",
      "Epoch 8/20\n",
      "916/916 [==============================] - 1s 1ms/step - loss: 6.6621e-04 - accuracy: 1.0000\n",
      "Epoch 9/20\n",
      "916/916 [==============================] - 1s 959us/step - loss: 6.3718e-04 - accuracy: 1.0000\n",
      "Epoch 10/20\n",
      "916/916 [==============================] - 1s 822us/step - loss: 6.0100e-04 - accuracy: 1.0000\n",
      "Epoch 11/20\n",
      "916/916 [==============================] - 1s 979us/step - loss: 5.9328e-04 - accuracy: 1.0000\n",
      "Epoch 12/20\n",
      "916/916 [==============================] - 1s 997us/step - loss: 5.6958e-04 - accuracy: 1.0000\n",
      "Epoch 13/20\n",
      "916/916 [==============================] - 1s 812us/step - loss: 5.5877e-04 - accuracy: 1.0000\n",
      "Epoch 14/20\n",
      "916/916 [==============================] - 1s 816us/step - loss: 5.4551e-04 - accuracy: 1.0000\n",
      "Epoch 15/20\n",
      "916/916 [==============================] - 1s 966us/step - loss: 5.3672e-04 - accuracy: 1.0000\n",
      "Epoch 16/20\n",
      "916/916 [==============================] - 1s 864us/step - loss: 5.0766e-04 - accuracy: 1.0000\n",
      "Epoch 17/20\n",
      "916/916 [==============================] - 1s 895us/step - loss: 5.1089e-04 - accuracy: 1.0000\n",
      "Epoch 18/20\n",
      "916/916 [==============================] - 1s 851us/step - loss: 4.9820e-04 - accuracy: 1.0000\n",
      "Epoch 19/20\n",
      "916/916 [==============================] - 1s 824us/step - loss: 4.9387e-04 - accuracy: 1.0000\n",
      "Epoch 20/20\n",
      "916/916 [==============================] - 1s 836us/step - loss: 4.8082e-04 - accuracy: 1.0000\n",
      "5\n",
      "(600, 1, 9)\n",
      "(600, 1, 9)\n",
      "Epoch 1/20\n",
      "600/600 [==============================] - 0s 819us/step - loss: 7.5230e-04 - accuracy: 1.0000\n",
      "Epoch 2/20\n",
      "600/600 [==============================] - 0s 818us/step - loss: 5.6159e-04 - accuracy: 1.0000\n",
      "Epoch 3/20\n",
      "600/600 [==============================] - 0s 824us/step - loss: 5.1254e-04 - accuracy: 1.0000\n",
      "Epoch 4/20\n",
      "600/600 [==============================] - 1s 871us/step - loss: 4.6137e-04 - accuracy: 1.0000\n",
      "Epoch 5/20\n",
      "600/600 [==============================] - 1s 906us/step - loss: 4.4297e-04 - accuracy: 1.0000\n",
      "Epoch 6/20\n",
      "600/600 [==============================] - 0s 829us/step - loss: 4.3109e-04 - accuracy: 1.0000\n",
      "Epoch 7/20\n",
      "600/600 [==============================] - 1s 944us/step - loss: 4.1810e-04 - accuracy: 1.0000\n",
      "Epoch 8/20\n",
      "600/600 [==============================] - 1s 874us/step - loss: 4.1717e-04 - accuracy: 1.0000\n",
      "Epoch 9/20\n",
      "600/600 [==============================] - 0s 811us/step - loss: 4.1675e-04 - accuracy: 1.0000\n",
      "Epoch 10/20\n",
      "600/600 [==============================] - 1s 879us/step - loss: 4.1059e-04 - accuracy: 1.0000\n",
      "Epoch 11/20\n",
      "600/600 [==============================] - 0s 828us/step - loss: 3.9532e-04 - accuracy: 1.0000\n",
      "Epoch 12/20\n",
      "600/600 [==============================] - 1s 906us/step - loss: 3.8499e-04 - accuracy: 1.0000\n",
      "Epoch 13/20\n",
      "600/600 [==============================] - 1s 901us/step - loss: 3.8808e-04 - accuracy: 1.0000\n",
      "Epoch 14/20\n",
      "600/600 [==============================] - 1s 877us/step - loss: 3.7862e-04 - accuracy: 1.0000\n",
      "Epoch 15/20\n",
      "600/600 [==============================] - 1s 894us/step - loss: 3.7367e-04 - accuracy: 1.0000\n",
      "Epoch 16/20\n",
      "600/600 [==============================] - 1s 962us/step - loss: 3.5825e-04 - accuracy: 1.0000\n",
      "Epoch 17/20\n",
      "600/600 [==============================] - 1s 886us/step - loss: 3.6651e-04 - accuracy: 1.0000\n",
      "Epoch 18/20\n",
      "600/600 [==============================] - 1s 949us/step - loss: 3.7211e-04 - accuracy: 1.0000\n",
      "Epoch 19/20\n",
      "600/600 [==============================] - 1s 929us/step - loss: 3.4840e-04 - accuracy: 1.0000\n",
      "Epoch 20/20\n",
      "600/600 [==============================] - 1s 872us/step - loss: 3.6792e-04 - accuracy: 1.0000\n",
      "6\n",
      "(600, 1, 9)\n",
      "(600, 1, 9)\n",
      "Epoch 1/20\n",
      "600/600 [==============================] - 1s 866us/step - loss: 0.0025 - accuracy: 0.9983\n",
      "Epoch 2/20\n",
      "600/600 [==============================] - 1s 854us/step - loss: 0.0012 - accuracy: 1.0000\n",
      "Epoch 3/20\n",
      "600/600 [==============================] - 1s 849us/step - loss: 0.0011 - accuracy: 1.0000\n",
      "Epoch 4/20\n",
      "600/600 [==============================] - 1s 836us/step - loss: 9.6294e-04 - accuracy: 1.0000\n",
      "Epoch 5/20\n",
      "600/600 [==============================] - 1s 836us/step - loss: 9.1632e-04 - accuracy: 1.0000\n",
      "Epoch 6/20\n",
      "600/600 [==============================] - 1s 836us/step - loss: 9.1622e-04 - accuracy: 1.0000\n",
      "Epoch 7/20\n",
      "600/600 [==============================] - 1s 899us/step - loss: 8.6241e-04 - accuracy: 1.0000\n",
      "Epoch 8/20\n",
      "600/600 [==============================] - 1s 842us/step - loss: 8.3438e-04 - accuracy: 1.0000\n",
      "Epoch 9/20\n",
      "600/600 [==============================] - 1s 844us/step - loss: 8.1626e-04 - accuracy: 1.0000\n",
      "Epoch 10/20\n",
      "600/600 [==============================] - 1s 866us/step - loss: 7.9610e-04 - accuracy: 1.0000\n",
      "Epoch 11/20\n",
      "600/600 [==============================] - 1s 906us/step - loss: 7.7304e-04 - accuracy: 1.0000\n",
      "Epoch 12/20\n",
      "600/600 [==============================] - 1s 947us/step - loss: 7.7631e-04 - accuracy: 1.0000\n",
      "Epoch 13/20\n",
      "600/600 [==============================] - 1s 921us/step - loss: 7.4564e-04 - accuracy: 1.0000\n",
      "Epoch 14/20\n",
      "600/600 [==============================] - 1s 864us/step - loss: 7.4072e-04 - accuracy: 1.0000\n",
      "Epoch 15/20\n",
      "600/600 [==============================] - 0s 831us/step - loss: 7.2197e-04 - accuracy: 1.0000\n",
      "Epoch 16/20\n",
      "600/600 [==============================] - 1s 991us/step - loss: 7.3335e-04 - accuracy: 1.0000\n",
      "Epoch 17/20\n",
      "600/600 [==============================] - 1s 1ms/step - loss: 7.0368e-04 - accuracy: 1.0000\n",
      "Epoch 18/20\n",
      "600/600 [==============================] - 1s 961us/step - loss: 6.9260e-04 - accuracy: 1.0000\n",
      "Epoch 19/20\n",
      "600/600 [==============================] - 1s 1ms/step - loss: 6.8858e-04 - accuracy: 1.0000\n",
      "Epoch 20/20\n",
      "600/600 [==============================] - 1s 941us/step - loss: 6.6429e-04 - accuracy: 1.0000\n",
      "7\n",
      "(934, 1, 9)\n",
      "(934, 1, 9)\n",
      "Epoch 1/20\n",
      "934/934 [==============================] - 1s 940us/step - loss: 0.0049 - accuracy: 1.0000\n",
      "Epoch 2/20\n",
      "934/934 [==============================] - 1s 904us/step - loss: 0.0036 - accuracy: 1.0000\n",
      "Epoch 3/20\n",
      "934/934 [==============================] - 1s 888us/step - loss: 0.0032 - accuracy: 1.0000\n",
      "Epoch 4/20\n",
      "934/934 [==============================] - 1s 851us/step - loss: 0.0030 - accuracy: 1.0000\n",
      "Epoch 5/20\n",
      "934/934 [==============================] - 1s 950us/step - loss: 0.0028 - accuracy: 1.0000\n",
      "Epoch 6/20\n",
      "934/934 [==============================] - 1s 942us/step - loss: 0.0026 - accuracy: 1.0000\n",
      "Epoch 7/20\n",
      "934/934 [==============================] - 1s 866us/step - loss: 0.0025 - accuracy: 1.0000\n",
      "Epoch 8/20\n",
      "934/934 [==============================] - 1s 871us/step - loss: 0.0023 - accuracy: 1.0000\n",
      "Epoch 9/20\n",
      "934/934 [==============================] - 1s 881us/step - loss: 0.0023 - accuracy: 1.0000\n",
      "Epoch 10/20\n",
      "934/934 [==============================] - 1s 870us/step - loss: 0.0022 - accuracy: 1.0000\n",
      "Epoch 11/20\n",
      "934/934 [==============================] - 1s 910us/step - loss: 0.0022 - accuracy: 1.0000\n",
      "Epoch 12/20\n",
      "934/934 [==============================] - 1s 888us/step - loss: 0.0022 - accuracy: 1.0000\n",
      "Epoch 13/20\n",
      "934/934 [==============================] - 1s 881us/step - loss: 0.0020 - accuracy: 1.0000\n",
      "Epoch 14/20\n",
      "934/934 [==============================] - 1s 857us/step - loss: 0.0020 - accuracy: 1.0000\n",
      "Epoch 15/20\n",
      "934/934 [==============================] - 1s 903us/step - loss: 0.0020 - accuracy: 1.0000\n",
      "Epoch 16/20\n",
      "934/934 [==============================] - 1s 1ms/step - loss: 0.0020 - accuracy: 1.0000\n",
      "Epoch 17/20\n",
      "934/934 [==============================] - 1s 1ms/step - loss: 0.0020 - accuracy: 1.0000\n",
      "Epoch 18/20\n",
      "934/934 [==============================] - 1s 1ms/step - loss: 0.0019 - accuracy: 1.0000\n",
      "Epoch 19/20\n",
      "934/934 [==============================] - 1s 2ms/step - loss: 0.0019 - accuracy: 1.0000\n",
      "Epoch 20/20\n",
      "934/934 [==============================] - 1s 2ms/step - loss: 0.0019 - accuracy: 1.0000\n",
      "8\n",
      "(600, 1, 9)\n",
      "(600, 1, 9)\n",
      "Epoch 1/20\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0020 - accuracy: 1.0000\n",
      "Epoch 2/20\n",
      "600/600 [==============================] - 1s 1ms/step - loss: 0.0016 - accuracy: 1.0000\n",
      "Epoch 3/20\n",
      "600/600 [==============================] - 1s 884us/step - loss: 0.0014 - accuracy: 1.0000\n",
      "Epoch 4/20\n",
      "600/600 [==============================] - 1s 1ms/step - loss: 0.0013 - accuracy: 1.0000\n",
      "Epoch 5/20\n",
      "600/600 [==============================] - 1s 901us/step - loss: 0.0013 - accuracy: 1.0000\n",
      "Epoch 6/20\n",
      "600/600 [==============================] - 1s 957us/step - loss: 0.0011 - accuracy: 1.0000\n",
      "Epoch 7/20\n",
      "600/600 [==============================] - 1s 1ms/step - loss: 0.0011 - accuracy: 1.0000\n",
      "Epoch 8/20\n",
      "600/600 [==============================] - 1s 891us/step - loss: 0.0010 - accuracy: 1.0000\n",
      "Epoch 9/20\n",
      "600/600 [==============================] - 1s 1ms/step - loss: 9.7667e-04 - accuracy: 1.0000\n",
      "Epoch 10/20\n",
      "600/600 [==============================] - 1s 1ms/step - loss: 9.1854e-04 - accuracy: 1.0000\n",
      "Epoch 11/20\n",
      "600/600 [==============================] - 1s 1ms/step - loss: 9.1670e-04 - accuracy: 1.0000\n",
      "Epoch 12/20\n",
      "600/600 [==============================] - 1s 1ms/step - loss: 9.0493e-04 - accuracy: 1.0000\n",
      "Epoch 13/20\n",
      "600/600 [==============================] - 1s 1ms/step - loss: 8.7519e-04 - accuracy: 1.0000\n",
      "Epoch 14/20\n",
      "600/600 [==============================] - 1s 896us/step - loss: 8.2329e-04 - accuracy: 1.0000\n",
      "Epoch 15/20\n",
      "600/600 [==============================] - 1s 855us/step - loss: 8.0610e-04 - accuracy: 1.0000\n",
      "Epoch 16/20\n",
      "600/600 [==============================] - 1s 886us/step - loss: 8.1453e-04 - accuracy: 1.0000\n",
      "Epoch 17/20\n",
      "600/600 [==============================] - 1s 1ms/step - loss: 7.6512e-04 - accuracy: 1.0000\n",
      "Epoch 18/20\n",
      "600/600 [==============================] - 1s 1ms/step - loss: 7.4304e-04 - accuracy: 1.0000\n",
      "Epoch 19/20\n",
      "600/600 [==============================] - 1s 1ms/step - loss: 7.5881e-04 - accuracy: 1.0000\n",
      "Epoch 20/20\n",
      "600/600 [==============================] - 1s 929us/step - loss: 7.4898e-04 - accuracy: 1.0000\n",
      "9\n",
      "(600, 1, 9)\n",
      "(600, 1, 9)\n",
      "Epoch 1/20\n",
      "600/600 [==============================] - 1s 1ms/step - loss: 0.0018 - accuracy: 1.0000\n",
      "Epoch 2/20\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0011 - accuracy: 1.0000\n",
      "Epoch 3/20\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 9.3776e-04 - accuracy: 1.0000\n",
      "Epoch 4/20\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 8.3660e-04 - accuracy: 1.0000\n",
      "Epoch 5/20\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 8.1431e-04 - accuracy: 1.0000\n",
      "Epoch 6/20\n",
      "600/600 [==============================] - 1s 1ms/step - loss: 7.4275e-04 - accuracy: 1.0000\n",
      "Epoch 7/20\n",
      "600/600 [==============================] - 1s 1ms/step - loss: 7.1147e-04 - accuracy: 1.0000\n",
      "Epoch 8/20\n",
      "600/600 [==============================] - 1s 1ms/step - loss: 6.8859e-04 - accuracy: 1.0000\n",
      "Epoch 9/20\n",
      "600/600 [==============================] - 1s 1ms/step - loss: 6.6691e-04 - accuracy: 1.0000\n",
      "Epoch 10/20\n",
      "600/600 [==============================] - 1s 1ms/step - loss: 6.3582e-04 - accuracy: 1.0000\n",
      "Epoch 11/20\n",
      "600/600 [==============================] - 1s 1ms/step - loss: 6.1458e-04 - accuracy: 1.0000\n",
      "Epoch 12/20\n",
      "600/600 [==============================] - 1s 1ms/step - loss: 6.3436e-04 - accuracy: 1.0000\n",
      "Epoch 13/20\n",
      "600/600 [==============================] - 1s 959us/step - loss: 6.1041e-04 - accuracy: 1.0000\n",
      "Epoch 14/20\n",
      "600/600 [==============================] - 1s 1ms/step - loss: 5.8522e-04 - accuracy: 1.0000\n",
      "Epoch 15/20\n",
      "600/600 [==============================] - 1s 1ms/step - loss: 5.7624e-04 - accuracy: 1.0000\n",
      "Epoch 16/20\n",
      "600/600 [==============================] - 1s 939us/step - loss: 5.9365e-04 - accuracy: 1.0000\n",
      "Epoch 17/20\n",
      "600/600 [==============================] - 1s 937us/step - loss: 5.4859e-04 - accuracy: 1.0000\n",
      "Epoch 18/20\n",
      "600/600 [==============================] - 1s 924us/step - loss: 5.6268e-04 - accuracy: 1.0000\n",
      "Epoch 19/20\n",
      "600/600 [==============================] - 1s 979us/step - loss: 5.3682e-04 - accuracy: 1.0000\n",
      "Epoch 20/20\n",
      "600/600 [==============================] - 1s 997us/step - loss: 5.4891e-04 - accuracy: 1.0000\n",
      "10\n",
      "(600, 1, 9)\n",
      "(600, 1, 9)\n",
      "Epoch 1/20\n",
      "600/600 [==============================] - 1s 901us/step - loss: 0.0015 - accuracy: 1.0000\n",
      "Epoch 2/20\n",
      "600/600 [==============================] - 1s 861us/step - loss: 0.0011 - accuracy: 1.0000\n",
      "Epoch 3/20\n",
      "600/600 [==============================] - 1s 867us/step - loss: 0.0010 - accuracy: 1.0000\n",
      "Epoch 4/20\n",
      "600/600 [==============================] - 1s 906us/step - loss: 9.2697e-04 - accuracy: 1.0000\n",
      "Epoch 5/20\n",
      "600/600 [==============================] - 1s 921us/step - loss: 8.9822e-04 - accuracy: 1.0000\n",
      "Epoch 6/20\n",
      "600/600 [==============================] - 1s 924us/step - loss: 8.3971e-04 - accuracy: 1.0000\n",
      "Epoch 7/20\n",
      "600/600 [==============================] - 1s 884us/step - loss: 8.0275e-04 - accuracy: 1.0000\n",
      "Epoch 8/20\n",
      "600/600 [==============================] - 1s 899us/step - loss: 8.2627e-04 - accuracy: 1.0000\n",
      "Epoch 9/20\n",
      "600/600 [==============================] - 1s 894us/step - loss: 8.3555e-04 - accuracy: 1.0000\n",
      "Epoch 10/20\n",
      "600/600 [==============================] - 1s 864us/step - loss: 8.1080e-04 - accuracy: 1.0000\n",
      "Epoch 11/20\n",
      "600/600 [==============================] - 1s 877us/step - loss: 7.7326e-04 - accuracy: 1.0000\n",
      "Epoch 12/20\n",
      "600/600 [==============================] - 1s 872us/step - loss: 7.2367e-04 - accuracy: 1.0000\n",
      "Epoch 13/20\n",
      "600/600 [==============================] - 1s 906us/step - loss: 7.2728e-04 - accuracy: 1.0000\n",
      "Epoch 14/20\n",
      "600/600 [==============================] - 1s 1ms/step - loss: 7.1903e-04 - accuracy: 1.0000\n",
      "Epoch 15/20\n",
      "600/600 [==============================] - 1s 1ms/step - loss: 6.9924e-04 - accuracy: 1.0000\n",
      "Epoch 16/20\n",
      "600/600 [==============================] - 1s 1ms/step - loss: 7.0910e-04 - accuracy: 1.0000\n",
      "Epoch 17/20\n",
      "600/600 [==============================] - 1s 1ms/step - loss: 6.8436e-04 - accuracy: 1.0000\n",
      "Epoch 18/20\n",
      "600/600 [==============================] - 1s 996us/step - loss: 6.7752e-04 - accuracy: 1.0000\n",
      "Epoch 19/20\n",
      "600/600 [==============================] - 1s 869us/step - loss: 6.8535e-04 - accuracy: 1.0000\n",
      "Epoch 20/20\n",
      "600/600 [==============================] - 1s 897us/step - loss: 6.4519e-04 - accuracy: 1.0000\n",
      "11\n",
      "(600, 1, 9)\n",
      "(600, 1, 9)\n",
      "Epoch 1/20\n",
      "600/600 [==============================] - 1s 934us/step - loss: 0.0068 - accuracy: 0.9667\n",
      "Epoch 2/20\n",
      "600/600 [==============================] - 1s 876us/step - loss: 0.0019 - accuracy: 1.0000\n",
      "Epoch 3/20\n",
      "600/600 [==============================] - 1s 906us/step - loss: 0.0014 - accuracy: 1.0000\n",
      "Epoch 4/20\n",
      "600/600 [==============================] - 1s 871us/step - loss: 0.0012 - accuracy: 1.0000\n",
      "Epoch 5/20\n",
      "600/600 [==============================] - 1s 1ms/step - loss: 9.1701e-04 - accuracy: 1.0000\n",
      "Epoch 6/20\n",
      "600/600 [==============================] - 1s 1ms/step - loss: 7.8043e-04 - accuracy: 1.0000\n",
      "Epoch 7/20\n",
      "600/600 [==============================] - 1s 1ms/step - loss: 6.3668e-04 - accuracy: 1.0000\n",
      "Epoch 8/20\n",
      "600/600 [==============================] - 1s 882us/step - loss: 5.5265e-04 - accuracy: 1.0000\n",
      "Epoch 9/20\n",
      "600/600 [==============================] - 1s 907us/step - loss: 5.0830e-04 - accuracy: 1.0000\n",
      "Epoch 10/20\n",
      "600/600 [==============================] - 1s 907us/step - loss: 4.6115e-04 - accuracy: 1.0000\n",
      "Epoch 11/20\n",
      "600/600 [==============================] - 1s 911us/step - loss: 3.9875e-04 - accuracy: 1.0000\n",
      "Epoch 12/20\n",
      "600/600 [==============================] - 1s 941us/step - loss: 3.8546e-04 - accuracy: 1.0000\n",
      "Epoch 13/20\n",
      "600/600 [==============================] - 1s 922us/step - loss: 3.6678e-04 - accuracy: 1.0000\n",
      "Epoch 14/20\n",
      "600/600 [==============================] - 1s 922us/step - loss: 3.3012e-04 - accuracy: 1.0000\n",
      "Epoch 15/20\n",
      "600/600 [==============================] - 1s 942us/step - loss: 3.0275e-04 - accuracy: 1.0000\n",
      "Epoch 16/20\n",
      "600/600 [==============================] - 1s 914us/step - loss: 3.2414e-04 - accuracy: 1.0000\n",
      "Epoch 17/20\n",
      "600/600 [==============================] - 1s 911us/step - loss: 3.0287e-04 - accuracy: 1.0000\n",
      "Epoch 18/20\n",
      "600/600 [==============================] - 1s 884us/step - loss: 2.7003e-04 - accuracy: 1.0000\n",
      "Epoch 19/20\n",
      "600/600 [==============================] - 1s 952us/step - loss: 2.9592e-04 - accuracy: 1.0000\n",
      "Epoch 20/20\n",
      "600/600 [==============================] - 1s 1ms/step - loss: 2.9491e-04 - accuracy: 1.0000\n",
      "12\n",
      "(600, 1, 9)\n",
      "(600, 1, 9)\n",
      "Epoch 1/20\n",
      "600/600 [==============================] - 1s 956us/step - loss: 0.0069 - accuracy: 0.9883\n",
      "Epoch 2/20\n",
      "600/600 [==============================] - 1s 969us/step - loss: 0.0030 - accuracy: 1.0000\n",
      "Epoch 3/20\n",
      "600/600 [==============================] - 1s 936us/step - loss: 0.0027 - accuracy: 1.0000\n",
      "Epoch 4/20\n",
      "600/600 [==============================] - 1s 1ms/step - loss: 0.0024 - accuracy: 1.0000\n",
      "Epoch 5/20\n",
      "600/600 [==============================] - 1s 1ms/step - loss: 0.0023 - accuracy: 1.0000\n",
      "Epoch 6/20\n",
      "600/600 [==============================] - 1s 1ms/step - loss: 0.0022 - accuracy: 1.0000\n",
      "Epoch 7/20\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0021 - accuracy: 1.0000\n",
      "Epoch 8/20\n",
      "600/600 [==============================] - 1s 1ms/step - loss: 0.0021 - accuracy: 1.0000\n",
      "Epoch 9/20\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0020 - accuracy: 1.0000\n",
      "Epoch 10/20\n",
      "600/600 [==============================] - 1s 1ms/step - loss: 0.0019 - accuracy: 1.0000\n",
      "Epoch 11/20\n",
      "600/600 [==============================] - 1s 1ms/step - loss: 0.0019 - accuracy: 1.0000\n",
      "Epoch 12/20\n",
      "600/600 [==============================] - 1s 1ms/step - loss: 0.0018 - accuracy: 1.0000\n",
      "Epoch 13/20\n",
      "600/600 [==============================] - 1s 1ms/step - loss: 0.0018 - accuracy: 1.0000\n",
      "Epoch 14/20\n",
      "600/600 [==============================] - 1s 1ms/step - loss: 0.0017 - accuracy: 1.0000\n",
      "Epoch 15/20\n",
      "600/600 [==============================] - 1s 876us/step - loss: 0.0017 - accuracy: 1.0000\n",
      "Epoch 16/20\n",
      "600/600 [==============================] - 1s 949us/step - loss: 0.0016 - accuracy: 1.0000\n",
      "Epoch 17/20\n",
      "600/600 [==============================] - 1s 881us/step - loss: 0.0016 - accuracy: 1.0000\n",
      "Epoch 18/20\n",
      "600/600 [==============================] - 1s 880us/step - loss: 0.0015 - accuracy: 1.0000\n",
      "Epoch 19/20\n",
      "600/600 [==============================] - 1s 874us/step - loss: 0.0015 - accuracy: 1.0000\n",
      "Epoch 20/20\n",
      "600/600 [==============================] - 1s 842us/step - loss: 0.0015 - accuracy: 1.0000\n",
      "13\n",
      "(600, 1, 9)\n",
      "(600, 1, 9)\n",
      "Epoch 1/20\n",
      "600/600 [==============================] - 1s 1ms/step - loss: 0.0167 - accuracy: 0.7983\n",
      "Epoch 2/20\n",
      "600/600 [==============================] - 1s 1ms/step - loss: 0.0043 - accuracy: 0.9300\n",
      "Epoch 3/20\n",
      "600/600 [==============================] - 1s 887us/step - loss: 0.0030 - accuracy: 0.9467\n",
      "Epoch 4/20\n",
      "600/600 [==============================] - 0s 818us/step - loss: 0.0023 - accuracy: 0.9450\n",
      "Epoch 5/20\n",
      "600/600 [==============================] - 1s 872us/step - loss: 0.0018 - accuracy: 0.9500\n",
      "Epoch 6/20\n",
      "600/600 [==============================] - 1s 836us/step - loss: 0.0014 - accuracy: 0.9567\n",
      "Epoch 7/20\n",
      "600/600 [==============================] - 1s 1ms/step - loss: 0.0012 - accuracy: 0.9500\n",
      "Epoch 8/20\n",
      "600/600 [==============================] - 1s 859us/step - loss: 9.9486e-04 - accuracy: 0.9567\n",
      "Epoch 9/20\n",
      "600/600 [==============================] - 1s 1ms/step - loss: 8.3662e-04 - accuracy: 0.9600\n",
      "Epoch 10/20\n",
      "600/600 [==============================] - 1s 967us/step - loss: 7.3169e-04 - accuracy: 0.9550\n",
      "Epoch 11/20\n",
      "600/600 [==============================] - 1s 1ms/step - loss: 6.2363e-04 - accuracy: 0.9517\n",
      "Epoch 12/20\n",
      "600/600 [==============================] - 1s 971us/step - loss: 5.7760e-04 - accuracy: 0.9567\n",
      "Epoch 13/20\n",
      "600/600 [==============================] - 1s 927us/step - loss: 5.1129e-04 - accuracy: 0.9500\n",
      "Epoch 14/20\n",
      "600/600 [==============================] - 1s 1ms/step - loss: 4.7934e-04 - accuracy: 0.9483\n",
      "Epoch 15/20\n",
      "600/600 [==============================] - 1s 1ms/step - loss: 4.4582e-04 - accuracy: 0.9517\n",
      "Epoch 16/20\n",
      "600/600 [==============================] - 1s 877us/step - loss: 4.0685e-04 - accuracy: 0.9533\n",
      "Epoch 17/20\n",
      "600/600 [==============================] - 1s 1ms/step - loss: 3.8378e-04 - accuracy: 0.9517\n",
      "Epoch 18/20\n",
      "600/600 [==============================] - 1s 1ms/step - loss: 3.6746e-04 - accuracy: 0.9567\n",
      "Epoch 19/20\n",
      "600/600 [==============================] - 1s 874us/step - loss: 3.5248e-04 - accuracy: 0.9567\n",
      "Epoch 20/20\n",
      "600/600 [==============================] - 1s 841us/step - loss: 3.2742e-04 - accuracy: 0.9567\n",
      "14\n",
      "(1103, 1, 9)\n",
      "(1103, 1, 9)\n",
      "Epoch 1/20\n",
      "1103/1103 [==============================] - 1s 1ms/step - loss: 0.0013 - accuracy: 1.0000\n",
      "Epoch 2/20\n",
      "1103/1103 [==============================] - 1s 1ms/step - loss: 3.5832e-04 - accuracy: 1.0000\n",
      "Epoch 3/20\n",
      "1103/1103 [==============================] - 1s 1ms/step - loss: 2.6651e-04 - accuracy: 1.0000\n",
      "Epoch 4/20\n",
      "1103/1103 [==============================] - 1s 1ms/step - loss: 2.2198e-04 - accuracy: 1.0000\n",
      "Epoch 5/20\n",
      "1103/1103 [==============================] - 1s 897us/step - loss: 2.0092e-04 - accuracy: 1.0000\n",
      "Epoch 6/20\n",
      "1103/1103 [==============================] - 1s 993us/step - loss: 1.7888e-04 - accuracy: 1.0000\n",
      "Epoch 7/20\n",
      "1103/1103 [==============================] - 1s 1ms/step - loss: 1.6251e-04 - accuracy: 1.0000\n",
      "Epoch 8/20\n",
      "1103/1103 [==============================] - 1s 968us/step - loss: 1.4920e-04 - accuracy: 1.0000\n",
      "Epoch 9/20\n",
      "1103/1103 [==============================] - 1s 922us/step - loss: 1.3696e-04 - accuracy: 1.0000\n",
      "Epoch 10/20\n",
      "1103/1103 [==============================] - 1s 917us/step - loss: 1.2123e-04 - accuracy: 1.0000\n",
      "Epoch 11/20\n",
      "1103/1103 [==============================] - 1s 920us/step - loss: 1.1443e-04 - accuracy: 1.0000\n",
      "Epoch 12/20\n",
      "1103/1103 [==============================] - 1s 901us/step - loss: 1.0867e-04 - accuracy: 1.0000\n",
      "Epoch 13/20\n",
      "1103/1103 [==============================] - 1s 969us/step - loss: 9.5969e-05 - accuracy: 1.0000\n",
      "Epoch 14/20\n",
      "1103/1103 [==============================] - 1s 1ms/step - loss: 9.7227e-05 - accuracy: 1.0000\n",
      "Epoch 15/20\n",
      "1103/1103 [==============================] - 1s 838us/step - loss: 8.7906e-05 - accuracy: 1.0000\n",
      "Epoch 16/20\n",
      "1103/1103 [==============================] - 1s 843us/step - loss: 8.6355e-05 - accuracy: 1.0000\n",
      "Epoch 17/20\n",
      "1103/1103 [==============================] - 1s 825us/step - loss: 8.4882e-05 - accuracy: 1.0000\n",
      "Epoch 18/20\n",
      "1103/1103 [==============================] - 1s 924us/step - loss: 7.9245e-05 - accuracy: 1.0000\n",
      "Epoch 19/20\n",
      "1103/1103 [==============================] - 1s 936us/step - loss: 8.1988e-05 - accuracy: 1.0000\n",
      "Epoch 20/20\n",
      "1103/1103 [==============================] - 1s 872us/step - loss: 7.5086e-05 - accuracy: 1.0000\n",
      "15\n",
      "(600, 1, 9)\n",
      "(600, 1, 9)\n",
      "Epoch 1/20\n",
      "600/600 [==============================] - 1s 867us/step - loss: 3.1750e-04 - accuracy: 1.0000\n",
      "Epoch 2/20\n",
      "600/600 [==============================] - 1s 909us/step - loss: 1.7141e-04 - accuracy: 1.0000\n",
      "Epoch 3/20\n",
      "600/600 [==============================] - 1s 914us/step - loss: 1.2633e-04 - accuracy: 1.0000\n",
      "Epoch 4/20\n",
      "600/600 [==============================] - 1s 1ms/step - loss: 1.0781e-04 - accuracy: 1.0000\n",
      "Epoch 5/20\n",
      "600/600 [==============================] - 1s 1ms/step - loss: 9.7736e-05 - accuracy: 1.0000\n",
      "Epoch 6/20\n",
      "600/600 [==============================] - 1s 982us/step - loss: 8.4442e-05 - accuracy: 1.0000\n",
      "Epoch 7/20\n",
      "600/600 [==============================] - 1s 1ms/step - loss: 7.9457e-05 - accuracy: 1.0000\n",
      "Epoch 8/20\n",
      "600/600 [==============================] - 1s 1ms/step - loss: 7.6958e-05 - accuracy: 1.0000\n",
      "Epoch 9/20\n",
      "600/600 [==============================] - 1s 947us/step - loss: 7.6124e-05 - accuracy: 1.0000\n",
      "Epoch 10/20\n",
      "600/600 [==============================] - 1s 1ms/step - loss: 6.5906e-05 - accuracy: 1.0000\n",
      "Epoch 11/20\n",
      "600/600 [==============================] - 1s 901us/step - loss: 6.6629e-05 - accuracy: 1.0000\n",
      "Epoch 12/20\n",
      "600/600 [==============================] - 1s 997us/step - loss: 6.5388e-05 - accuracy: 1.0000\n",
      "Epoch 13/20\n",
      "600/600 [==============================] - 1s 922us/step - loss: 6.5504e-05 - accuracy: 1.0000\n",
      "Epoch 14/20\n",
      "600/600 [==============================] - 1s 969us/step - loss: 6.3148e-05 - accuracy: 1.0000\n",
      "Epoch 15/20\n",
      "600/600 [==============================] - 1s 879us/step - loss: 5.6990e-05 - accuracy: 1.0000\n",
      "Epoch 16/20\n",
      "600/600 [==============================] - 1s 1ms/step - loss: 5.6481e-05 - accuracy: 1.0000\n",
      "Epoch 17/20\n",
      "600/600 [==============================] - 1s 1ms/step - loss: 5.4296e-05 - accuracy: 1.0000\n",
      "Epoch 18/20\n",
      "600/600 [==============================] - 1s 914us/step - loss: 5.4239e-05 - accuracy: 1.0000\n",
      "Epoch 19/20\n",
      "600/600 [==============================] - 1s 866us/step - loss: 5.3873e-05 - accuracy: 1.0000\n",
      "Epoch 20/20\n",
      "600/600 [==============================] - 1s 927us/step - loss: 5.5627e-05 - accuracy: 1.0000\n",
      "16\n",
      "(600, 1, 9)\n",
      "(600, 1, 9)\n",
      "Epoch 1/20\n",
      "600/600 [==============================] - 1s 954us/step - loss: 2.2985e-04 - accuracy: 1.0000\n",
      "Epoch 2/20\n",
      "600/600 [==============================] - 1s 851us/step - loss: 1.2467e-04 - accuracy: 1.0000\n",
      "Epoch 3/20\n",
      "600/600 [==============================] - 1s 951us/step - loss: 1.0141e-04 - accuracy: 1.0000\n",
      "Epoch 4/20\n",
      "600/600 [==============================] - 1s 952us/step - loss: 7.9985e-05 - accuracy: 1.0000\n",
      "Epoch 5/20\n",
      "600/600 [==============================] - 1s 894us/step - loss: 7.6751e-05 - accuracy: 1.0000\n",
      "Epoch 6/20\n",
      "600/600 [==============================] - 1s 934us/step - loss: 7.0534e-05 - accuracy: 1.0000\n",
      "Epoch 7/20\n",
      "600/600 [==============================] - 1s 906us/step - loss: 6.5886e-05 - accuracy: 1.0000\n",
      "Epoch 8/20\n",
      "600/600 [==============================] - 1s 866us/step - loss: 6.9149e-05 - accuracy: 1.0000\n",
      "Epoch 9/20\n",
      "600/600 [==============================] - 1s 991us/step - loss: 6.5360e-05 - accuracy: 1.0000\n",
      "Epoch 10/20\n",
      "600/600 [==============================] - 1s 1ms/step - loss: 6.0639e-05 - accuracy: 1.0000\n",
      "Epoch 11/20\n",
      "600/600 [==============================] - 1s 879us/step - loss: 5.8274e-05 - accuracy: 1.0000\n",
      "Epoch 12/20\n",
      "600/600 [==============================] - 1s 864us/step - loss: 5.3956e-05 - accuracy: 1.0000\n",
      "Epoch 13/20\n",
      "600/600 [==============================] - 1s 971us/step - loss: 5.8295e-05 - accuracy: 1.0000\n",
      "Epoch 14/20\n",
      "600/600 [==============================] - 1s 864us/step - loss: 5.5074e-05 - accuracy: 1.0000\n",
      "Epoch 15/20\n",
      "600/600 [==============================] - 1s 1ms/step - loss: 5.0756e-05 - accuracy: 1.0000\n",
      "Epoch 16/20\n",
      "600/600 [==============================] - 1s 1ms/step - loss: 4.8796e-05 - accuracy: 1.0000\n",
      "Epoch 17/20\n",
      "600/600 [==============================] - 1s 1ms/step - loss: 5.2516e-05 - accuracy: 1.0000\n",
      "Epoch 18/20\n",
      "600/600 [==============================] - 1s 1ms/step - loss: 5.0312e-05 - accuracy: 1.0000\n",
      "Epoch 19/20\n",
      "600/600 [==============================] - 1s 926us/step - loss: 4.7985e-05 - accuracy: 1.0000\n",
      "Epoch 20/20\n",
      "600/600 [==============================] - 1s 1ms/step - loss: 5.0612e-05 - accuracy: 1.0000\n",
      "17\n",
      "(972, 1, 9)\n",
      "(972, 1, 9)\n",
      "Epoch 1/20\n",
      "972/972 [==============================] - 1s 1ms/step - loss: 0.0083 - accuracy: 0.9949\n",
      "Epoch 2/20\n",
      "972/972 [==============================] - 1s 1ms/step - loss: 0.0033 - accuracy: 1.0000\n",
      "Epoch 3/20\n",
      "972/972 [==============================] - 1s 1ms/step - loss: 0.0026 - accuracy: 1.0000\n",
      "Epoch 4/20\n",
      "972/972 [==============================] - 1s 940us/step - loss: 0.0022 - accuracy: 1.0000\n",
      "Epoch 5/20\n",
      "972/972 [==============================] - 1s 908us/step - loss: 0.0019 - accuracy: 1.0000\n",
      "Epoch 6/20\n",
      "972/972 [==============================] - 1s 929us/step - loss: 0.0017 - accuracy: 1.0000\n",
      "Epoch 7/20\n",
      "972/972 [==============================] - 1s 898us/step - loss: 0.0015 - accuracy: 1.0000\n",
      "Epoch 8/20\n",
      "972/972 [==============================] - 1s 959us/step - loss: 0.0014 - accuracy: 1.0000\n",
      "Epoch 9/20\n",
      "972/972 [==============================] - 1s 1ms/step - loss: 0.0013 - accuracy: 1.0000\n",
      "Epoch 10/20\n",
      "972/972 [==============================] - 1s 1ms/step - loss: 0.0012 - accuracy: 1.0000\n",
      "Epoch 11/20\n",
      "972/972 [==============================] - 1s 1ms/step - loss: 0.0011 - accuracy: 1.0000\n",
      "Epoch 12/20\n",
      "972/972 [==============================] - 1s 1ms/step - loss: 0.0011 - accuracy: 1.0000\n",
      "Epoch 13/20\n",
      "972/972 [==============================] - 2s 2ms/step - loss: 0.0010 - accuracy: 1.0000\n",
      "Epoch 14/20\n",
      "972/972 [==============================] - 1s 1ms/step - loss: 9.7359e-04 - accuracy: 1.0000\n",
      "Epoch 15/20\n",
      "972/972 [==============================] - 1s 1ms/step - loss: 9.4791e-04 - accuracy: 1.0000\n",
      "Epoch 16/20\n",
      "972/972 [==============================] - 1s 975us/step - loss: 8.9904e-04 - accuracy: 1.0000\n",
      "Epoch 17/20\n",
      "972/972 [==============================] - 1s 967us/step - loss: 8.6671e-04 - accuracy: 1.0000\n",
      "Epoch 18/20\n",
      "972/972 [==============================] - 1s 883us/step - loss: 8.4149e-04 - accuracy: 1.0000\n",
      "Epoch 19/20\n",
      "972/972 [==============================] - 1s 935us/step - loss: 8.1341e-04 - accuracy: 1.0000\n",
      "Epoch 20/20\n",
      "972/972 [==============================] - 1s 940us/step - loss: 7.6531e-04 - accuracy: 1.0000\n",
      "18\n",
      "(1078, 1, 9)\n",
      "(1078, 1, 9)\n",
      "Epoch 1/20\n",
      "1078/1078 [==============================] - 1s 917us/step - loss: 0.0124 - accuracy: 0.9573\n",
      "Epoch 2/20\n",
      "1078/1078 [==============================] - 1s 894us/step - loss: 0.0062 - accuracy: 0.9917\n",
      "Epoch 3/20\n",
      "1078/1078 [==============================] - 1s 900us/step - loss: 0.0052 - accuracy: 0.9898\n",
      "Epoch 4/20\n",
      "1078/1078 [==============================] - 1s 893us/step - loss: 0.0046 - accuracy: 0.9898\n",
      "Epoch 5/20\n",
      "1078/1078 [==============================] - 1s 878us/step - loss: 0.0042 - accuracy: 0.9917\n",
      "Epoch 6/20\n",
      "1078/1078 [==============================] - 1s 904us/step - loss: 0.0039 - accuracy: 0.9898\n",
      "Epoch 7/20\n",
      "1078/1078 [==============================] - 1s 899us/step - loss: 0.0037 - accuracy: 0.9935\n",
      "Epoch 8/20\n",
      "1078/1078 [==============================] - 1s 890us/step - loss: 0.0035 - accuracy: 0.9926\n",
      "Epoch 9/20\n",
      "1078/1078 [==============================] - 1s 901us/step - loss: 0.0034 - accuracy: 0.9944\n",
      "Epoch 10/20\n",
      "1078/1078 [==============================] - 1s 939us/step - loss: 0.0032 - accuracy: 0.9917\n",
      "Epoch 11/20\n",
      "1078/1078 [==============================] - 1s 912us/step - loss: 0.0031 - accuracy: 0.9926\n",
      "Epoch 12/20\n",
      "1078/1078 [==============================] - 1s 895us/step - loss: 0.0030 - accuracy: 0.9917\n",
      "Epoch 13/20\n",
      "1078/1078 [==============================] - 1s 1ms/step - loss: 0.0029 - accuracy: 0.9944\n",
      "Epoch 14/20\n",
      "1078/1078 [==============================] - 1s 1ms/step - loss: 0.0028 - accuracy: 0.9917\n",
      "Epoch 15/20\n",
      "1078/1078 [==============================] - 2s 2ms/step - loss: 0.0027 - accuracy: 0.9935\n",
      "Epoch 16/20\n",
      "1078/1078 [==============================] - 2s 2ms/step - loss: 0.0026 - accuracy: 0.9954\n",
      "Epoch 17/20\n",
      "1078/1078 [==============================] - 2s 1ms/step - loss: 0.0026 - accuracy: 0.9972\n",
      "Epoch 18/20\n",
      "1078/1078 [==============================] - 1s 850us/step - loss: 0.0024 - accuracy: 0.9963\n",
      "Epoch 19/20\n",
      "1078/1078 [==============================] - 1s 887us/step - loss: 0.0023 - accuracy: 0.9944\n",
      "Epoch 20/20\n",
      "1078/1078 [==============================] - 1s 872us/step - loss: 0.0022 - accuracy: 0.9935\n",
      "19\n",
      "(600, 1, 9)\n",
      "(600, 1, 9)\n",
      "Epoch 1/20\n",
      "600/600 [==============================] - 1s 1ms/step - loss: 0.0070 - accuracy: 0.9950\n",
      "Epoch 2/20\n",
      "600/600 [==============================] - 1s 1ms/step - loss: 0.0022 - accuracy: 1.0000\n",
      "Epoch 3/20\n",
      "600/600 [==============================] - 1s 1ms/step - loss: 0.0016 - accuracy: 1.0000\n",
      "Epoch 4/20\n",
      "600/600 [==============================] - 1s 986us/step - loss: 0.0013 - accuracy: 1.0000\n",
      "Epoch 5/20\n",
      "600/600 [==============================] - 1s 1ms/step - loss: 0.0011 - accuracy: 1.0000\n",
      "Epoch 6/20\n",
      "600/600 [==============================] - 1s 1ms/step - loss: 9.8294e-04 - accuracy: 1.0000\n",
      "Epoch 7/20\n",
      "600/600 [==============================] - 1s 1ms/step - loss: 9.2474e-04 - accuracy: 1.0000\n",
      "Epoch 8/20\n",
      "600/600 [==============================] - 1s 1ms/step - loss: 8.1049e-04 - accuracy: 1.0000\n",
      "Epoch 9/20\n",
      "600/600 [==============================] - 1s 1ms/step - loss: 7.4514e-04 - accuracy: 1.0000\n",
      "Epoch 10/20\n",
      "600/600 [==============================] - 1s 1ms/step - loss: 7.0071e-04 - accuracy: 1.0000\n",
      "Epoch 11/20\n",
      "600/600 [==============================] - 1s 1ms/step - loss: 6.8119e-04 - accuracy: 1.0000\n",
      "Epoch 12/20\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 6.5080e-04 - accuracy: 1.0000\n",
      "Epoch 13/20\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 6.0985e-04 - accuracy: 1.0000\n",
      "Epoch 14/20\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 5.6432e-04 - accuracy: 1.0000\n",
      "Epoch 15/20\n",
      "600/600 [==============================] - 1s 1ms/step - loss: 5.1652e-04 - accuracy: 1.0000\n",
      "Epoch 16/20\n",
      "600/600 [==============================] - 1s 944us/step - loss: 5.2227e-04 - accuracy: 1.0000\n",
      "Epoch 17/20\n",
      "600/600 [==============================] - 1s 963us/step - loss: 4.7613e-04 - accuracy: 1.0000\n",
      "Epoch 18/20\n",
      "600/600 [==============================] - 1s 907us/step - loss: 4.6611e-04 - accuracy: 1.0000\n",
      "Epoch 19/20\n",
      "600/600 [==============================] - 1s 904us/step - loss: 4.6045e-04 - accuracy: 1.0000\n",
      "Epoch 20/20\n",
      "600/600 [==============================] - 1s 964us/step - loss: 4.4734e-04 - accuracy: 1.0000\n",
      "20\n",
      "(600, 1, 9)\n",
      "(600, 1, 9)\n",
      "Epoch 1/20\n",
      "600/600 [==============================] - 1s 976us/step - loss: 0.0070 - accuracy: 0.9450\n",
      "Epoch 2/20\n",
      "600/600 [==============================] - 1s 1ms/step - loss: 0.0028 - accuracy: 0.9817\n",
      "Epoch 3/20\n",
      "600/600 [==============================] - 1s 959us/step - loss: 0.0021 - accuracy: 0.9867\n",
      "Epoch 4/20\n",
      "600/600 [==============================] - 1s 964us/step - loss: 0.0018 - accuracy: 0.9917\n",
      "Epoch 5/20\n",
      "600/600 [==============================] - 1s 943us/step - loss: 0.0015 - accuracy: 0.9900\n",
      "Epoch 6/20\n",
      "600/600 [==============================] - 1s 967us/step - loss: 0.0014 - accuracy: 0.9900\n",
      "Epoch 7/20\n",
      "600/600 [==============================] - 1s 945us/step - loss: 0.0012 - accuracy: 0.9900\n",
      "Epoch 8/20\n",
      "600/600 [==============================] - 1s 1ms/step - loss: 0.0011 - accuracy: 0.9933\n",
      "Epoch 9/20\n",
      "600/600 [==============================] - 1s 1ms/step - loss: 0.0011 - accuracy: 0.9900\n",
      "Epoch 10/20\n",
      "600/600 [==============================] - 1s 1ms/step - loss: 9.4564e-04 - accuracy: 0.9917\n",
      "Epoch 11/20\n",
      "600/600 [==============================] - 1s 1ms/step - loss: 8.8213e-04 - accuracy: 0.9917\n",
      "Epoch 12/20\n",
      "600/600 [==============================] - 1s 1ms/step - loss: 8.7425e-04 - accuracy: 0.9850\n",
      "Epoch 13/20\n",
      "600/600 [==============================] - 1s 1ms/step - loss: 8.1598e-04 - accuracy: 0.9900\n",
      "Epoch 14/20\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 7.3214e-04 - accuracy: 0.9950\n",
      "Epoch 15/20\n",
      "600/600 [==============================] - 1s 1ms/step - loss: 7.1727e-04 - accuracy: 0.9950\n",
      "Epoch 16/20\n",
      "600/600 [==============================] - 1s 1ms/step - loss: 7.0664e-04 - accuracy: 0.9967\n",
      "Epoch 17/20\n",
      "600/600 [==============================] - 1s 1ms/step - loss: 6.5331e-04 - accuracy: 0.9950\n",
      "Epoch 18/20\n",
      "600/600 [==============================] - 1s 966us/step - loss: 6.3257e-04 - accuracy: 0.9933\n",
      "Epoch 19/20\n",
      "600/600 [==============================] - 1s 1ms/step - loss: 6.2913e-04 - accuracy: 0.9933\n",
      "Epoch 20/20\n",
      "600/600 [==============================] - 1s 997us/step - loss: 6.0465e-04 - accuracy: 0.9967\n",
      "21\n",
      "(600, 1, 9)\n",
      "(600, 1, 9)\n",
      "Epoch 1/20\n",
      "600/600 [==============================] - 1s 1ms/step - loss: 0.0091 - accuracy: 0.9133\n",
      "Epoch 2/20\n",
      "600/600 [==============================] - 1s 980us/step - loss: 0.0030 - accuracy: 0.9783\n",
      "Epoch 3/20\n",
      "600/600 [==============================] - 1s 991us/step - loss: 0.0022 - accuracy: 0.9783\n",
      "Epoch 4/20\n",
      "600/600 [==============================] - 1s 1ms/step - loss: 0.0018 - accuracy: 0.9867\n",
      "Epoch 5/20\n",
      "600/600 [==============================] - 1s 1ms/step - loss: 0.0015 - accuracy: 0.9817\n",
      "Epoch 6/20\n",
      "600/600 [==============================] - 1s 1ms/step - loss: 0.0012 - accuracy: 0.9867\n",
      "Epoch 7/20\n",
      "600/600 [==============================] - 1s 955us/step - loss: 0.0011 - accuracy: 0.9867\n",
      "Epoch 8/20\n",
      "600/600 [==============================] - 1s 979us/step - loss: 9.8655e-04 - accuracy: 0.9850\n",
      "Epoch 9/20\n",
      "600/600 [==============================] - 1s 1ms/step - loss: 9.3363e-04 - accuracy: 0.9883\n",
      "Epoch 10/20\n",
      "600/600 [==============================] - 1s 960us/step - loss: 8.2521e-04 - accuracy: 0.9850\n",
      "Epoch 11/20\n",
      "600/600 [==============================] - 1s 979us/step - loss: 7.9901e-04 - accuracy: 0.9850\n",
      "Epoch 12/20\n",
      "600/600 [==============================] - 1s 932us/step - loss: 7.3578e-04 - accuracy: 0.9833\n",
      "Epoch 13/20\n",
      "600/600 [==============================] - 1s 968us/step - loss: 6.8237e-04 - accuracy: 0.9850\n",
      "Epoch 14/20\n",
      "600/600 [==============================] - 1s 1ms/step - loss: 6.5495e-04 - accuracy: 0.9867\n",
      "Epoch 15/20\n",
      "600/600 [==============================] - 1s 1ms/step - loss: 6.1497e-04 - accuracy: 0.9917\n",
      "Epoch 16/20\n",
      "600/600 [==============================] - 1s 1ms/step - loss: 5.6624e-04 - accuracy: 0.9933\n",
      "Epoch 17/20\n",
      "600/600 [==============================] - 1s 1ms/step - loss: 5.6605e-04 - accuracy: 0.9883\n",
      "Epoch 18/20\n",
      "600/600 [==============================] - 1s 1ms/step - loss: 5.5133e-04 - accuracy: 0.9900\n",
      "Epoch 19/20\n",
      "600/600 [==============================] - 1s 1ms/step - loss: 5.1333e-04 - accuracy: 0.9917\n",
      "Epoch 20/20\n",
      "600/600 [==============================] - 1s 1ms/step - loss: 4.7903e-04 - accuracy: 0.9917\n",
      "22\n",
      "(600, 1, 9)\n",
      "(600, 1, 9)\n",
      "Epoch 1/20\n",
      "600/600 [==============================] - 1s 907us/step - loss: 0.0155 - accuracy: 0.8083\n",
      "Epoch 2/20\n",
      "600/600 [==============================] - 1s 954us/step - loss: 0.0054 - accuracy: 0.8250\n",
      "Epoch 3/20\n",
      "600/600 [==============================] - 1s 914us/step - loss: 0.0040 - accuracy: 0.8183\n",
      "Epoch 4/20\n",
      "600/600 [==============================] - 1s 926us/step - loss: 0.0031 - accuracy: 0.8200\n",
      "Epoch 5/20\n",
      "600/600 [==============================] - 1s 964us/step - loss: 0.0025 - accuracy: 0.8383\n",
      "Epoch 6/20\n",
      "600/600 [==============================] - 1s 1ms/step - loss: 0.0020 - accuracy: 0.8367\n",
      "Epoch 7/20\n",
      "600/600 [==============================] - 1s 955us/step - loss: 0.0017 - accuracy: 0.8400\n",
      "Epoch 8/20\n",
      "600/600 [==============================] - 1s 928us/step - loss: 0.0015 - accuracy: 0.8467\n",
      "Epoch 9/20\n",
      "600/600 [==============================] - 1s 997us/step - loss: 0.0012 - accuracy: 0.8633\n",
      "Epoch 10/20\n",
      "600/600 [==============================] - 1s 908us/step - loss: 0.0011 - accuracy: 0.8667\n",
      "Epoch 11/20\n",
      "600/600 [==============================] - 1s 930us/step - loss: 0.0011 - accuracy: 0.8650\n",
      "Epoch 12/20\n",
      "600/600 [==============================] - 1s 1ms/step - loss: 9.5558e-04 - accuracy: 0.8850\n",
      "Epoch 13/20\n",
      "600/600 [==============================] - 1s 927us/step - loss: 9.3517e-04 - accuracy: 0.8900\n",
      "Epoch 14/20\n",
      "600/600 [==============================] - 1s 957us/step - loss: 8.9346e-04 - accuracy: 0.8800\n",
      "Epoch 15/20\n",
      "600/600 [==============================] - 1s 914us/step - loss: 8.8365e-04 - accuracy: 0.8850\n",
      "Epoch 16/20\n",
      "600/600 [==============================] - 1s 1ms/step - loss: 8.2455e-04 - accuracy: 0.9017\n",
      "Epoch 17/20\n",
      "600/600 [==============================] - 1s 952us/step - loss: 7.2798e-04 - accuracy: 0.9133\n",
      "Epoch 18/20\n",
      "600/600 [==============================] - 1s 916us/step - loss: 7.3943e-04 - accuracy: 0.8950\n",
      "Epoch 19/20\n",
      "600/600 [==============================] - 1s 947us/step - loss: 6.7540e-04 - accuracy: 0.9083\n",
      "Epoch 20/20\n",
      "600/600 [==============================] - 1s 935us/step - loss: 7.2702e-04 - accuracy: 0.8933\n"
     ]
    }
   ],
   "source": [
    "#Fit list of datasets to model\n",
    "\n",
    "FitToList(x, y, model_2, 20, 1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/22 [==============================] - 0s 3ms/step - loss: 0.1530 - accuracy: 0.7135\n",
      "Loss:  0.1529950911137793\n"
     ]
    }
   ],
   "source": [
    "EvaluateModel(x_test, y_test, model_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5400, 9)\n",
      "(5400, 9)\n",
      "predictions shape: (5400, 9)\n",
      "Actual Values\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Waist_X</th>\n",
       "      <th>Waist_Y</th>\n",
       "      <th>Waist_Z</th>\n",
       "      <th>R_Foot_X</th>\n",
       "      <th>R_Foot_Y</th>\n",
       "      <th>R_Foot_Z</th>\n",
       "      <th>L_Foot_X</th>\n",
       "      <th>L_Foot_Y</th>\n",
       "      <th>L_Foot_Z</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4000</th>\n",
       "      <td>0.568578</td>\n",
       "      <td>0.948957</td>\n",
       "      <td>0.022448</td>\n",
       "      <td>0.443972</td>\n",
       "      <td>0.099626</td>\n",
       "      <td>0.040721</td>\n",
       "      <td>0.847469</td>\n",
       "      <td>0.105724</td>\n",
       "      <td>-0.260403</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4001</th>\n",
       "      <td>0.559899</td>\n",
       "      <td>0.950990</td>\n",
       "      <td>0.032180</td>\n",
       "      <td>0.444332</td>\n",
       "      <td>0.099212</td>\n",
       "      <td>0.041662</td>\n",
       "      <td>0.843127</td>\n",
       "      <td>0.109440</td>\n",
       "      <td>-0.254307</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4002</th>\n",
       "      <td>0.552315</td>\n",
       "      <td>0.952658</td>\n",
       "      <td>0.041153</td>\n",
       "      <td>0.445992</td>\n",
       "      <td>0.098248</td>\n",
       "      <td>0.041867</td>\n",
       "      <td>0.839128</td>\n",
       "      <td>0.115232</td>\n",
       "      <td>-0.247011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4003</th>\n",
       "      <td>0.545255</td>\n",
       "      <td>0.954177</td>\n",
       "      <td>0.050479</td>\n",
       "      <td>0.446416</td>\n",
       "      <td>0.097923</td>\n",
       "      <td>0.042506</td>\n",
       "      <td>0.833094</td>\n",
       "      <td>0.123043</td>\n",
       "      <td>-0.237844</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4004</th>\n",
       "      <td>0.538411</td>\n",
       "      <td>0.956142</td>\n",
       "      <td>0.058618</td>\n",
       "      <td>0.445989</td>\n",
       "      <td>0.098010</td>\n",
       "      <td>0.042564</td>\n",
       "      <td>0.824971</td>\n",
       "      <td>0.134583</td>\n",
       "      <td>-0.227277</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4005</th>\n",
       "      <td>0.532336</td>\n",
       "      <td>0.957229</td>\n",
       "      <td>0.066987</td>\n",
       "      <td>0.446254</td>\n",
       "      <td>0.098038</td>\n",
       "      <td>0.043005</td>\n",
       "      <td>0.815174</td>\n",
       "      <td>0.146172</td>\n",
       "      <td>-0.213108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4006</th>\n",
       "      <td>0.526953</td>\n",
       "      <td>0.958167</td>\n",
       "      <td>0.075301</td>\n",
       "      <td>0.446580</td>\n",
       "      <td>0.097753</td>\n",
       "      <td>0.043268</td>\n",
       "      <td>0.802805</td>\n",
       "      <td>0.157194</td>\n",
       "      <td>-0.196686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4007</th>\n",
       "      <td>0.521210</td>\n",
       "      <td>0.958752</td>\n",
       "      <td>0.083070</td>\n",
       "      <td>0.446938</td>\n",
       "      <td>0.097715</td>\n",
       "      <td>0.043523</td>\n",
       "      <td>0.788901</td>\n",
       "      <td>0.167540</td>\n",
       "      <td>-0.179047</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4008</th>\n",
       "      <td>0.516434</td>\n",
       "      <td>0.959480</td>\n",
       "      <td>0.089992</td>\n",
       "      <td>0.447252</td>\n",
       "      <td>0.097746</td>\n",
       "      <td>0.043566</td>\n",
       "      <td>0.773595</td>\n",
       "      <td>0.175675</td>\n",
       "      <td>-0.159534</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4009</th>\n",
       "      <td>0.511045</td>\n",
       "      <td>0.960276</td>\n",
       "      <td>0.097480</td>\n",
       "      <td>0.447538</td>\n",
       "      <td>0.097748</td>\n",
       "      <td>0.043522</td>\n",
       "      <td>0.756670</td>\n",
       "      <td>0.183696</td>\n",
       "      <td>-0.137797</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4010</th>\n",
       "      <td>0.505994</td>\n",
       "      <td>0.961631</td>\n",
       "      <td>0.105464</td>\n",
       "      <td>0.447084</td>\n",
       "      <td>0.097761</td>\n",
       "      <td>0.043952</td>\n",
       "      <td>0.736907</td>\n",
       "      <td>0.189907</td>\n",
       "      <td>-0.115033</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4011</th>\n",
       "      <td>0.501060</td>\n",
       "      <td>0.963514</td>\n",
       "      <td>0.113224</td>\n",
       "      <td>0.447289</td>\n",
       "      <td>0.098065</td>\n",
       "      <td>0.043852</td>\n",
       "      <td>0.718735</td>\n",
       "      <td>0.194573</td>\n",
       "      <td>-0.089927</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4012</th>\n",
       "      <td>0.497181</td>\n",
       "      <td>0.965736</td>\n",
       "      <td>0.121204</td>\n",
       "      <td>0.447935</td>\n",
       "      <td>0.097684</td>\n",
       "      <td>0.043980</td>\n",
       "      <td>0.700745</td>\n",
       "      <td>0.195623</td>\n",
       "      <td>-0.063531</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4013</th>\n",
       "      <td>0.492730</td>\n",
       "      <td>0.967985</td>\n",
       "      <td>0.129048</td>\n",
       "      <td>0.448210</td>\n",
       "      <td>0.097652</td>\n",
       "      <td>0.043929</td>\n",
       "      <td>0.683046</td>\n",
       "      <td>0.195105</td>\n",
       "      <td>-0.035010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4014</th>\n",
       "      <td>0.487984</td>\n",
       "      <td>0.970552</td>\n",
       "      <td>0.137147</td>\n",
       "      <td>0.447211</td>\n",
       "      <td>0.098017</td>\n",
       "      <td>0.044219</td>\n",
       "      <td>0.663190</td>\n",
       "      <td>0.193462</td>\n",
       "      <td>-0.004627</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4015</th>\n",
       "      <td>0.483878</td>\n",
       "      <td>0.972058</td>\n",
       "      <td>0.144661</td>\n",
       "      <td>0.447179</td>\n",
       "      <td>0.098128</td>\n",
       "      <td>0.043876</td>\n",
       "      <td>0.645762</td>\n",
       "      <td>0.189569</td>\n",
       "      <td>0.026836</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4016</th>\n",
       "      <td>0.480100</td>\n",
       "      <td>0.972570</td>\n",
       "      <td>0.152082</td>\n",
       "      <td>0.448109</td>\n",
       "      <td>0.097806</td>\n",
       "      <td>0.043312</td>\n",
       "      <td>0.629897</td>\n",
       "      <td>0.184080</td>\n",
       "      <td>0.060084</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4017</th>\n",
       "      <td>0.476089</td>\n",
       "      <td>0.972603</td>\n",
       "      <td>0.159639</td>\n",
       "      <td>0.448173</td>\n",
       "      <td>0.097596</td>\n",
       "      <td>0.043511</td>\n",
       "      <td>0.614180</td>\n",
       "      <td>0.177997</td>\n",
       "      <td>0.094819</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4018</th>\n",
       "      <td>0.470672</td>\n",
       "      <td>0.972154</td>\n",
       "      <td>0.167044</td>\n",
       "      <td>0.448213</td>\n",
       "      <td>0.097463</td>\n",
       "      <td>0.043347</td>\n",
       "      <td>0.599382</td>\n",
       "      <td>0.171280</td>\n",
       "      <td>0.131079</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4019</th>\n",
       "      <td>0.466225</td>\n",
       "      <td>0.970958</td>\n",
       "      <td>0.174480</td>\n",
       "      <td>0.448066</td>\n",
       "      <td>0.097075</td>\n",
       "      <td>0.043585</td>\n",
       "      <td>0.585077</td>\n",
       "      <td>0.162224</td>\n",
       "      <td>0.169669</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4020</th>\n",
       "      <td>0.462624</td>\n",
       "      <td>0.969290</td>\n",
       "      <td>0.181816</td>\n",
       "      <td>0.448085</td>\n",
       "      <td>0.096945</td>\n",
       "      <td>0.043493</td>\n",
       "      <td>0.571497</td>\n",
       "      <td>0.155512</td>\n",
       "      <td>0.208370</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4021</th>\n",
       "      <td>0.458146</td>\n",
       "      <td>0.967822</td>\n",
       "      <td>0.189635</td>\n",
       "      <td>0.448089</td>\n",
       "      <td>0.096759</td>\n",
       "      <td>0.043534</td>\n",
       "      <td>0.558076</td>\n",
       "      <td>0.149055</td>\n",
       "      <td>0.248250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4022</th>\n",
       "      <td>0.452870</td>\n",
       "      <td>0.966987</td>\n",
       "      <td>0.197797</td>\n",
       "      <td>0.448094</td>\n",
       "      <td>0.096641</td>\n",
       "      <td>0.043480</td>\n",
       "      <td>0.544902</td>\n",
       "      <td>0.144602</td>\n",
       "      <td>0.287997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4023</th>\n",
       "      <td>0.447986</td>\n",
       "      <td>0.965518</td>\n",
       "      <td>0.205965</td>\n",
       "      <td>0.447804</td>\n",
       "      <td>0.096432</td>\n",
       "      <td>0.043497</td>\n",
       "      <td>0.530956</td>\n",
       "      <td>0.140891</td>\n",
       "      <td>0.326812</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4024</th>\n",
       "      <td>0.444108</td>\n",
       "      <td>0.963987</td>\n",
       "      <td>0.214884</td>\n",
       "      <td>0.447800</td>\n",
       "      <td>0.096272</td>\n",
       "      <td>0.043516</td>\n",
       "      <td>0.517288</td>\n",
       "      <td>0.139413</td>\n",
       "      <td>0.364315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4025</th>\n",
       "      <td>0.439237</td>\n",
       "      <td>0.961938</td>\n",
       "      <td>0.223742</td>\n",
       "      <td>0.447584</td>\n",
       "      <td>0.096043</td>\n",
       "      <td>0.043398</td>\n",
       "      <td>0.503301</td>\n",
       "      <td>0.139888</td>\n",
       "      <td>0.399043</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4026</th>\n",
       "      <td>0.434039</td>\n",
       "      <td>0.960554</td>\n",
       "      <td>0.233430</td>\n",
       "      <td>0.447377</td>\n",
       "      <td>0.096088</td>\n",
       "      <td>0.043383</td>\n",
       "      <td>0.490578</td>\n",
       "      <td>0.142417</td>\n",
       "      <td>0.430378</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4027</th>\n",
       "      <td>0.429162</td>\n",
       "      <td>0.958419</td>\n",
       "      <td>0.243273</td>\n",
       "      <td>0.447356</td>\n",
       "      <td>0.095743</td>\n",
       "      <td>0.043270</td>\n",
       "      <td>0.477822</td>\n",
       "      <td>0.145582</td>\n",
       "      <td>0.457679</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4028</th>\n",
       "      <td>0.424764</td>\n",
       "      <td>0.956456</td>\n",
       "      <td>0.253615</td>\n",
       "      <td>0.447343</td>\n",
       "      <td>0.095533</td>\n",
       "      <td>0.043288</td>\n",
       "      <td>0.467107</td>\n",
       "      <td>0.148031</td>\n",
       "      <td>0.480336</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4029</th>\n",
       "      <td>0.420210</td>\n",
       "      <td>0.954821</td>\n",
       "      <td>0.264729</td>\n",
       "      <td>0.447331</td>\n",
       "      <td>0.095146</td>\n",
       "      <td>0.043367</td>\n",
       "      <td>0.457828</td>\n",
       "      <td>0.149444</td>\n",
       "      <td>0.498081</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4030</th>\n",
       "      <td>0.415205</td>\n",
       "      <td>0.953217</td>\n",
       "      <td>0.276003</td>\n",
       "      <td>0.447321</td>\n",
       "      <td>0.094924</td>\n",
       "      <td>0.043467</td>\n",
       "      <td>0.451516</td>\n",
       "      <td>0.147820</td>\n",
       "      <td>0.509887</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4031</th>\n",
       "      <td>0.410454</td>\n",
       "      <td>0.951309</td>\n",
       "      <td>0.288038</td>\n",
       "      <td>0.447316</td>\n",
       "      <td>0.094673</td>\n",
       "      <td>0.043637</td>\n",
       "      <td>0.447195</td>\n",
       "      <td>0.143816</td>\n",
       "      <td>0.517581</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4032</th>\n",
       "      <td>0.406161</td>\n",
       "      <td>0.949621</td>\n",
       "      <td>0.299935</td>\n",
       "      <td>0.447414</td>\n",
       "      <td>0.094404</td>\n",
       "      <td>0.043936</td>\n",
       "      <td>0.447570</td>\n",
       "      <td>0.135851</td>\n",
       "      <td>0.520894</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4033</th>\n",
       "      <td>0.401562</td>\n",
       "      <td>0.947863</td>\n",
       "      <td>0.312901</td>\n",
       "      <td>0.447525</td>\n",
       "      <td>0.094004</td>\n",
       "      <td>0.044406</td>\n",
       "      <td>0.446177</td>\n",
       "      <td>0.130944</td>\n",
       "      <td>0.527465</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4034</th>\n",
       "      <td>0.396791</td>\n",
       "      <td>0.946150</td>\n",
       "      <td>0.326215</td>\n",
       "      <td>0.447871</td>\n",
       "      <td>0.093697</td>\n",
       "      <td>0.045342</td>\n",
       "      <td>0.442834</td>\n",
       "      <td>0.126118</td>\n",
       "      <td>0.533446</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4035</th>\n",
       "      <td>0.392619</td>\n",
       "      <td>0.944012</td>\n",
       "      <td>0.338988</td>\n",
       "      <td>0.448445</td>\n",
       "      <td>0.093374</td>\n",
       "      <td>0.047004</td>\n",
       "      <td>0.440127</td>\n",
       "      <td>0.123888</td>\n",
       "      <td>0.530821</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4036</th>\n",
       "      <td>0.388589</td>\n",
       "      <td>0.942443</td>\n",
       "      <td>0.350914</td>\n",
       "      <td>0.447880</td>\n",
       "      <td>0.094232</td>\n",
       "      <td>0.049083</td>\n",
       "      <td>0.439229</td>\n",
       "      <td>0.123125</td>\n",
       "      <td>0.533201</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4037</th>\n",
       "      <td>0.384596</td>\n",
       "      <td>0.940871</td>\n",
       "      <td>0.362115</td>\n",
       "      <td>0.447604</td>\n",
       "      <td>0.094297</td>\n",
       "      <td>0.051640</td>\n",
       "      <td>0.437672</td>\n",
       "      <td>0.122573</td>\n",
       "      <td>0.531510</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4038</th>\n",
       "      <td>0.380390</td>\n",
       "      <td>0.942303</td>\n",
       "      <td>0.374583</td>\n",
       "      <td>0.444669</td>\n",
       "      <td>0.097126</td>\n",
       "      <td>0.053802</td>\n",
       "      <td>0.437692</td>\n",
       "      <td>0.121848</td>\n",
       "      <td>0.533038</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4039</th>\n",
       "      <td>0.376812</td>\n",
       "      <td>0.943929</td>\n",
       "      <td>0.385915</td>\n",
       "      <td>0.443235</td>\n",
       "      <td>0.098140</td>\n",
       "      <td>0.057847</td>\n",
       "      <td>0.437616</td>\n",
       "      <td>0.121301</td>\n",
       "      <td>0.532192</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4040</th>\n",
       "      <td>0.376345</td>\n",
       "      <td>0.945864</td>\n",
       "      <td>0.397520</td>\n",
       "      <td>0.442318</td>\n",
       "      <td>0.098922</td>\n",
       "      <td>0.062338</td>\n",
       "      <td>0.437441</td>\n",
       "      <td>0.121266</td>\n",
       "      <td>0.532595</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4041</th>\n",
       "      <td>0.373506</td>\n",
       "      <td>0.948583</td>\n",
       "      <td>0.408046</td>\n",
       "      <td>0.440233</td>\n",
       "      <td>0.100678</td>\n",
       "      <td>0.068281</td>\n",
       "      <td>0.438060</td>\n",
       "      <td>0.121193</td>\n",
       "      <td>0.531668</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4042</th>\n",
       "      <td>0.369117</td>\n",
       "      <td>0.952351</td>\n",
       "      <td>0.416625</td>\n",
       "      <td>0.438190</td>\n",
       "      <td>0.103580</td>\n",
       "      <td>0.074945</td>\n",
       "      <td>0.438051</td>\n",
       "      <td>0.121261</td>\n",
       "      <td>0.531832</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4043</th>\n",
       "      <td>0.365712</td>\n",
       "      <td>0.954526</td>\n",
       "      <td>0.425479</td>\n",
       "      <td>0.435759</td>\n",
       "      <td>0.107673</td>\n",
       "      <td>0.082810</td>\n",
       "      <td>0.438731</td>\n",
       "      <td>0.121141</td>\n",
       "      <td>0.531561</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4044</th>\n",
       "      <td>0.364353</td>\n",
       "      <td>0.954738</td>\n",
       "      <td>0.434494</td>\n",
       "      <td>0.433404</td>\n",
       "      <td>0.112919</td>\n",
       "      <td>0.091088</td>\n",
       "      <td>0.439025</td>\n",
       "      <td>0.121381</td>\n",
       "      <td>0.531536</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4045</th>\n",
       "      <td>0.362149</td>\n",
       "      <td>0.955975</td>\n",
       "      <td>0.442964</td>\n",
       "      <td>0.429883</td>\n",
       "      <td>0.119869</td>\n",
       "      <td>0.101290</td>\n",
       "      <td>0.439979</td>\n",
       "      <td>0.121336</td>\n",
       "      <td>0.531775</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4046</th>\n",
       "      <td>0.357867</td>\n",
       "      <td>0.957519</td>\n",
       "      <td>0.449158</td>\n",
       "      <td>0.425781</td>\n",
       "      <td>0.129184</td>\n",
       "      <td>0.112561</td>\n",
       "      <td>0.440549</td>\n",
       "      <td>0.121340</td>\n",
       "      <td>0.532071</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4047</th>\n",
       "      <td>0.354851</td>\n",
       "      <td>0.957959</td>\n",
       "      <td>0.457115</td>\n",
       "      <td>0.417646</td>\n",
       "      <td>0.139614</td>\n",
       "      <td>0.127915</td>\n",
       "      <td>0.441123</td>\n",
       "      <td>0.120926</td>\n",
       "      <td>0.532936</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4048</th>\n",
       "      <td>0.352622</td>\n",
       "      <td>0.957660</td>\n",
       "      <td>0.465403</td>\n",
       "      <td>0.409010</td>\n",
       "      <td>0.147944</td>\n",
       "      <td>0.145439</td>\n",
       "      <td>0.441125</td>\n",
       "      <td>0.120376</td>\n",
       "      <td>0.533262</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4049</th>\n",
       "      <td>0.349562</td>\n",
       "      <td>0.958056</td>\n",
       "      <td>0.472996</td>\n",
       "      <td>0.399853</td>\n",
       "      <td>0.153085</td>\n",
       "      <td>0.163181</td>\n",
       "      <td>0.441568</td>\n",
       "      <td>0.119662</td>\n",
       "      <td>0.533700</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Waist_X   Waist_Y   Waist_Z  R_Foot_X  R_Foot_Y  R_Foot_Z  L_Foot_X  \\\n",
       "4000  0.568578  0.948957  0.022448  0.443972  0.099626  0.040721  0.847469   \n",
       "4001  0.559899  0.950990  0.032180  0.444332  0.099212  0.041662  0.843127   \n",
       "4002  0.552315  0.952658  0.041153  0.445992  0.098248  0.041867  0.839128   \n",
       "4003  0.545255  0.954177  0.050479  0.446416  0.097923  0.042506  0.833094   \n",
       "4004  0.538411  0.956142  0.058618  0.445989  0.098010  0.042564  0.824971   \n",
       "4005  0.532336  0.957229  0.066987  0.446254  0.098038  0.043005  0.815174   \n",
       "4006  0.526953  0.958167  0.075301  0.446580  0.097753  0.043268  0.802805   \n",
       "4007  0.521210  0.958752  0.083070  0.446938  0.097715  0.043523  0.788901   \n",
       "4008  0.516434  0.959480  0.089992  0.447252  0.097746  0.043566  0.773595   \n",
       "4009  0.511045  0.960276  0.097480  0.447538  0.097748  0.043522  0.756670   \n",
       "4010  0.505994  0.961631  0.105464  0.447084  0.097761  0.043952  0.736907   \n",
       "4011  0.501060  0.963514  0.113224  0.447289  0.098065  0.043852  0.718735   \n",
       "4012  0.497181  0.965736  0.121204  0.447935  0.097684  0.043980  0.700745   \n",
       "4013  0.492730  0.967985  0.129048  0.448210  0.097652  0.043929  0.683046   \n",
       "4014  0.487984  0.970552  0.137147  0.447211  0.098017  0.044219  0.663190   \n",
       "4015  0.483878  0.972058  0.144661  0.447179  0.098128  0.043876  0.645762   \n",
       "4016  0.480100  0.972570  0.152082  0.448109  0.097806  0.043312  0.629897   \n",
       "4017  0.476089  0.972603  0.159639  0.448173  0.097596  0.043511  0.614180   \n",
       "4018  0.470672  0.972154  0.167044  0.448213  0.097463  0.043347  0.599382   \n",
       "4019  0.466225  0.970958  0.174480  0.448066  0.097075  0.043585  0.585077   \n",
       "4020  0.462624  0.969290  0.181816  0.448085  0.096945  0.043493  0.571497   \n",
       "4021  0.458146  0.967822  0.189635  0.448089  0.096759  0.043534  0.558076   \n",
       "4022  0.452870  0.966987  0.197797  0.448094  0.096641  0.043480  0.544902   \n",
       "4023  0.447986  0.965518  0.205965  0.447804  0.096432  0.043497  0.530956   \n",
       "4024  0.444108  0.963987  0.214884  0.447800  0.096272  0.043516  0.517288   \n",
       "4025  0.439237  0.961938  0.223742  0.447584  0.096043  0.043398  0.503301   \n",
       "4026  0.434039  0.960554  0.233430  0.447377  0.096088  0.043383  0.490578   \n",
       "4027  0.429162  0.958419  0.243273  0.447356  0.095743  0.043270  0.477822   \n",
       "4028  0.424764  0.956456  0.253615  0.447343  0.095533  0.043288  0.467107   \n",
       "4029  0.420210  0.954821  0.264729  0.447331  0.095146  0.043367  0.457828   \n",
       "4030  0.415205  0.953217  0.276003  0.447321  0.094924  0.043467  0.451516   \n",
       "4031  0.410454  0.951309  0.288038  0.447316  0.094673  0.043637  0.447195   \n",
       "4032  0.406161  0.949621  0.299935  0.447414  0.094404  0.043936  0.447570   \n",
       "4033  0.401562  0.947863  0.312901  0.447525  0.094004  0.044406  0.446177   \n",
       "4034  0.396791  0.946150  0.326215  0.447871  0.093697  0.045342  0.442834   \n",
       "4035  0.392619  0.944012  0.338988  0.448445  0.093374  0.047004  0.440127   \n",
       "4036  0.388589  0.942443  0.350914  0.447880  0.094232  0.049083  0.439229   \n",
       "4037  0.384596  0.940871  0.362115  0.447604  0.094297  0.051640  0.437672   \n",
       "4038  0.380390  0.942303  0.374583  0.444669  0.097126  0.053802  0.437692   \n",
       "4039  0.376812  0.943929  0.385915  0.443235  0.098140  0.057847  0.437616   \n",
       "4040  0.376345  0.945864  0.397520  0.442318  0.098922  0.062338  0.437441   \n",
       "4041  0.373506  0.948583  0.408046  0.440233  0.100678  0.068281  0.438060   \n",
       "4042  0.369117  0.952351  0.416625  0.438190  0.103580  0.074945  0.438051   \n",
       "4043  0.365712  0.954526  0.425479  0.435759  0.107673  0.082810  0.438731   \n",
       "4044  0.364353  0.954738  0.434494  0.433404  0.112919  0.091088  0.439025   \n",
       "4045  0.362149  0.955975  0.442964  0.429883  0.119869  0.101290  0.439979   \n",
       "4046  0.357867  0.957519  0.449158  0.425781  0.129184  0.112561  0.440549   \n",
       "4047  0.354851  0.957959  0.457115  0.417646  0.139614  0.127915  0.441123   \n",
       "4048  0.352622  0.957660  0.465403  0.409010  0.147944  0.145439  0.441125   \n",
       "4049  0.349562  0.958056  0.472996  0.399853  0.153085  0.163181  0.441568   \n",
       "\n",
       "      L_Foot_Y  L_Foot_Z  \n",
       "4000  0.105724 -0.260403  \n",
       "4001  0.109440 -0.254307  \n",
       "4002  0.115232 -0.247011  \n",
       "4003  0.123043 -0.237844  \n",
       "4004  0.134583 -0.227277  \n",
       "4005  0.146172 -0.213108  \n",
       "4006  0.157194 -0.196686  \n",
       "4007  0.167540 -0.179047  \n",
       "4008  0.175675 -0.159534  \n",
       "4009  0.183696 -0.137797  \n",
       "4010  0.189907 -0.115033  \n",
       "4011  0.194573 -0.089927  \n",
       "4012  0.195623 -0.063531  \n",
       "4013  0.195105 -0.035010  \n",
       "4014  0.193462 -0.004627  \n",
       "4015  0.189569  0.026836  \n",
       "4016  0.184080  0.060084  \n",
       "4017  0.177997  0.094819  \n",
       "4018  0.171280  0.131079  \n",
       "4019  0.162224  0.169669  \n",
       "4020  0.155512  0.208370  \n",
       "4021  0.149055  0.248250  \n",
       "4022  0.144602  0.287997  \n",
       "4023  0.140891  0.326812  \n",
       "4024  0.139413  0.364315  \n",
       "4025  0.139888  0.399043  \n",
       "4026  0.142417  0.430378  \n",
       "4027  0.145582  0.457679  \n",
       "4028  0.148031  0.480336  \n",
       "4029  0.149444  0.498081  \n",
       "4030  0.147820  0.509887  \n",
       "4031  0.143816  0.517581  \n",
       "4032  0.135851  0.520894  \n",
       "4033  0.130944  0.527465  \n",
       "4034  0.126118  0.533446  \n",
       "4035  0.123888  0.530821  \n",
       "4036  0.123125  0.533201  \n",
       "4037  0.122573  0.531510  \n",
       "4038  0.121848  0.533038  \n",
       "4039  0.121301  0.532192  \n",
       "4040  0.121266  0.532595  \n",
       "4041  0.121193  0.531668  \n",
       "4042  0.121261  0.531832  \n",
       "4043  0.121141  0.531561  \n",
       "4044  0.121381  0.531536  \n",
       "4045  0.121336  0.531775  \n",
       "4046  0.121340  0.532071  \n",
       "4047  0.120926  0.532936  \n",
       "4048  0.120376  0.533262  \n",
       "4049  0.119662  0.533700  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicited Values\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Waist_X</th>\n",
       "      <th>Waist_Y</th>\n",
       "      <th>Waist_Z</th>\n",
       "      <th>R_Foot_X</th>\n",
       "      <th>R_Foot_Y</th>\n",
       "      <th>R_Foot_Z</th>\n",
       "      <th>L_Foot_X</th>\n",
       "      <th>L_Foot_Y</th>\n",
       "      <th>L_Foot_Z</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4000</th>\n",
       "      <td>0.976676</td>\n",
       "      <td>1.224183</td>\n",
       "      <td>0.289238</td>\n",
       "      <td>-0.496335</td>\n",
       "      <td>0.053867</td>\n",
       "      <td>-0.043498</td>\n",
       "      <td>0.865819</td>\n",
       "      <td>0.463140</td>\n",
       "      <td>-0.526091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4001</th>\n",
       "      <td>0.950934</td>\n",
       "      <td>1.210933</td>\n",
       "      <td>0.300699</td>\n",
       "      <td>-0.506135</td>\n",
       "      <td>0.052749</td>\n",
       "      <td>-0.019599</td>\n",
       "      <td>0.873998</td>\n",
       "      <td>0.448951</td>\n",
       "      <td>-0.522736</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4002</th>\n",
       "      <td>0.925361</td>\n",
       "      <td>1.197943</td>\n",
       "      <td>0.312225</td>\n",
       "      <td>-0.514672</td>\n",
       "      <td>0.052463</td>\n",
       "      <td>0.002984</td>\n",
       "      <td>0.880861</td>\n",
       "      <td>0.434433</td>\n",
       "      <td>-0.517868</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4003</th>\n",
       "      <td>0.898110</td>\n",
       "      <td>1.184754</td>\n",
       "      <td>0.323552</td>\n",
       "      <td>-0.518997</td>\n",
       "      <td>0.052893</td>\n",
       "      <td>0.025483</td>\n",
       "      <td>0.885049</td>\n",
       "      <td>0.419374</td>\n",
       "      <td>-0.509630</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4004</th>\n",
       "      <td>0.870596</td>\n",
       "      <td>1.172180</td>\n",
       "      <td>0.333662</td>\n",
       "      <td>-0.519373</td>\n",
       "      <td>0.054445</td>\n",
       "      <td>0.046371</td>\n",
       "      <td>0.886216</td>\n",
       "      <td>0.404494</td>\n",
       "      <td>-0.499613</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4005</th>\n",
       "      <td>0.842546</td>\n",
       "      <td>1.159517</td>\n",
       "      <td>0.344233</td>\n",
       "      <td>-0.518455</td>\n",
       "      <td>0.056463</td>\n",
       "      <td>0.066589</td>\n",
       "      <td>0.885809</td>\n",
       "      <td>0.388882</td>\n",
       "      <td>-0.487463</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4006</th>\n",
       "      <td>0.815824</td>\n",
       "      <td>1.147585</td>\n",
       "      <td>0.354245</td>\n",
       "      <td>-0.516732</td>\n",
       "      <td>0.059069</td>\n",
       "      <td>0.085118</td>\n",
       "      <td>0.883878</td>\n",
       "      <td>0.373643</td>\n",
       "      <td>-0.475693</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4007</th>\n",
       "      <td>0.788414</td>\n",
       "      <td>1.135978</td>\n",
       "      <td>0.363500</td>\n",
       "      <td>-0.511211</td>\n",
       "      <td>0.062655</td>\n",
       "      <td>0.103100</td>\n",
       "      <td>0.878378</td>\n",
       "      <td>0.358224</td>\n",
       "      <td>-0.462178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4008</th>\n",
       "      <td>0.762003</td>\n",
       "      <td>1.125182</td>\n",
       "      <td>0.371663</td>\n",
       "      <td>-0.503980</td>\n",
       "      <td>0.066878</td>\n",
       "      <td>0.119850</td>\n",
       "      <td>0.870706</td>\n",
       "      <td>0.343416</td>\n",
       "      <td>-0.449404</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4009</th>\n",
       "      <td>0.735495</td>\n",
       "      <td>1.114798</td>\n",
       "      <td>0.379169</td>\n",
       "      <td>-0.493962</td>\n",
       "      <td>0.071835</td>\n",
       "      <td>0.136177</td>\n",
       "      <td>0.860058</td>\n",
       "      <td>0.328665</td>\n",
       "      <td>-0.436027</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4010</th>\n",
       "      <td>0.709318</td>\n",
       "      <td>1.104973</td>\n",
       "      <td>0.385494</td>\n",
       "      <td>-0.480940</td>\n",
       "      <td>0.077525</td>\n",
       "      <td>0.152238</td>\n",
       "      <td>0.846115</td>\n",
       "      <td>0.314367</td>\n",
       "      <td>-0.423016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4011</th>\n",
       "      <td>0.684416</td>\n",
       "      <td>1.095954</td>\n",
       "      <td>0.390867</td>\n",
       "      <td>-0.466669</td>\n",
       "      <td>0.083552</td>\n",
       "      <td>0.167791</td>\n",
       "      <td>0.830458</td>\n",
       "      <td>0.300869</td>\n",
       "      <td>-0.411670</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4012</th>\n",
       "      <td>0.660509</td>\n",
       "      <td>1.087606</td>\n",
       "      <td>0.395156</td>\n",
       "      <td>-0.452427</td>\n",
       "      <td>0.089923</td>\n",
       "      <td>0.183246</td>\n",
       "      <td>0.813351</td>\n",
       "      <td>0.287924</td>\n",
       "      <td>-0.403495</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4013</th>\n",
       "      <td>0.636370</td>\n",
       "      <td>1.079752</td>\n",
       "      <td>0.398385</td>\n",
       "      <td>-0.434636</td>\n",
       "      <td>0.096838</td>\n",
       "      <td>0.199738</td>\n",
       "      <td>0.793527</td>\n",
       "      <td>0.275436</td>\n",
       "      <td>-0.395706</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4014</th>\n",
       "      <td>0.611334</td>\n",
       "      <td>1.072404</td>\n",
       "      <td>0.399886</td>\n",
       "      <td>-0.413480</td>\n",
       "      <td>0.104681</td>\n",
       "      <td>0.217796</td>\n",
       "      <td>0.770366</td>\n",
       "      <td>0.263098</td>\n",
       "      <td>-0.389809</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4015</th>\n",
       "      <td>0.587391</td>\n",
       "      <td>1.065872</td>\n",
       "      <td>0.399864</td>\n",
       "      <td>-0.390492</td>\n",
       "      <td>0.112881</td>\n",
       "      <td>0.237442</td>\n",
       "      <td>0.745170</td>\n",
       "      <td>0.251755</td>\n",
       "      <td>-0.387773</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4016</th>\n",
       "      <td>0.564533</td>\n",
       "      <td>1.060154</td>\n",
       "      <td>0.397988</td>\n",
       "      <td>-0.366591</td>\n",
       "      <td>0.121524</td>\n",
       "      <td>0.259783</td>\n",
       "      <td>0.717565</td>\n",
       "      <td>0.241478</td>\n",
       "      <td>-0.392333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4017</th>\n",
       "      <td>0.541590</td>\n",
       "      <td>1.055078</td>\n",
       "      <td>0.394300</td>\n",
       "      <td>-0.337541</td>\n",
       "      <td>0.130895</td>\n",
       "      <td>0.284781</td>\n",
       "      <td>0.686405</td>\n",
       "      <td>0.232045</td>\n",
       "      <td>-0.398495</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4018</th>\n",
       "      <td>0.518180</td>\n",
       "      <td>1.050709</td>\n",
       "      <td>0.387990</td>\n",
       "      <td>-0.304435</td>\n",
       "      <td>0.141313</td>\n",
       "      <td>0.312195</td>\n",
       "      <td>0.651199</td>\n",
       "      <td>0.223136</td>\n",
       "      <td>-0.408391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4019</th>\n",
       "      <td>0.496597</td>\n",
       "      <td>1.046943</td>\n",
       "      <td>0.380897</td>\n",
       "      <td>-0.269973</td>\n",
       "      <td>0.151629</td>\n",
       "      <td>0.340976</td>\n",
       "      <td>0.615220</td>\n",
       "      <td>0.215403</td>\n",
       "      <td>-0.420662</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4020</th>\n",
       "      <td>0.475542</td>\n",
       "      <td>1.043524</td>\n",
       "      <td>0.372586</td>\n",
       "      <td>-0.232858</td>\n",
       "      <td>0.162407</td>\n",
       "      <td>0.371599</td>\n",
       "      <td>0.576542</td>\n",
       "      <td>0.208191</td>\n",
       "      <td>-0.435238</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4021</th>\n",
       "      <td>0.456593</td>\n",
       "      <td>1.040430</td>\n",
       "      <td>0.364801</td>\n",
       "      <td>-0.196586</td>\n",
       "      <td>0.172615</td>\n",
       "      <td>0.401032</td>\n",
       "      <td>0.539849</td>\n",
       "      <td>0.201747</td>\n",
       "      <td>-0.448758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4022</th>\n",
       "      <td>0.437558</td>\n",
       "      <td>1.037541</td>\n",
       "      <td>0.356855</td>\n",
       "      <td>-0.159136</td>\n",
       "      <td>0.183140</td>\n",
       "      <td>0.430216</td>\n",
       "      <td>0.503015</td>\n",
       "      <td>0.195140</td>\n",
       "      <td>-0.461101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4023</th>\n",
       "      <td>0.421793</td>\n",
       "      <td>1.034603</td>\n",
       "      <td>0.351074</td>\n",
       "      <td>-0.125513</td>\n",
       "      <td>0.192252</td>\n",
       "      <td>0.456381</td>\n",
       "      <td>0.470691</td>\n",
       "      <td>0.189230</td>\n",
       "      <td>-0.470297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4024</th>\n",
       "      <td>0.406802</td>\n",
       "      <td>1.031611</td>\n",
       "      <td>0.346160</td>\n",
       "      <td>-0.092680</td>\n",
       "      <td>0.201132</td>\n",
       "      <td>0.481534</td>\n",
       "      <td>0.439564</td>\n",
       "      <td>0.183140</td>\n",
       "      <td>-0.477588</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4025</th>\n",
       "      <td>0.394667</td>\n",
       "      <td>1.028484</td>\n",
       "      <td>0.343763</td>\n",
       "      <td>-0.063725</td>\n",
       "      <td>0.208549</td>\n",
       "      <td>0.503252</td>\n",
       "      <td>0.413610</td>\n",
       "      <td>0.177519</td>\n",
       "      <td>-0.480243</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4026</th>\n",
       "      <td>0.383449</td>\n",
       "      <td>1.025322</td>\n",
       "      <td>0.342435</td>\n",
       "      <td>-0.036687</td>\n",
       "      <td>0.215500</td>\n",
       "      <td>0.523334</td>\n",
       "      <td>0.389785</td>\n",
       "      <td>0.171742</td>\n",
       "      <td>-0.480810</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4027</th>\n",
       "      <td>0.374466</td>\n",
       "      <td>1.021966</td>\n",
       "      <td>0.343284</td>\n",
       "      <td>-0.012267</td>\n",
       "      <td>0.221239</td>\n",
       "      <td>0.541335</td>\n",
       "      <td>0.369765</td>\n",
       "      <td>0.166301</td>\n",
       "      <td>-0.477211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4028</th>\n",
       "      <td>0.366849</td>\n",
       "      <td>1.018457</td>\n",
       "      <td>0.345263</td>\n",
       "      <td>0.010247</td>\n",
       "      <td>0.226303</td>\n",
       "      <td>0.558417</td>\n",
       "      <td>0.351261</td>\n",
       "      <td>0.160971</td>\n",
       "      <td>-0.472121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4029</th>\n",
       "      <td>0.360769</td>\n",
       "      <td>1.014857</td>\n",
       "      <td>0.348987</td>\n",
       "      <td>0.031132</td>\n",
       "      <td>0.230373</td>\n",
       "      <td>0.574262</td>\n",
       "      <td>0.335845</td>\n",
       "      <td>0.155965</td>\n",
       "      <td>-0.462988</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4030</th>\n",
       "      <td>0.355438</td>\n",
       "      <td>1.011311</td>\n",
       "      <td>0.353382</td>\n",
       "      <td>0.050697</td>\n",
       "      <td>0.233915</td>\n",
       "      <td>0.589386</td>\n",
       "      <td>0.321705</td>\n",
       "      <td>0.151075</td>\n",
       "      <td>-0.452152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4031</th>\n",
       "      <td>0.351460</td>\n",
       "      <td>1.007571</td>\n",
       "      <td>0.359094</td>\n",
       "      <td>0.069037</td>\n",
       "      <td>0.236550</td>\n",
       "      <td>0.604210</td>\n",
       "      <td>0.309030</td>\n",
       "      <td>0.146429</td>\n",
       "      <td>-0.438362</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4032</th>\n",
       "      <td>0.348179</td>\n",
       "      <td>1.003819</td>\n",
       "      <td>0.365347</td>\n",
       "      <td>0.086246</td>\n",
       "      <td>0.238588</td>\n",
       "      <td>0.618763</td>\n",
       "      <td>0.296895</td>\n",
       "      <td>0.141897</td>\n",
       "      <td>-0.422750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4033</th>\n",
       "      <td>0.345602</td>\n",
       "      <td>0.999959</td>\n",
       "      <td>0.372440</td>\n",
       "      <td>0.102339</td>\n",
       "      <td>0.239853</td>\n",
       "      <td>0.633427</td>\n",
       "      <td>0.285417</td>\n",
       "      <td>0.137539</td>\n",
       "      <td>-0.404635</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4034</th>\n",
       "      <td>0.343469</td>\n",
       "      <td>0.996078</td>\n",
       "      <td>0.379977</td>\n",
       "      <td>0.117491</td>\n",
       "      <td>0.240447</td>\n",
       "      <td>0.648144</td>\n",
       "      <td>0.274036</td>\n",
       "      <td>0.133361</td>\n",
       "      <td>-0.384092</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4035</th>\n",
       "      <td>0.341403</td>\n",
       "      <td>0.992119</td>\n",
       "      <td>0.388193</td>\n",
       "      <td>0.130977</td>\n",
       "      <td>0.240229</td>\n",
       "      <td>0.662780</td>\n",
       "      <td>0.263263</td>\n",
       "      <td>0.129205</td>\n",
       "      <td>-0.360312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4036</th>\n",
       "      <td>0.339155</td>\n",
       "      <td>0.988076</td>\n",
       "      <td>0.396830</td>\n",
       "      <td>0.142792</td>\n",
       "      <td>0.239199</td>\n",
       "      <td>0.677421</td>\n",
       "      <td>0.252486</td>\n",
       "      <td>0.125207</td>\n",
       "      <td>-0.333344</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4037</th>\n",
       "      <td>0.336801</td>\n",
       "      <td>0.983870</td>\n",
       "      <td>0.405891</td>\n",
       "      <td>0.152838</td>\n",
       "      <td>0.237116</td>\n",
       "      <td>0.692085</td>\n",
       "      <td>0.241604</td>\n",
       "      <td>0.121560</td>\n",
       "      <td>-0.302876</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4038</th>\n",
       "      <td>0.333105</td>\n",
       "      <td>0.979750</td>\n",
       "      <td>0.414312</td>\n",
       "      <td>0.161487</td>\n",
       "      <td>0.234518</td>\n",
       "      <td>0.705852</td>\n",
       "      <td>0.229448</td>\n",
       "      <td>0.117937</td>\n",
       "      <td>-0.269058</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4039</th>\n",
       "      <td>0.330032</td>\n",
       "      <td>0.975605</td>\n",
       "      <td>0.422764</td>\n",
       "      <td>0.169016</td>\n",
       "      <td>0.231014</td>\n",
       "      <td>0.719067</td>\n",
       "      <td>0.217504</td>\n",
       "      <td>0.114786</td>\n",
       "      <td>-0.234155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4040</th>\n",
       "      <td>0.328282</td>\n",
       "      <td>0.971520</td>\n",
       "      <td>0.431214</td>\n",
       "      <td>0.176834</td>\n",
       "      <td>0.226859</td>\n",
       "      <td>0.731885</td>\n",
       "      <td>0.206181</td>\n",
       "      <td>0.112219</td>\n",
       "      <td>-0.199331</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4041</th>\n",
       "      <td>0.324529</td>\n",
       "      <td>0.967723</td>\n",
       "      <td>0.438819</td>\n",
       "      <td>0.184827</td>\n",
       "      <td>0.222666</td>\n",
       "      <td>0.742995</td>\n",
       "      <td>0.194192</td>\n",
       "      <td>0.109065</td>\n",
       "      <td>-0.160477</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4042</th>\n",
       "      <td>0.319104</td>\n",
       "      <td>0.964336</td>\n",
       "      <td>0.445151</td>\n",
       "      <td>0.192473</td>\n",
       "      <td>0.218583</td>\n",
       "      <td>0.751217</td>\n",
       "      <td>0.182009</td>\n",
       "      <td>0.105614</td>\n",
       "      <td>-0.120001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4043</th>\n",
       "      <td>0.314059</td>\n",
       "      <td>0.961267</td>\n",
       "      <td>0.451263</td>\n",
       "      <td>0.202130</td>\n",
       "      <td>0.214608</td>\n",
       "      <td>0.757850</td>\n",
       "      <td>0.170872</td>\n",
       "      <td>0.101835</td>\n",
       "      <td>-0.078618</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4044</th>\n",
       "      <td>0.310310</td>\n",
       "      <td>0.958472</td>\n",
       "      <td>0.457469</td>\n",
       "      <td>0.214932</td>\n",
       "      <td>0.210587</td>\n",
       "      <td>0.763316</td>\n",
       "      <td>0.161033</td>\n",
       "      <td>0.098190</td>\n",
       "      <td>-0.036514</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4045</th>\n",
       "      <td>0.304991</td>\n",
       "      <td>0.956055</td>\n",
       "      <td>0.462331</td>\n",
       "      <td>0.227870</td>\n",
       "      <td>0.206989</td>\n",
       "      <td>0.764946</td>\n",
       "      <td>0.150830</td>\n",
       "      <td>0.094319</td>\n",
       "      <td>0.006489</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4046</th>\n",
       "      <td>0.298788</td>\n",
       "      <td>0.954124</td>\n",
       "      <td>0.465799</td>\n",
       "      <td>0.241492</td>\n",
       "      <td>0.203805</td>\n",
       "      <td>0.762510</td>\n",
       "      <td>0.141203</td>\n",
       "      <td>0.090635</td>\n",
       "      <td>0.048694</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4047</th>\n",
       "      <td>0.293078</td>\n",
       "      <td>0.952430</td>\n",
       "      <td>0.469163</td>\n",
       "      <td>0.256913</td>\n",
       "      <td>0.200933</td>\n",
       "      <td>0.758058</td>\n",
       "      <td>0.132249</td>\n",
       "      <td>0.086963</td>\n",
       "      <td>0.090425</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4048</th>\n",
       "      <td>0.288414</td>\n",
       "      <td>0.950995</td>\n",
       "      <td>0.472476</td>\n",
       "      <td>0.274387</td>\n",
       "      <td>0.198202</td>\n",
       "      <td>0.751966</td>\n",
       "      <td>0.124431</td>\n",
       "      <td>0.083759</td>\n",
       "      <td>0.130905</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4049</th>\n",
       "      <td>0.283392</td>\n",
       "      <td>0.949928</td>\n",
       "      <td>0.475184</td>\n",
       "      <td>0.291758</td>\n",
       "      <td>0.195881</td>\n",
       "      <td>0.742996</td>\n",
       "      <td>0.117925</td>\n",
       "      <td>0.080658</td>\n",
       "      <td>0.169807</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Waist_X   Waist_Y   Waist_Z  R_Foot_X  R_Foot_Y  R_Foot_Z  L_Foot_X  \\\n",
       "4000  0.976676  1.224183  0.289238 -0.496335  0.053867 -0.043498  0.865819   \n",
       "4001  0.950934  1.210933  0.300699 -0.506135  0.052749 -0.019599  0.873998   \n",
       "4002  0.925361  1.197943  0.312225 -0.514672  0.052463  0.002984  0.880861   \n",
       "4003  0.898110  1.184754  0.323552 -0.518997  0.052893  0.025483  0.885049   \n",
       "4004  0.870596  1.172180  0.333662 -0.519373  0.054445  0.046371  0.886216   \n",
       "4005  0.842546  1.159517  0.344233 -0.518455  0.056463  0.066589  0.885809   \n",
       "4006  0.815824  1.147585  0.354245 -0.516732  0.059069  0.085118  0.883878   \n",
       "4007  0.788414  1.135978  0.363500 -0.511211  0.062655  0.103100  0.878378   \n",
       "4008  0.762003  1.125182  0.371663 -0.503980  0.066878  0.119850  0.870706   \n",
       "4009  0.735495  1.114798  0.379169 -0.493962  0.071835  0.136177  0.860058   \n",
       "4010  0.709318  1.104973  0.385494 -0.480940  0.077525  0.152238  0.846115   \n",
       "4011  0.684416  1.095954  0.390867 -0.466669  0.083552  0.167791  0.830458   \n",
       "4012  0.660509  1.087606  0.395156 -0.452427  0.089923  0.183246  0.813351   \n",
       "4013  0.636370  1.079752  0.398385 -0.434636  0.096838  0.199738  0.793527   \n",
       "4014  0.611334  1.072404  0.399886 -0.413480  0.104681  0.217796  0.770366   \n",
       "4015  0.587391  1.065872  0.399864 -0.390492  0.112881  0.237442  0.745170   \n",
       "4016  0.564533  1.060154  0.397988 -0.366591  0.121524  0.259783  0.717565   \n",
       "4017  0.541590  1.055078  0.394300 -0.337541  0.130895  0.284781  0.686405   \n",
       "4018  0.518180  1.050709  0.387990 -0.304435  0.141313  0.312195  0.651199   \n",
       "4019  0.496597  1.046943  0.380897 -0.269973  0.151629  0.340976  0.615220   \n",
       "4020  0.475542  1.043524  0.372586 -0.232858  0.162407  0.371599  0.576542   \n",
       "4021  0.456593  1.040430  0.364801 -0.196586  0.172615  0.401032  0.539849   \n",
       "4022  0.437558  1.037541  0.356855 -0.159136  0.183140  0.430216  0.503015   \n",
       "4023  0.421793  1.034603  0.351074 -0.125513  0.192252  0.456381  0.470691   \n",
       "4024  0.406802  1.031611  0.346160 -0.092680  0.201132  0.481534  0.439564   \n",
       "4025  0.394667  1.028484  0.343763 -0.063725  0.208549  0.503252  0.413610   \n",
       "4026  0.383449  1.025322  0.342435 -0.036687  0.215500  0.523334  0.389785   \n",
       "4027  0.374466  1.021966  0.343284 -0.012267  0.221239  0.541335  0.369765   \n",
       "4028  0.366849  1.018457  0.345263  0.010247  0.226303  0.558417  0.351261   \n",
       "4029  0.360769  1.014857  0.348987  0.031132  0.230373  0.574262  0.335845   \n",
       "4030  0.355438  1.011311  0.353382  0.050697  0.233915  0.589386  0.321705   \n",
       "4031  0.351460  1.007571  0.359094  0.069037  0.236550  0.604210  0.309030   \n",
       "4032  0.348179  1.003819  0.365347  0.086246  0.238588  0.618763  0.296895   \n",
       "4033  0.345602  0.999959  0.372440  0.102339  0.239853  0.633427  0.285417   \n",
       "4034  0.343469  0.996078  0.379977  0.117491  0.240447  0.648144  0.274036   \n",
       "4035  0.341403  0.992119  0.388193  0.130977  0.240229  0.662780  0.263263   \n",
       "4036  0.339155  0.988076  0.396830  0.142792  0.239199  0.677421  0.252486   \n",
       "4037  0.336801  0.983870  0.405891  0.152838  0.237116  0.692085  0.241604   \n",
       "4038  0.333105  0.979750  0.414312  0.161487  0.234518  0.705852  0.229448   \n",
       "4039  0.330032  0.975605  0.422764  0.169016  0.231014  0.719067  0.217504   \n",
       "4040  0.328282  0.971520  0.431214  0.176834  0.226859  0.731885  0.206181   \n",
       "4041  0.324529  0.967723  0.438819  0.184827  0.222666  0.742995  0.194192   \n",
       "4042  0.319104  0.964336  0.445151  0.192473  0.218583  0.751217  0.182009   \n",
       "4043  0.314059  0.961267  0.451263  0.202130  0.214608  0.757850  0.170872   \n",
       "4044  0.310310  0.958472  0.457469  0.214932  0.210587  0.763316  0.161033   \n",
       "4045  0.304991  0.956055  0.462331  0.227870  0.206989  0.764946  0.150830   \n",
       "4046  0.298788  0.954124  0.465799  0.241492  0.203805  0.762510  0.141203   \n",
       "4047  0.293078  0.952430  0.469163  0.256913  0.200933  0.758058  0.132249   \n",
       "4048  0.288414  0.950995  0.472476  0.274387  0.198202  0.751966  0.124431   \n",
       "4049  0.283392  0.949928  0.475184  0.291758  0.195881  0.742996  0.117925   \n",
       "\n",
       "      L_Foot_Y  L_Foot_Z  \n",
       "4000  0.463140 -0.526091  \n",
       "4001  0.448951 -0.522736  \n",
       "4002  0.434433 -0.517868  \n",
       "4003  0.419374 -0.509630  \n",
       "4004  0.404494 -0.499613  \n",
       "4005  0.388882 -0.487463  \n",
       "4006  0.373643 -0.475693  \n",
       "4007  0.358224 -0.462178  \n",
       "4008  0.343416 -0.449404  \n",
       "4009  0.328665 -0.436027  \n",
       "4010  0.314367 -0.423016  \n",
       "4011  0.300869 -0.411670  \n",
       "4012  0.287924 -0.403495  \n",
       "4013  0.275436 -0.395706  \n",
       "4014  0.263098 -0.389809  \n",
       "4015  0.251755 -0.387773  \n",
       "4016  0.241478 -0.392333  \n",
       "4017  0.232045 -0.398495  \n",
       "4018  0.223136 -0.408391  \n",
       "4019  0.215403 -0.420662  \n",
       "4020  0.208191 -0.435238  \n",
       "4021  0.201747 -0.448758  \n",
       "4022  0.195140 -0.461101  \n",
       "4023  0.189230 -0.470297  \n",
       "4024  0.183140 -0.477588  \n",
       "4025  0.177519 -0.480243  \n",
       "4026  0.171742 -0.480810  \n",
       "4027  0.166301 -0.477211  \n",
       "4028  0.160971 -0.472121  \n",
       "4029  0.155965 -0.462988  \n",
       "4030  0.151075 -0.452152  \n",
       "4031  0.146429 -0.438362  \n",
       "4032  0.141897 -0.422750  \n",
       "4033  0.137539 -0.404635  \n",
       "4034  0.133361 -0.384092  \n",
       "4035  0.129205 -0.360312  \n",
       "4036  0.125207 -0.333344  \n",
       "4037  0.121560 -0.302876  \n",
       "4038  0.117937 -0.269058  \n",
       "4039  0.114786 -0.234155  \n",
       "4040  0.112219 -0.199331  \n",
       "4041  0.109065 -0.160477  \n",
       "4042  0.105614 -0.120001  \n",
       "4043  0.101835 -0.078618  \n",
       "4044  0.098190 -0.036514  \n",
       "4045  0.094319  0.006489  \n",
       "4046  0.090635  0.048694  \n",
       "4047  0.086963  0.090425  \n",
       "4048  0.083759  0.130905  \n",
       "4049  0.080658  0.169807  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pred, actual = PredictModel(x_test, y_test, model_2)\n",
    "print(actual.shape)\n",
    "print(pred.shape)\n",
    "toWriteActual, toWritePred = DisplayPredictions(pred, actual, 4000, 4050)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Write Results to File "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Actual_Waist_X</th>\n",
       "      <th>Actual_Waist_Y</th>\n",
       "      <th>Actual_Waist_Z</th>\n",
       "      <th>Actual_R_Foot_X</th>\n",
       "      <th>Actual_R_Foot_Y</th>\n",
       "      <th>Actual_R_Foot_Z</th>\n",
       "      <th>Actual_L_Foot_X</th>\n",
       "      <th>Actual_L_Foot_Y</th>\n",
       "      <th>Actual_L_Foot_Z</th>\n",
       "      <th>Pred_Waist_X</th>\n",
       "      <th>Pred_Waist_Y</th>\n",
       "      <th>Pred_Waist_Z</th>\n",
       "      <th>Pred_R_Foot_X</th>\n",
       "      <th>Pred_R_Foot_Y</th>\n",
       "      <th>Pred_R_Foot_Z</th>\n",
       "      <th>Pred_L_Foot_X</th>\n",
       "      <th>Pred_L_Foot_Y</th>\n",
       "      <th>Pred_L_Foot_Z</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.060439</td>\n",
       "      <td>1.022620</td>\n",
       "      <td>-0.098646</td>\n",
       "      <td>0.296409</td>\n",
       "      <td>0.111177</td>\n",
       "      <td>-0.002579</td>\n",
       "      <td>-0.105350</td>\n",
       "      <td>0.107512</td>\n",
       "      <td>0.008089</td>\n",
       "      <td>0.220943</td>\n",
       "      <td>1.013334</td>\n",
       "      <td>0.143499</td>\n",
       "      <td>-0.211455</td>\n",
       "      <td>0.113909</td>\n",
       "      <td>0.241544</td>\n",
       "      <td>0.342301</td>\n",
       "      <td>0.102949</td>\n",
       "      <td>-0.439054</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.060260</td>\n",
       "      <td>1.022620</td>\n",
       "      <td>-0.098646</td>\n",
       "      <td>0.296409</td>\n",
       "      <td>0.111177</td>\n",
       "      <td>-0.002579</td>\n",
       "      <td>-0.105350</td>\n",
       "      <td>0.107512</td>\n",
       "      <td>0.008089</td>\n",
       "      <td>0.220573</td>\n",
       "      <td>1.013352</td>\n",
       "      <td>0.143619</td>\n",
       "      <td>-0.211553</td>\n",
       "      <td>0.114130</td>\n",
       "      <td>0.241542</td>\n",
       "      <td>0.342526</td>\n",
       "      <td>0.102511</td>\n",
       "      <td>-0.438521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.059955</td>\n",
       "      <td>1.022620</td>\n",
       "      <td>-0.098769</td>\n",
       "      <td>0.296418</td>\n",
       "      <td>0.111177</td>\n",
       "      <td>-0.002579</td>\n",
       "      <td>-0.105350</td>\n",
       "      <td>0.107433</td>\n",
       "      <td>0.008048</td>\n",
       "      <td>0.220180</td>\n",
       "      <td>1.013364</td>\n",
       "      <td>0.143694</td>\n",
       "      <td>-0.211643</td>\n",
       "      <td>0.114310</td>\n",
       "      <td>0.241550</td>\n",
       "      <td>0.342751</td>\n",
       "      <td>0.102095</td>\n",
       "      <td>-0.438064</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.059620</td>\n",
       "      <td>1.022658</td>\n",
       "      <td>-0.099001</td>\n",
       "      <td>0.296458</td>\n",
       "      <td>0.111177</td>\n",
       "      <td>-0.002579</td>\n",
       "      <td>-0.105350</td>\n",
       "      <td>0.107417</td>\n",
       "      <td>0.007960</td>\n",
       "      <td>0.219799</td>\n",
       "      <td>1.013369</td>\n",
       "      <td>0.143689</td>\n",
       "      <td>-0.211898</td>\n",
       "      <td>0.114430</td>\n",
       "      <td>0.241617</td>\n",
       "      <td>0.343008</td>\n",
       "      <td>0.101757</td>\n",
       "      <td>-0.437876</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.059283</td>\n",
       "      <td>1.022695</td>\n",
       "      <td>-0.099210</td>\n",
       "      <td>0.296351</td>\n",
       "      <td>0.111177</td>\n",
       "      <td>-0.002579</td>\n",
       "      <td>-0.105444</td>\n",
       "      <td>0.107411</td>\n",
       "      <td>0.007911</td>\n",
       "      <td>0.219224</td>\n",
       "      <td>1.013377</td>\n",
       "      <td>0.143643</td>\n",
       "      <td>-0.212029</td>\n",
       "      <td>0.114606</td>\n",
       "      <td>0.241705</td>\n",
       "      <td>0.343034</td>\n",
       "      <td>0.101401</td>\n",
       "      <td>-0.437544</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5395</th>\n",
       "      <td>-0.266650</td>\n",
       "      <td>1.028392</td>\n",
       "      <td>-0.104049</td>\n",
       "      <td>-0.055547</td>\n",
       "      <td>0.211484</td>\n",
       "      <td>0.045802</td>\n",
       "      <td>-0.325084</td>\n",
       "      <td>0.122200</td>\n",
       "      <td>0.065459</td>\n",
       "      <td>-0.332323</td>\n",
       "      <td>0.983205</td>\n",
       "      <td>-0.058999</td>\n",
       "      <td>-0.325814</td>\n",
       "      <td>0.159368</td>\n",
       "      <td>-0.105212</td>\n",
       "      <td>-0.483079</td>\n",
       "      <td>0.121700</td>\n",
       "      <td>-0.202876</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5396</th>\n",
       "      <td>-0.263761</td>\n",
       "      <td>1.024829</td>\n",
       "      <td>-0.101151</td>\n",
       "      <td>-0.055403</td>\n",
       "      <td>0.215707</td>\n",
       "      <td>0.044948</td>\n",
       "      <td>-0.325417</td>\n",
       "      <td>0.121903</td>\n",
       "      <td>0.065412</td>\n",
       "      <td>-0.333360</td>\n",
       "      <td>0.983648</td>\n",
       "      <td>-0.057928</td>\n",
       "      <td>-0.343283</td>\n",
       "      <td>0.162712</td>\n",
       "      <td>-0.114889</td>\n",
       "      <td>-0.477255</td>\n",
       "      <td>0.120932</td>\n",
       "      <td>-0.204384</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5397</th>\n",
       "      <td>-0.271885</td>\n",
       "      <td>1.032776</td>\n",
       "      <td>-0.107868</td>\n",
       "      <td>-0.054945</td>\n",
       "      <td>0.219580</td>\n",
       "      <td>0.044346</td>\n",
       "      <td>-0.325417</td>\n",
       "      <td>0.121905</td>\n",
       "      <td>0.065404</td>\n",
       "      <td>-0.334303</td>\n",
       "      <td>0.984181</td>\n",
       "      <td>-0.056783</td>\n",
       "      <td>-0.360225</td>\n",
       "      <td>0.166057</td>\n",
       "      <td>-0.124497</td>\n",
       "      <td>-0.470918</td>\n",
       "      <td>0.120186</td>\n",
       "      <td>-0.205555</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5398</th>\n",
       "      <td>-0.268666</td>\n",
       "      <td>1.028839</td>\n",
       "      <td>-0.104847</td>\n",
       "      <td>-0.054282</td>\n",
       "      <td>0.222948</td>\n",
       "      <td>0.044158</td>\n",
       "      <td>-0.325470</td>\n",
       "      <td>0.121905</td>\n",
       "      <td>0.065396</td>\n",
       "      <td>-0.334956</td>\n",
       "      <td>0.984779</td>\n",
       "      <td>-0.055500</td>\n",
       "      <td>-0.376179</td>\n",
       "      <td>0.169332</td>\n",
       "      <td>-0.133578</td>\n",
       "      <td>-0.464236</td>\n",
       "      <td>0.119449</td>\n",
       "      <td>-0.206404</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5399</th>\n",
       "      <td>-0.275608</td>\n",
       "      <td>1.036180</td>\n",
       "      <td>-0.110867</td>\n",
       "      <td>-0.053518</td>\n",
       "      <td>0.225989</td>\n",
       "      <td>0.044157</td>\n",
       "      <td>-0.325380</td>\n",
       "      <td>0.121905</td>\n",
       "      <td>0.065388</td>\n",
       "      <td>-0.335269</td>\n",
       "      <td>0.985408</td>\n",
       "      <td>-0.054083</td>\n",
       "      <td>-0.391227</td>\n",
       "      <td>0.172501</td>\n",
       "      <td>-0.142130</td>\n",
       "      <td>-0.457248</td>\n",
       "      <td>0.118699</td>\n",
       "      <td>-0.207083</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5400 rows  18 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Actual_Waist_X  Actual_Waist_Y  Actual_Waist_Z  Actual_R_Foot_X  \\\n",
       "0           0.060439        1.022620       -0.098646         0.296409   \n",
       "1           0.060260        1.022620       -0.098646         0.296409   \n",
       "2           0.059955        1.022620       -0.098769         0.296418   \n",
       "3           0.059620        1.022658       -0.099001         0.296458   \n",
       "4           0.059283        1.022695       -0.099210         0.296351   \n",
       "...              ...             ...             ...              ...   \n",
       "5395       -0.266650        1.028392       -0.104049        -0.055547   \n",
       "5396       -0.263761        1.024829       -0.101151        -0.055403   \n",
       "5397       -0.271885        1.032776       -0.107868        -0.054945   \n",
       "5398       -0.268666        1.028839       -0.104847        -0.054282   \n",
       "5399       -0.275608        1.036180       -0.110867        -0.053518   \n",
       "\n",
       "      Actual_R_Foot_Y  Actual_R_Foot_Z  Actual_L_Foot_X  Actual_L_Foot_Y  \\\n",
       "0            0.111177        -0.002579        -0.105350         0.107512   \n",
       "1            0.111177        -0.002579        -0.105350         0.107512   \n",
       "2            0.111177        -0.002579        -0.105350         0.107433   \n",
       "3            0.111177        -0.002579        -0.105350         0.107417   \n",
       "4            0.111177        -0.002579        -0.105444         0.107411   \n",
       "...               ...              ...              ...              ...   \n",
       "5395         0.211484         0.045802        -0.325084         0.122200   \n",
       "5396         0.215707         0.044948        -0.325417         0.121903   \n",
       "5397         0.219580         0.044346        -0.325417         0.121905   \n",
       "5398         0.222948         0.044158        -0.325470         0.121905   \n",
       "5399         0.225989         0.044157        -0.325380         0.121905   \n",
       "\n",
       "      Actual_L_Foot_Z  Pred_Waist_X  Pred_Waist_Y  Pred_Waist_Z  \\\n",
       "0            0.008089      0.220943      1.013334      0.143499   \n",
       "1            0.008089      0.220573      1.013352      0.143619   \n",
       "2            0.008048      0.220180      1.013364      0.143694   \n",
       "3            0.007960      0.219799      1.013369      0.143689   \n",
       "4            0.007911      0.219224      1.013377      0.143643   \n",
       "...               ...           ...           ...           ...   \n",
       "5395         0.065459     -0.332323      0.983205     -0.058999   \n",
       "5396         0.065412     -0.333360      0.983648     -0.057928   \n",
       "5397         0.065404     -0.334303      0.984181     -0.056783   \n",
       "5398         0.065396     -0.334956      0.984779     -0.055500   \n",
       "5399         0.065388     -0.335269      0.985408     -0.054083   \n",
       "\n",
       "      Pred_R_Foot_X  Pred_R_Foot_Y  Pred_R_Foot_Z  Pred_L_Foot_X  \\\n",
       "0         -0.211455       0.113909       0.241544       0.342301   \n",
       "1         -0.211553       0.114130       0.241542       0.342526   \n",
       "2         -0.211643       0.114310       0.241550       0.342751   \n",
       "3         -0.211898       0.114430       0.241617       0.343008   \n",
       "4         -0.212029       0.114606       0.241705       0.343034   \n",
       "...             ...            ...            ...            ...   \n",
       "5395      -0.325814       0.159368      -0.105212      -0.483079   \n",
       "5396      -0.343283       0.162712      -0.114889      -0.477255   \n",
       "5397      -0.360225       0.166057      -0.124497      -0.470918   \n",
       "5398      -0.376179       0.169332      -0.133578      -0.464236   \n",
       "5399      -0.391227       0.172501      -0.142130      -0.457248   \n",
       "\n",
       "      Pred_L_Foot_Y  Pred_L_Foot_Z  \n",
       "0          0.102949      -0.439054  \n",
       "1          0.102511      -0.438521  \n",
       "2          0.102095      -0.438064  \n",
       "3          0.101757      -0.437876  \n",
       "4          0.101401      -0.437544  \n",
       "...             ...            ...  \n",
       "5395       0.121700      -0.202876  \n",
       "5396       0.120932      -0.204384  \n",
       "5397       0.120186      -0.205555  \n",
       "5398       0.119449      -0.206404  \n",
       "5399       0.118699      -0.207083  \n",
       "\n",
       "[5400 rows x 18 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "results_path = \"../results/\"\n",
    "\n",
    "for column in toWriteActual.columns:\n",
    "    toWriteActual.rename(columns={column : \"Actual_\" + column}, inplace=True)\n",
    "\n",
    "for column in toWritePred.columns:\n",
    "    toWritePred.rename(columns={column : \"Pred_\" + column}, inplace=True)\n",
    "\n",
    "results = pd.concat([toWriteActual, toWritePred], axis=1)\n",
    "display(results)\n",
    "\n",
    "output = results_path+ \"output.csv\"\n",
    "results.to_csv(output, index = False)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "vscode": {
   "interpreter": {
    "hash": "85273928d8596bf28b6a3fc4ded2b0665eee93193e52c7eff13f3a9a291ee5c2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
